{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7fdfda1b28f7d93",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-27T13:55:42.624327200Z",
     "start_time": "2025-02-27T13:55:33.624685100Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from img2vec import rgb2flatPCA\n",
    "import math\n",
    "import os\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "from img2vec import pretrain_pca\n",
    "\n",
    "# Define the batch size\n",
    "batch_size = 64\n",
    "\n",
    "# PCA components - this will be the dimension of our feature vectors\n",
    "n_components = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63df6bf1d9882ce",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51d1d26e45f4b741",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-27T13:55:42.686501900Z",
     "start_time": "2025-02-27T13:55:42.624327200Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(os.path.join('..', '..', 'data', 'train.csv'))\n",
    "val_data = pd.read_csv(os.path.join('..', '..', 'data', 'val.csv'))\n",
    "test_data = pd.read_csv(os.path.join('..', '..', 'data', 'test.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d02278",
   "metadata": {},
   "source": [
    "### Encode age labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a86e2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists('age_encoder.pkl'):\n",
    "    age_encoder = joblib.load('age_encoder.pkl')\n",
    "    print(\"Age encoder loaded successfully.\")\n",
    "    \n",
    "    # Use transform only when encoder is loaded\n",
    "    train_data['age_label'] = age_encoder.transform(train_data['age'])\n",
    "    val_data['age_label'] = age_encoder.transform(val_data['age'])\n",
    "    test_data['age_label'] = age_encoder.transform(test_data['age'])\n",
    "else:\n",
    "    age_encoder = LabelEncoder()\n",
    "    \n",
    "    # Fit and transform on training data\n",
    "    train_data['age_label'] = age_encoder.fit_transform(train_data['age'])\n",
    "    \n",
    "    # Then just transform validation and test data\n",
    "    val_data['age_label'] = age_encoder.transform(val_data['age'])\n",
    "    test_data['age_label'] = age_encoder.transform(test_data['age'])\n",
    "    \n",
    "    # Save after fitting\n",
    "    joblib.dump(age_encoder, 'age_encoder.pkl')\n",
    "    print(\"Age encoder saved successfully.\")\n",
    "\n",
    "num_classes = len(age_encoder.classes_)\n",
    "print(\"Age classes:\", age_encoder.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d541bc891ff22bf2",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### add the path of the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b1d46fc867cb10a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-27T13:55:42.892867800Z",
     "start_time": "2025-02-27T13:55:42.736304700Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>face_id</th>\n",
       "      <th>original_image</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>img_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9855553@N08</td>\n",
       "      <td>1581</td>\n",
       "      <td>11658657103_4485e3f5ac_o.jpg</td>\n",
       "      <td>(60, 100)</td>\n",
       "      <td>m</td>\n",
       "      <td>..\\..\\data\\faces\\9855553@N08\\coarse_tilt_align...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>114841417@N06</td>\n",
       "      <td>502</td>\n",
       "      <td>12059583524_606ca96139_o.jpg</td>\n",
       "      <td>(15, 20)</td>\n",
       "      <td>m</td>\n",
       "      <td>..\\..\\data\\faces\\114841417@N06\\coarse_tilt_ali...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>66870968@N06</td>\n",
       "      <td>1227</td>\n",
       "      <td>11326189206_e08bdf6dfd_o.jpg</td>\n",
       "      <td>(25, 32)</td>\n",
       "      <td>m</td>\n",
       "      <td>..\\..\\data\\faces\\66870968@N06\\coarse_tilt_alig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8187011@N06</td>\n",
       "      <td>988</td>\n",
       "      <td>11133041085_e2ee5e12cb_o.jpg</td>\n",
       "      <td>(0, 2)</td>\n",
       "      <td>u</td>\n",
       "      <td>..\\..\\data\\faces\\8187011@N06\\coarse_tilt_align...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>114841417@N06</td>\n",
       "      <td>485</td>\n",
       "      <td>12059753735_7141b5443c_o.jpg</td>\n",
       "      <td>(15, 20)</td>\n",
       "      <td>f</td>\n",
       "      <td>..\\..\\data\\faces\\114841417@N06\\coarse_tilt_ali...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         user_id  face_id                original_image        age gender  \\\n",
       "0    9855553@N08     1581  11658657103_4485e3f5ac_o.jpg  (60, 100)      m   \n",
       "1  114841417@N06      502  12059583524_606ca96139_o.jpg   (15, 20)      m   \n",
       "2   66870968@N06     1227  11326189206_e08bdf6dfd_o.jpg   (25, 32)      m   \n",
       "3    8187011@N06      988  11133041085_e2ee5e12cb_o.jpg     (0, 2)      u   \n",
       "4  114841417@N06      485  12059753735_7141b5443c_o.jpg   (15, 20)      f   \n",
       "\n",
       "                                            img_path  \n",
       "0  ..\\..\\data\\faces\\9855553@N08\\coarse_tilt_align...  \n",
       "1  ..\\..\\data\\faces\\114841417@N06\\coarse_tilt_ali...  \n",
       "2  ..\\..\\data\\faces\\66870968@N06\\coarse_tilt_alig...  \n",
       "3  ..\\..\\data\\faces\\8187011@N06\\coarse_tilt_align...  \n",
       "4  ..\\..\\data\\faces\\114841417@N06\\coarse_tilt_ali...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def construct_img_path(row):\n",
    "    return os.path.join(\"..\", \"..\", \"data\", \"faces\", row['user_id'],\n",
    "                        \"coarse_tilt_aligned_face.\" + str(row['face_id']) + \".\" + row['original_image'])\n",
    "\n",
    "\n",
    "train_data['img_path'] = train_data.apply(construct_img_path, axis=1)\n",
    "val_data['img_path'] = val_data.apply(construct_img_path, axis=1)\n",
    "test_data['img_path'] = test_data.apply(construct_img_path, axis=1)\n",
    "train_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df65de30e5e25a7",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### add column for check if the image exists\n",
    "it will help us to detect if there is any missing image, or if there is any bug in the path construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c915be97d0fd9eb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-27T13:55:43.882862Z",
     "start_time": "2025-02-27T13:55:42.892867800Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>face_id</th>\n",
       "      <th>original_image</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>img_path</th>\n",
       "      <th>img_exists</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9855553@N08</td>\n",
       "      <td>1581</td>\n",
       "      <td>11658657103_4485e3f5ac_o.jpg</td>\n",
       "      <td>(60, 100)</td>\n",
       "      <td>m</td>\n",
       "      <td>..\\..\\data\\faces\\9855553@N08\\coarse_tilt_align...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>114841417@N06</td>\n",
       "      <td>502</td>\n",
       "      <td>12059583524_606ca96139_o.jpg</td>\n",
       "      <td>(15, 20)</td>\n",
       "      <td>m</td>\n",
       "      <td>..\\..\\data\\faces\\114841417@N06\\coarse_tilt_ali...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>66870968@N06</td>\n",
       "      <td>1227</td>\n",
       "      <td>11326189206_e08bdf6dfd_o.jpg</td>\n",
       "      <td>(25, 32)</td>\n",
       "      <td>m</td>\n",
       "      <td>..\\..\\data\\faces\\66870968@N06\\coarse_tilt_alig...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8187011@N06</td>\n",
       "      <td>988</td>\n",
       "      <td>11133041085_e2ee5e12cb_o.jpg</td>\n",
       "      <td>(0, 2)</td>\n",
       "      <td>u</td>\n",
       "      <td>..\\..\\data\\faces\\8187011@N06\\coarse_tilt_align...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>114841417@N06</td>\n",
       "      <td>485</td>\n",
       "      <td>12059753735_7141b5443c_o.jpg</td>\n",
       "      <td>(15, 20)</td>\n",
       "      <td>f</td>\n",
       "      <td>..\\..\\data\\faces\\114841417@N06\\coarse_tilt_ali...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         user_id  face_id                original_image        age gender  \\\n",
       "0    9855553@N08     1581  11658657103_4485e3f5ac_o.jpg  (60, 100)      m   \n",
       "1  114841417@N06      502  12059583524_606ca96139_o.jpg   (15, 20)      m   \n",
       "2   66870968@N06     1227  11326189206_e08bdf6dfd_o.jpg   (25, 32)      m   \n",
       "3    8187011@N06      988  11133041085_e2ee5e12cb_o.jpg     (0, 2)      u   \n",
       "4  114841417@N06      485  12059753735_7141b5443c_o.jpg   (15, 20)      f   \n",
       "\n",
       "                                            img_path  img_exists  \n",
       "0  ..\\..\\data\\faces\\9855553@N08\\coarse_tilt_align...        True  \n",
       "1  ..\\..\\data\\faces\\114841417@N06\\coarse_tilt_ali...        True  \n",
       "2  ..\\..\\data\\faces\\66870968@N06\\coarse_tilt_alig...        True  \n",
       "3  ..\\..\\data\\faces\\8187011@N06\\coarse_tilt_align...        True  \n",
       "4  ..\\..\\data\\faces\\114841417@N06\\coarse_tilt_ali...        True  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['img_exists'] = train_data['img_path'].apply(os.path.exists)\n",
    "val_data['img_exists'] = val_data['img_path'].apply(os.path.exists)\n",
    "test_data['img_exists'] = test_data['img_path'].apply(os.path.exists)\n",
    "\n",
    "train_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb2fa7830f47cf8",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Filter out any rows where the image doesn't exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "facd05e692b7b1f3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-27T13:55:44.025318400Z",
     "start_time": "2025-02-27T13:55:43.920113700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data_filtered = train_data[train_data['img_exists'] == True]\n",
    "val_data_filtered = val_data[val_data['img_exists'] == True]\n",
    "test_data_filtered = test_data[test_data['img_exists'] == True]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d88b72e1a3bb411",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Extract image paths and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b88ba22ca9223034",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-27T13:55:44.050151500Z",
     "start_time": "2025-02-27T13:55:43.930472100Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_image_paths = train_data_filtered['img_path'].tolist()\n",
    "train_labels = train_data_filtered['age_label'].values\n",
    "\n",
    "val_image_paths = val_data_filtered['img_path'].tolist()\n",
    "val_labels = val_data_filtered['age_label'].values\n",
    "\n",
    "test_image_paths = test_data_filtered['img_path'].tolist()\n",
    "test_labels = test_data_filtered['age_label'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e10a6213607095c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Define function that processes features in batches and stores them to avoid recomputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8883eccb2bc85b3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-27T13:56:07.455089900Z",
     "start_time": "2025-02-27T13:55:43.946396200Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-training PCA with 11856 images...\n",
      "Processing image 0/1000...\n",
      "Processing image 100/1000...\n",
      "Processing image 200/1000...\n",
      "Processing image 300/1000...\n",
      "Processing image 400/1000...\n",
      "Processing image 500/1000...\n",
      "Processing image 600/1000...\n",
      "Processing image 700/1000...\n",
      "Processing image 800/1000...\n",
      "Processing image 900/1000...\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 375. MiB for an array with shape (1000, 49152) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m pca \u001b[38;5;241m=\u001b[39m \u001b[43mpretrain_pca\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_image_paths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_components\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\projects\\ML\\final_project\\img2vec.py:126\u001b[0m, in \u001b[0;36mpretrain_pca\u001b[1;34m(image_paths, n_components, img_size)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;66;03m# Fit PCA\u001b[39;00m\n\u001b[0;32m    125\u001b[0m pca \u001b[38;5;241m=\u001b[39m PCA(n_components\u001b[38;5;241m=\u001b[39mn_components)\n\u001b[1;32m--> 126\u001b[0m \u001b[43mpca\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;66;03m# Save to cache\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m _PCA_CACHE\n",
      "File \u001b[1;32m~\\projects\\ML\\final_project\\venv\\lib\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\projects\\ML\\final_project\\venv\\lib\\site-packages\\sklearn\\decomposition\\_pca.py:442\u001b[0m, in \u001b[0;36mPCA.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    424\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    425\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    426\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Fit the model with X.\u001b[39;00m\n\u001b[0;32m    427\u001b[0m \n\u001b[0;32m    428\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    440\u001b[0m \u001b[38;5;124;03m        Returns the instance itself.\u001b[39;00m\n\u001b[0;32m    441\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 442\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    443\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\projects\\ML\\final_project\\venv\\lib\\site-packages\\sklearn\\decomposition\\_pca.py:505\u001b[0m, in \u001b[0;36mPCA._fit\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    495\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    496\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPCA with svd_solver=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124marpack\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is not supported for Array API inputs.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    497\u001b[0m     )\n\u001b[0;32m    499\u001b[0m \u001b[38;5;66;03m# Validate the data, without ever forcing a copy as any solver that\u001b[39;00m\n\u001b[0;32m    500\u001b[0m \u001b[38;5;66;03m# supports sparse input data and the `covariance_eigh` solver are\u001b[39;00m\n\u001b[0;32m    501\u001b[0m \u001b[38;5;66;03m# written in a way to avoid the need for any inplace modification of\u001b[39;00m\n\u001b[0;32m    502\u001b[0m \u001b[38;5;66;03m# the input data contrary to the other solvers.\u001b[39;00m\n\u001b[0;32m    503\u001b[0m \u001b[38;5;66;03m# The copy will happen\u001b[39;00m\n\u001b[0;32m    504\u001b[0m \u001b[38;5;66;03m# later, only if needed, once the solver negotiation below is done.\u001b[39;00m\n\u001b[1;32m--> 505\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    506\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    507\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat64\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    509\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    510\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    511\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    512\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    513\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    514\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_svd_solver \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msvd_solver\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_svd_solver \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m issparse(X):\n",
      "File \u001b[1;32m~\\projects\\ML\\final_project\\venv\\lib\\site-packages\\sklearn\\utils\\validation.py:2944\u001b[0m, in \u001b[0;36mvalidate_data\u001b[1;34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[0m\n\u001b[0;32m   2942\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m   2943\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[1;32m-> 2944\u001b[0m     out \u001b[38;5;241m=\u001b[39m check_array(X, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m   2945\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[0;32m   2946\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
      "File \u001b[1;32m~\\projects\\ML\\final_project\\venv\\lib\\site-packages\\sklearn\\utils\\validation.py:1055\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m   1053\u001b[0m         array \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mastype(array, dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1054\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1055\u001b[0m         array \u001b[38;5;241m=\u001b[39m \u001b[43m_asarray_with_order\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1056\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ComplexWarning \u001b[38;5;28;01mas\u001b[39;00m complex_warning:\n\u001b[0;32m   1057\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1058\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComplex data not supported\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(array)\n\u001b[0;32m   1059\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcomplex_warning\u001b[39;00m\n",
      "File \u001b[1;32m~\\projects\\ML\\final_project\\venv\\lib\\site-packages\\sklearn\\utils\\_array_api.py:839\u001b[0m, in \u001b[0;36m_asarray_with_order\u001b[1;34m(array, dtype, order, copy, xp, device)\u001b[0m\n\u001b[0;32m    837\u001b[0m     array \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39marray(array, order\u001b[38;5;241m=\u001b[39morder, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    838\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 839\u001b[0m     array \u001b[38;5;241m=\u001b[39m \u001b[43mnumpy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    841\u001b[0m \u001b[38;5;66;03m# At this point array is a NumPy ndarray. We convert it to an array\u001b[39;00m\n\u001b[0;32m    842\u001b[0m \u001b[38;5;66;03m# container that is consistent with the input's namespace.\u001b[39;00m\n\u001b[0;32m    843\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m xp\u001b[38;5;241m.\u001b[39masarray(array)\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 375. MiB for an array with shape (1000, 49152) and data type float64"
     ]
    }
   ],
   "source": [
    "\n",
    "def preprocess_and_save_features_pca(image_paths, output_file, n_components=256, batch_size=64):\n",
    "    \"\"\"\n",
    "    Process images using PCA and save features to avoid recomputation.\n",
    "    \n",
    "    Args:\n",
    "        image_paths: List of image paths to process\n",
    "        output_file: File to save/load processed features\n",
    "        n_components: Number of PCA components to use\n",
    "        batch_size: Batch size for processing\n",
    "        \n",
    "    Returns:\n",
    "        numpy.ndarray: PCA-reduced features\n",
    "    \"\"\"\n",
    "    if os.path.exists(output_file):\n",
    "        print(f\"Loading pre-processed PCA features from {output_file}\")\n",
    "        return np.load(output_file)\n",
    "    \n",
    "    print(f\"Processing {len(image_paths)} images with PCA and saving to {output_file}\")\n",
    "    all_features = []\n",
    "    \n",
    "    # Process images in batches to avoid memory issues\n",
    "    for i in range(0, len(image_paths), batch_size):\n",
    "        batch_paths = image_paths[i:i+batch_size]\n",
    "        print(f\"Processing batch {i//batch_size + 1}/{math.ceil(len(image_paths)/batch_size)}\")\n",
    "        \n",
    "        # Use rgb2flatPCA instead of rgb2emb\n",
    "        batch_features = rgb2flatPCA(batch_paths, n_components=n_components)\n",
    "        all_features.append(batch_features)\n",
    "    \n",
    "    all_features = np.vstack(all_features)\n",
    "    np.save(output_file, all_features)\n",
    "    return all_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dae38be19732add",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Use a subset of training images to pre-train PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11991b1a81b76ff4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-27T13:56:07.469607200Z",
     "start_time": "2025-02-27T13:56:07.455089900Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Pre-training PCA with a subset of training images...\")\n",
    "n_pretrain = min(5000, len(train_image_paths))  # Use at most 5000 images for pretraining\n",
    "pretrain_subset = train_image_paths[:n_pretrain]\n",
    "pretrain_pca(pretrain_subset, n_components=n_components)\n",
    "print(\"PCA pre-training completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd81221629cef95",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Process and save features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d999019a500f03",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-02-27T13:56:07.461549200Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Extracting PCA features from images...\")\n",
    "train_features_file = f'train_features_pca_{n_components}.npy'\n",
    "val_features_file = f'val_features_pca_{n_components}.npy'\n",
    "test_features_file = f'test_features_pca_{n_components}.npy'\n",
    "\n",
    "train_features = preprocess_and_save_features_pca(train_image_paths, train_features_file, n_components)\n",
    "val_features = preprocess_and_save_features_pca(val_image_paths, val_features_file, n_components)\n",
    "test_features = preprocess_and_save_features_pca(test_image_paths, test_features_file, n_components)\n",
    "\n",
    "print(\"Feature extraction completed.\")\n",
    "print(f\"Train features shape: {train_features.shape}\")\n",
    "print(f\"Validation features shape: {val_features.shape}\")\n",
    "print(f\"Test features shape: {test_features.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ec6014",
   "metadata": {},
   "source": [
    "### Standardize features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69895f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_file = f'feature_scaler_pca_{n_components}.pkl'\n",
    "\n",
    "if os.path.exists(scaler_file):\n",
    "    scaler = joblib.load(scaler_file)\n",
    "    print(\"Feature scaler loaded successfully.\")\n",
    "    \n",
    "    train_features_scaled = scaler.transform(train_features)\n",
    "    val_features_scaled = scaler.transform(val_features)\n",
    "    test_features_scaled = scaler.transform(test_features)\n",
    "else:\n",
    "    scaler = StandardScaler()\n",
    "    train_features_scaled = scaler.fit_transform(train_features)\n",
    "    val_features_scaled = scaler.transform(val_features)\n",
    "    test_features_scaled = scaler.transform(test_features)\n",
    "\n",
    "    # Save the scaler\n",
    "    joblib.dump(scaler, scaler_file)\n",
    "    print(\"Feature scaler created and saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de337a62f44c586",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f4a164009719c0",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-02-27T13:56:07.463564200Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Dense(num_classes, activation='softmax', input_shape=(n_components,),\n",
    "          kernel_regularizer=tf.keras.regularizers.l2(0.001))\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46f9041",
   "metadata": {},
   "source": [
    "### train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc634b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training the model...\")\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(train_labels), y=train_labels)\n",
    "class_weight_dict = dict(zip(np.unique(train_labels), class_weights))\n",
    "\n",
    "# Define early stopping callback\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_features_scaled, train_labels,\n",
    "    validation_data=(val_features_scaled, val_labels),\n",
    "    epochs=30,\n",
    "    batch_size=batch_size,\n",
    "    class_weight=class_weight_dict,\n",
    "    callbacks=[early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fdea336",
   "metadata": {},
   "source": [
    "### Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76d83e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file = f'softmax_age_classifier_pca_{n_components}.h5'\n",
    "model.save(model_file)\n",
    "print(f\"Model saved to {model_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db63f493a1683170",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Evaluate the model on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414fc01e63806105",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-02-27T13:56:07.467592600Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Evaluating the model on test data...\")\n",
    "test_loss, test_acc = model.evaluate(test_features_scaled, test_labels)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c494f80f",
   "metadata": {},
   "source": [
    "### Get predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2b38ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = model.predict(test_features_scaled)\n",
    "test_pred_classes = np.argmax(test_predictions, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd36e6294c8bf82d",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Visualize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed8e315739a638c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-27T13:56:07.472033Z",
     "start_time": "2025-02-27T13:56:07.471123Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Loss')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'training_history_pca_{n_components}.png')\n",
    "plt.show()\n",
    "\n",
    "# Confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "cm = confusion_matrix(test_labels, test_pred_classes)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=age_encoder.classes_,\n",
    "            yticklabels=age_encoder.classes_)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'confusion_matrix_pca_{n_components}.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d8fde4bf78fd4",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ce0d81984ed5e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-27T13:56:07.535707900Z",
     "start_time": "2025-02-27T13:56:07.472033Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Classification Report:\")\n",
    "print(classification_report(test_labels, test_pred_classes, target_names=age_encoder.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316cab2d0639b495",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### exapmle of use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a676b8e8ede7b14d",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-02-27T13:56:07.477945300Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the trained model\n",
    "model = tf.keras.models.load_model(model_file)\n",
    "print(\"Model loaded successfully.\")\n",
    "\n",
    "# Load the LabelEncoder\n",
    "age_encoder = joblib.load('age_encoder.pkl')\n",
    "print(\"Age encoder loaded successfully.\")\n",
    "\n",
    "# Load the scaler\n",
    "scaler = joblib.load(scaler_file)\n",
    "print(\"Feature scaler loaded successfully.\")\n",
    "\n",
    "def predict_age_pca(image_path, model, age_encoder, scaler, n_components=256):\n",
    "    \"\"\"\n",
    "    Predict the age range for a given face image using PCA features.\n",
    "    \n",
    "    Parameters:\n",
    "    image_path (str): Path to the image file\n",
    "    model: Trained Keras model\n",
    "    age_encoder: Trained LabelEncoder for age classes\n",
    "    scaler: Trained StandardScaler for feature normalization\n",
    "    n_components: Number of PCA components used\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (predicted_age_range, confidence)\n",
    "    \"\"\"\n",
    "    # Check if file exists\n",
    "    if not os.path.exists(image_path):\n",
    "        print(f\"Error: File {image_path} not found\")\n",
    "        return None, 0\n",
    "        \n",
    "    # Extract features using PCA\n",
    "    print(f\"Extracting PCA features from {image_path}...\")\n",
    "    features = rgb2flatPCA([image_path], n_components=n_components)\n",
    "    \n",
    "    # Check if feature extraction was successful\n",
    "    if features.size == 0:\n",
    "        print(\"Error: Feature extraction failed\")\n",
    "        return None, 0\n",
    "        \n",
    "    # Standardize features\n",
    "    print(\"Standardizing features...\")\n",
    "    features_scaled = scaler.transform(features)\n",
    "    \n",
    "    # Make prediction\n",
    "    print(\"Making prediction...\")\n",
    "    pred_probs = model.predict(features_scaled, verbose=0)[0]\n",
    "    \n",
    "    # Get predicted class\n",
    "    pred_class = np.argmax(pred_probs)\n",
    "    \n",
    "    # Convert to age range\n",
    "    pred_age_range = age_encoder.classes_[pred_class]\n",
    "    confidence = pred_probs[pred_class]\n",
    "\n",
    "    return pred_age_range, confidence\n",
    "\n",
    "# Example usage with a sample image\n",
    "# Replace with an actual path to test\n",
    "sample_image_path = test_image_paths[0]  # Using the first test image as an example\n",
    "print(f\"Using sample image: {sample_image_path}\")\n",
    "\n",
    "# Make prediction\n",
    "pred_age, confidence = predict_age_pca(sample_image_path, model, age_encoder, scaler, n_components)\n",
    "\n",
    "if pred_age is not None:\n",
    "    print(f\"Predicted age range: {pred_age} with confidence {confidence:.2f}\")\n",
    "    \n",
    "    # Display the image if possible\n",
    "    try:\n",
    "        from PIL import Image\n",
    "        img = Image.open(sample_image_path)\n",
    "        plt.figure(figsize=(4, 4))\n",
    "        plt.imshow(img)\n",
    "        plt.title(f\"Predicted: {pred_age} (Confidence: {confidence:.2f})\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(f\"Could not display image: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c706a072368c3614",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-02-27T13:56:07.479954900Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
