{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from img2vec import rgb2flatPCA, pretrain_pca\n",
    "import math\n",
    "import os\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "# Define the batch size\n",
    "batch_size = 256"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-27T12:32:58.660264400Z",
     "start_time": "2025-02-27T12:32:56.501370800Z"
    }
   },
   "id": "b7fdfda1b28f7d93"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### read the data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a63df6bf1d9882ce"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(os.path.join('..', '..', 'data', 'train.csv'))\n",
    "val_data = pd.read_csv(os.path.join('..', '..', 'data', 'val.csv'))\n",
    "test_data = pd.read_csv(os.path.join('..', '..', 'data', 'test.csv'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-27T12:32:58.692007700Z",
     "start_time": "2025-02-27T12:32:58.660264400Z"
    }
   },
   "id": "51d1d26e45f4b741"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### add the path of the images"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d541bc891ff22bf2"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "         user_id  face_id                original_image        age gender  \\\n0    9855553@N08     1581  11658657103_4485e3f5ac_o.jpg  (60, 100)      m   \n1  114841417@N06      502  12059583524_606ca96139_o.jpg   (15, 20)      m   \n2   66870968@N06     1227  11326189206_e08bdf6dfd_o.jpg   (25, 32)      m   \n3    8187011@N06      988  11133041085_e2ee5e12cb_o.jpg     (0, 2)      u   \n4  114841417@N06      485  12059753735_7141b5443c_o.jpg   (15, 20)      f   \n\n                                            img_path  \n0  ..\\..\\data\\faces\\9855553@N08\\coarse_tilt_align...  \n1  ..\\..\\data\\faces\\114841417@N06\\coarse_tilt_ali...  \n2  ..\\..\\data\\faces\\66870968@N06\\coarse_tilt_alig...  \n3  ..\\..\\data\\faces\\8187011@N06\\coarse_tilt_align...  \n4  ..\\..\\data\\faces\\114841417@N06\\coarse_tilt_ali...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user_id</th>\n      <th>face_id</th>\n      <th>original_image</th>\n      <th>age</th>\n      <th>gender</th>\n      <th>img_path</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>9855553@N08</td>\n      <td>1581</td>\n      <td>11658657103_4485e3f5ac_o.jpg</td>\n      <td>(60, 100)</td>\n      <td>m</td>\n      <td>..\\..\\data\\faces\\9855553@N08\\coarse_tilt_align...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>114841417@N06</td>\n      <td>502</td>\n      <td>12059583524_606ca96139_o.jpg</td>\n      <td>(15, 20)</td>\n      <td>m</td>\n      <td>..\\..\\data\\faces\\114841417@N06\\coarse_tilt_ali...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>66870968@N06</td>\n      <td>1227</td>\n      <td>11326189206_e08bdf6dfd_o.jpg</td>\n      <td>(25, 32)</td>\n      <td>m</td>\n      <td>..\\..\\data\\faces\\66870968@N06\\coarse_tilt_alig...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>8187011@N06</td>\n      <td>988</td>\n      <td>11133041085_e2ee5e12cb_o.jpg</td>\n      <td>(0, 2)</td>\n      <td>u</td>\n      <td>..\\..\\data\\faces\\8187011@N06\\coarse_tilt_align...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>114841417@N06</td>\n      <td>485</td>\n      <td>12059753735_7141b5443c_o.jpg</td>\n      <td>(15, 20)</td>\n      <td>f</td>\n      <td>..\\..\\data\\faces\\114841417@N06\\coarse_tilt_ali...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def construct_img_path(row):\n",
    "    return os.path.join(\"..\", \"..\", \"data\", \"faces\", row['user_id'],\n",
    "                        \"coarse_tilt_aligned_face.\" + str(row['face_id']) + \".\" + row['original_image'])\n",
    "\n",
    "\n",
    "train_data['img_path'] = train_data.apply(construct_img_path, axis=1)\n",
    "val_data['img_path'] = val_data.apply(construct_img_path, axis=1)\n",
    "test_data['img_path'] = test_data.apply(construct_img_path, axis=1)\n",
    "train_data.head(5)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-27T12:32:58.881853100Z",
     "start_time": "2025-02-27T12:32:58.692007700Z"
    }
   },
   "id": "7b1d46fc867cb10a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### add column for check if the image exists\n",
    "it will help us to detect if there is any missing image, or if there is any bug in the path construction"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8df65de30e5e25a7"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "         user_id  face_id                original_image        age gender  \\\n0    9855553@N08     1581  11658657103_4485e3f5ac_o.jpg  (60, 100)      m   \n1  114841417@N06      502  12059583524_606ca96139_o.jpg   (15, 20)      m   \n2   66870968@N06     1227  11326189206_e08bdf6dfd_o.jpg   (25, 32)      m   \n3    8187011@N06      988  11133041085_e2ee5e12cb_o.jpg     (0, 2)      u   \n4  114841417@N06      485  12059753735_7141b5443c_o.jpg   (15, 20)      f   \n\n                                            img_path  img_exists  \n0  ..\\..\\data\\faces\\9855553@N08\\coarse_tilt_align...        True  \n1  ..\\..\\data\\faces\\114841417@N06\\coarse_tilt_ali...        True  \n2  ..\\..\\data\\faces\\66870968@N06\\coarse_tilt_alig...        True  \n3  ..\\..\\data\\faces\\8187011@N06\\coarse_tilt_align...        True  \n4  ..\\..\\data\\faces\\114841417@N06\\coarse_tilt_ali...        True  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user_id</th>\n      <th>face_id</th>\n      <th>original_image</th>\n      <th>age</th>\n      <th>gender</th>\n      <th>img_path</th>\n      <th>img_exists</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>9855553@N08</td>\n      <td>1581</td>\n      <td>11658657103_4485e3f5ac_o.jpg</td>\n      <td>(60, 100)</td>\n      <td>m</td>\n      <td>..\\..\\data\\faces\\9855553@N08\\coarse_tilt_align...</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>114841417@N06</td>\n      <td>502</td>\n      <td>12059583524_606ca96139_o.jpg</td>\n      <td>(15, 20)</td>\n      <td>m</td>\n      <td>..\\..\\data\\faces\\114841417@N06\\coarse_tilt_ali...</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>66870968@N06</td>\n      <td>1227</td>\n      <td>11326189206_e08bdf6dfd_o.jpg</td>\n      <td>(25, 32)</td>\n      <td>m</td>\n      <td>..\\..\\data\\faces\\66870968@N06\\coarse_tilt_alig...</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>8187011@N06</td>\n      <td>988</td>\n      <td>11133041085_e2ee5e12cb_o.jpg</td>\n      <td>(0, 2)</td>\n      <td>u</td>\n      <td>..\\..\\data\\faces\\8187011@N06\\coarse_tilt_align...</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>114841417@N06</td>\n      <td>485</td>\n      <td>12059753735_7141b5443c_o.jpg</td>\n      <td>(15, 20)</td>\n      <td>f</td>\n      <td>..\\..\\data\\faces\\114841417@N06\\coarse_tilt_ali...</td>\n      <td>True</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['img_exists'] = train_data['img_path'].apply(os.path.exists)\n",
    "val_data['img_exists'] = val_data['img_path'].apply(os.path.exists)\n",
    "test_data['img_exists'] = test_data['img_path'].apply(os.path.exists)\n",
    "\n",
    "train_data.head(5)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-27T12:32:59.222851700Z",
     "start_time": "2025-02-27T12:32:58.881853100Z"
    }
   },
   "id": "2c915be97d0fd9eb"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age classes: ['(0, 2)' '(15, 20)' '(25, 32)' '(38, 43)' '(4, 6)' '(48, 53)' '(60, 100)'\n",
      " '(8, 23)']\n"
     ]
    },
    {
     "data": {
      "text/plain": "         user_id  face_id                original_image        age gender  \\\n0    9855553@N08     1581  11658657103_4485e3f5ac_o.jpg  (60, 100)      m   \n1  114841417@N06      502  12059583524_606ca96139_o.jpg   (15, 20)      m   \n2   66870968@N06     1227  11326189206_e08bdf6dfd_o.jpg   (25, 32)      m   \n3    8187011@N06      988  11133041085_e2ee5e12cb_o.jpg     (0, 2)      u   \n4  114841417@N06      485  12059753735_7141b5443c_o.jpg   (15, 20)      f   \n\n                                            img_path  img_exists  age_label  \n0  ..\\..\\data\\faces\\9855553@N08\\coarse_tilt_align...        True          6  \n1  ..\\..\\data\\faces\\114841417@N06\\coarse_tilt_ali...        True          1  \n2  ..\\..\\data\\faces\\66870968@N06\\coarse_tilt_alig...        True          2  \n3  ..\\..\\data\\faces\\8187011@N06\\coarse_tilt_align...        True          0  \n4  ..\\..\\data\\faces\\114841417@N06\\coarse_tilt_ali...        True          1  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user_id</th>\n      <th>face_id</th>\n      <th>original_image</th>\n      <th>age</th>\n      <th>gender</th>\n      <th>img_path</th>\n      <th>img_exists</th>\n      <th>age_label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>9855553@N08</td>\n      <td>1581</td>\n      <td>11658657103_4485e3f5ac_o.jpg</td>\n      <td>(60, 100)</td>\n      <td>m</td>\n      <td>..\\..\\data\\faces\\9855553@N08\\coarse_tilt_align...</td>\n      <td>True</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>114841417@N06</td>\n      <td>502</td>\n      <td>12059583524_606ca96139_o.jpg</td>\n      <td>(15, 20)</td>\n      <td>m</td>\n      <td>..\\..\\data\\faces\\114841417@N06\\coarse_tilt_ali...</td>\n      <td>True</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>66870968@N06</td>\n      <td>1227</td>\n      <td>11326189206_e08bdf6dfd_o.jpg</td>\n      <td>(25, 32)</td>\n      <td>m</td>\n      <td>..\\..\\data\\faces\\66870968@N06\\coarse_tilt_alig...</td>\n      <td>True</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>8187011@N06</td>\n      <td>988</td>\n      <td>11133041085_e2ee5e12cb_o.jpg</td>\n      <td>(0, 2)</td>\n      <td>u</td>\n      <td>..\\..\\data\\faces\\8187011@N06\\coarse_tilt_align...</td>\n      <td>True</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>114841417@N06</td>\n      <td>485</td>\n      <td>12059753735_7141b5443c_o.jpg</td>\n      <td>(15, 20)</td>\n      <td>f</td>\n      <td>..\\..\\data\\faces\\114841417@N06\\coarse_tilt_ali...</td>\n      <td>True</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encode age labels\n",
    "age_encoder = LabelEncoder()\n",
    "train_data['age_label'] = age_encoder.fit_transform(train_data['age'])\n",
    "val_data['age_label'] = age_encoder.transform(val_data['age'])\n",
    "test_data['age_label'] = age_encoder.transform(test_data['age'])\n",
    "num_classes = len(age_encoder.classes_)\n",
    "print(\"Age classes:\", age_encoder.classes_)\n",
    "train_data.head(5)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-27T12:32:59.270430100Z",
     "start_time": "2025-02-27T12:32:59.222851700Z"
    }
   },
   "id": "13f972e81bf1aa"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Filter out any rows where the image doesn't exist"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bcb2fa7830f47cf8"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "train_data_filtered = train_data[train_data['img_exists'] == True]\n",
    "val_data_filtered = val_data[val_data['img_exists'] == True]\n",
    "test_data_filtered = test_data[test_data['img_exists'] == True]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-27T12:32:59.334251100Z",
     "start_time": "2025-02-27T12:32:59.254584600Z"
    }
   },
   "id": "facd05e692b7b1f3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Extract image paths and labels"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7d88b72e1a3bb411"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "train_image_paths = train_data_filtered['img_path'].tolist()\n",
    "train_labels = train_data_filtered['age_label'].values\n",
    "\n",
    "val_image_paths = val_data_filtered['img_path'].tolist()\n",
    "val_labels = val_data_filtered['age_label'].values\n",
    "\n",
    "test_image_paths = test_data_filtered['img_path'].tolist()\n",
    "test_labels = test_data_filtered['age_label'].values"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-27T12:32:59.444653500Z",
     "start_time": "2025-02-27T12:32:59.270430100Z"
    }
   },
   "id": "b88ba22ca9223034"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Pre-train PCA on a large subset of training data to ensure we can use 256 components"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4e10a6213607095c"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-training PCA with 11856 images...\n",
      "Processing image 0/1000...\n",
      "Processing image 100/1000...\n",
      "Processing image 200/1000...\n",
      "Processing image 300/1000...\n",
      "Processing image 400/1000...\n",
      "Processing image 500/1000...\n",
      "Processing image 600/1000...\n",
      "Processing image 700/1000...\n",
      "Processing image 800/1000...\n",
      "Processing image 900/1000...\n",
      "PCA pre-training complete. Saved to pca_cache_rgb_256_128x128.joblib\n"
     ]
    }
   ],
   "source": [
    "pca = pretrain_pca(train_image_paths, n_components=256)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-27T12:33:09.216267800Z",
     "start_time": "2025-02-27T12:32:59.286175700Z"
    }
   },
   "id": "e8883eccb2bc85b3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Define a generator function to process images in batches"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9dae38be19732add"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def image_batch_generator(image_paths, labels, batch_size):\n",
    "    num_samples = len(image_paths)\n",
    "    num_batches = math.ceil(num_samples / batch_size)\n",
    "\n",
    "    for i in range(num_batches):\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = min((i + 1) * batch_size, num_samples)\n",
    "\n",
    "        batch_paths = image_paths[start_idx:end_idx]\n",
    "        batch_features = rgb2flatPCA(batch_paths) / 255.0\n",
    "        batch_labels = labels[start_idx:end_idx]\n",
    "\n",
    "        yield batch_features, batch_labels"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-27T12:33:09.232264300Z",
     "start_time": "2025-02-27T12:33:09.216267800Z"
    }
   },
   "id": "11991b1a81b76ff4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Process a single batch to determine input shape"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "afd81221629cef95"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing a sample batch to determine input dimensions...\n",
      "Input shape: 256\n"
     ]
    }
   ],
   "source": [
    "print(\"Processing a sample batch to determine input dimensions...\")\n",
    "sample_batch_size = min(256, len(train_image_paths))\n",
    "sample_paths = train_image_paths[:sample_batch_size]\n",
    "sample_batch = rgb2flatPCA(sample_paths)\n",
    "input_shape = sample_batch.shape[1]\n",
    "print(f\"Input shape: {input_shape}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-27T12:33:10.103102600Z",
     "start_time": "2025-02-27T12:33:09.232264300Z"
    }
   },
   "id": "74d999019a500f03"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Define the model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "de337a62f44c586"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shayg\\projects\\ML\\final_project\\venv\\lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": "\u001B[1mModel: \"sequential\"\u001B[0m\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃\u001B[1m \u001B[0m\u001B[1mLayer (type)                   \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1mOutput Shape          \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1m      Param #\u001B[0m\u001B[1m \u001B[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ dense (\u001B[38;5;33mDense\u001B[0m)                   │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m8\u001B[0m)              │         \u001B[38;5;34m2,056\u001B[0m │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)              │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,056</span> │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "\u001B[1m Total params: \u001B[0m\u001B[38;5;34m2,056\u001B[0m (8.03 KB)\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,056</span> (8.03 KB)\n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "\u001B[1m Trainable params: \u001B[0m\u001B[38;5;34m2,056\u001B[0m (8.03 KB)\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,056</span> (8.03 KB)\n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "\u001B[1m Non-trainable params: \u001B[0m\u001B[38;5;34m0\u001B[0m (0.00 B)\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Dense(num_classes, activation='softmax', input_shape=(input_shape,))\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-27T12:33:10.198582300Z",
     "start_time": "2025-02-27T12:33:10.103102600Z"
    }
   },
   "id": "91f4a164009719c0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Train the model using batches"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "def226d53aac2ea5"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model...\n",
      "Epoch 1/100\n",
      "Batch 47/47 - loss: 5.3925 - accuracy: 0.1342\n",
      "Epoch 1: loss=6.0006, accuracy=0.1236, val_loss=5.2387, val_accuracy=0.1387\n",
      "Epoch 2/100\n",
      "Batch 47/47 - loss: 4.3977 - accuracy: 0.1636\n",
      "Epoch 2: loss=4.7556, accuracy=0.1519, val_loss=4.3016, val_accuracy=0.1682\n",
      "Epoch 3/100\n",
      "Batch 47/47 - loss: 3.7175 - accuracy: 0.1985\n",
      "Epoch 3: loss=3.9580, accuracy=0.1847, val_loss=3.6562, val_accuracy=0.2025\n",
      "Epoch 4/100\n",
      "Batch 47/47 - loss: 3.2645 - accuracy: 0.2307\n",
      "Epoch 4: loss=3.4248, accuracy=0.2183, val_loss=3.2240, val_accuracy=0.2343\n",
      "Epoch 5/100\n",
      "Batch 47/47 - loss: 2.9540 - accuracy: 0.2602\n",
      "Epoch 5: loss=3.0651, accuracy=0.2492, val_loss=2.9258, val_accuracy=0.2633\n",
      "Epoch 6/100\n",
      "Batch 47/47 - loss: 2.7309 - accuracy: 0.2850\n",
      "Epoch 6: loss=2.8114, accuracy=0.2761, val_loss=2.7102, val_accuracy=0.2874\n",
      "Epoch 7/100\n",
      "Batch 47/47 - loss: 2.5635 - accuracy: 0.3054\n",
      "Epoch 7: loss=2.6244, accuracy=0.2980, val_loss=2.5478, val_accuracy=0.3073\n",
      "Epoch 8/100\n",
      "Batch 47/47 - loss: 2.4336 - accuracy: 0.3223\n",
      "Epoch 8: loss=2.4811, accuracy=0.3161, val_loss=2.4213, val_accuracy=0.3239\n",
      "Epoch 9/100\n",
      "Batch 47/47 - loss: 2.3299 - accuracy: 0.3365\n",
      "Epoch 9: loss=2.3680, accuracy=0.3314, val_loss=2.3200, val_accuracy=0.3379\n",
      "Epoch 10/100\n",
      "Batch 47/47 - loss: 2.2453 - accuracy: 0.3485\n",
      "Epoch 10: loss=2.2765, accuracy=0.3442, val_loss=2.2372, val_accuracy=0.3497\n",
      "Epoch 11/100\n",
      "Batch 47/47 - loss: 2.1749 - accuracy: 0.3587\n",
      "Epoch 11: loss=2.2010, accuracy=0.3551, val_loss=2.1681, val_accuracy=0.3597\n",
      "Epoch 12/100\n",
      "Batch 47/47 - loss: 2.1154 - accuracy: 0.3676\n",
      "Epoch 12: loss=2.1375, accuracy=0.3644, val_loss=2.1097, val_accuracy=0.3684\n",
      "Epoch 13/100\n",
      "Batch 47/47 - loss: 2.0646 - accuracy: 0.3753\n",
      "Epoch 13: loss=2.0835, accuracy=0.3725, val_loss=2.0597, val_accuracy=0.3760\n",
      "Epoch 14/100\n",
      "Batch 47/47 - loss: 2.0205 - accuracy: 0.3822\n",
      "Epoch 14: loss=2.0370, accuracy=0.3797, val_loss=2.0163, val_accuracy=0.3828\n",
      "Epoch 15/100\n",
      "Batch 47/47 - loss: 1.9820 - accuracy: 0.3883\n",
      "Epoch 15: loss=1.9964, accuracy=0.3861, val_loss=1.9783, val_accuracy=0.3888\n",
      "Epoch 16/100\n",
      "Batch 47/47 - loss: 1.9480 - accuracy: 0.3938\n",
      "Epoch 16: loss=1.9608, accuracy=0.3918, val_loss=1.9448, val_accuracy=0.3942\n",
      "Epoch 17/100\n",
      "Batch 47/47 - loss: 1.9178 - accuracy: 0.3986\n",
      "Epoch 17: loss=1.9291, accuracy=0.3968, val_loss=1.9149, val_accuracy=0.3990\n",
      "Epoch 18/100\n",
      "Batch 47/47 - loss: 1.8908 - accuracy: 0.4030\n",
      "Epoch 18: loss=1.9009, accuracy=0.4014, val_loss=1.8882, val_accuracy=0.4034\n",
      "Epoch 19/100\n",
      "Batch 47/47 - loss: 1.8664 - accuracy: 0.4070\n",
      "Epoch 19: loss=1.8756, accuracy=0.4055, val_loss=1.8641, val_accuracy=0.4073\n",
      "Epoch 20/100\n",
      "Batch 47/47 - loss: 1.8444 - accuracy: 0.4106\n",
      "Epoch 20: loss=1.8527, accuracy=0.4092, val_loss=1.8423, val_accuracy=0.4109\n",
      "Epoch 21/100\n",
      "Batch 47/47 - loss: 1.8243 - accuracy: 0.4139\n",
      "Epoch 21: loss=1.8319, accuracy=0.4127, val_loss=1.8224, val_accuracy=0.4142\n",
      "Epoch 22/100\n",
      "Batch 47/47 - loss: 1.8060 - accuracy: 0.4170\n",
      "Epoch 22: loss=1.8129, accuracy=0.4158, val_loss=1.8043, val_accuracy=0.4172\n",
      "Epoch 23/100\n",
      "Batch 47/47 - loss: 1.7892 - accuracy: 0.4197\n",
      "Epoch 23: loss=1.7956, accuracy=0.4187, val_loss=1.7876, val_accuracy=0.4200\n",
      "Epoch 24/100\n",
      "Batch 47/47 - loss: 1.7737 - accuracy: 0.4223"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 12.0 MiB for an array with shape (256, 49152) and data type uint8",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mMemoryError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[12], line 44\u001B[0m\n\u001B[0;32m     41\u001B[0m val_acc \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m     42\u001B[0m batch_count \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m---> 44\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m batch_features, batch_labels \u001B[38;5;129;01min\u001B[39;00m image_batch_generator(val_image_paths, val_labels, batch_size):\n\u001B[0;32m     45\u001B[0m     \u001B[38;5;66;03m# Evaluate on batch\u001B[39;00m\n\u001B[0;32m     46\u001B[0m     batch_val_loss, batch_val_acc \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mtest_on_batch(\n\u001B[0;32m     47\u001B[0m         batch_features,\n\u001B[0;32m     48\u001B[0m         batch_labels,\n\u001B[0;32m     49\u001B[0m     )\n\u001B[0;32m     51\u001B[0m     val_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m batch_val_loss\n",
      "Cell \u001B[1;32mIn[9], line 10\u001B[0m, in \u001B[0;36mimage_batch_generator\u001B[1;34m(image_paths, labels, batch_size)\u001B[0m\n\u001B[0;32m      7\u001B[0m end_idx \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mmin\u001B[39m((i \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m) \u001B[38;5;241m*\u001B[39m batch_size, num_samples)\n\u001B[0;32m      9\u001B[0m batch_paths \u001B[38;5;241m=\u001B[39m image_paths[start_idx:end_idx]\n\u001B[1;32m---> 10\u001B[0m batch_features \u001B[38;5;241m=\u001B[39m \u001B[43mrgb2flatPCA\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch_paths\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;241m/\u001B[39m \u001B[38;5;241m255.0\u001B[39m\n\u001B[0;32m     11\u001B[0m batch_labels \u001B[38;5;241m=\u001B[39m labels[start_idx:end_idx]\n\u001B[0;32m     13\u001B[0m \u001B[38;5;28;01myield\u001B[39;00m batch_features, batch_labels\n",
      "File \u001B[1;32m~\\projects\\ML\\final_project\\img2vec.py:49\u001B[0m, in \u001B[0;36mrgb2flatPCA\u001B[1;34m(img_paths, n_components, img_size)\u001B[0m\n\u001B[0;32m     46\u001B[0m         flattened_images\u001B[38;5;241m.\u001B[39mappend(np\u001B[38;5;241m.\u001B[39mzeros(img_size[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m*\u001B[39m img_size[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m3\u001B[39m))\n\u001B[0;32m     48\u001B[0m \u001B[38;5;66;03m# Stack all flattened images into a matrix\u001B[39;00m\n\u001B[1;32m---> 49\u001B[0m X \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstack\u001B[49m\u001B[43m(\u001B[49m\u001B[43mflattened_images\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     51\u001B[0m num_samples, n_features \u001B[38;5;241m=\u001B[39m X\u001B[38;5;241m.\u001B[39mshape\n\u001B[0;32m     52\u001B[0m min_dim \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mmin\u001B[39m(num_samples, n_features)\n",
      "File \u001B[1;32m~\\projects\\ML\\final_project\\venv\\lib\\site-packages\\numpy\\_core\\shape_base.py:455\u001B[0m, in \u001B[0;36mstack\u001B[1;34m(arrays, axis, out, dtype, casting)\u001B[0m\n\u001B[0;32m    453\u001B[0m sl \u001B[38;5;241m=\u001B[39m (\u001B[38;5;28mslice\u001B[39m(\u001B[38;5;28;01mNone\u001B[39;00m),) \u001B[38;5;241m*\u001B[39m axis \u001B[38;5;241m+\u001B[39m (_nx\u001B[38;5;241m.\u001B[39mnewaxis,)\n\u001B[0;32m    454\u001B[0m expanded_arrays \u001B[38;5;241m=\u001B[39m [arr[sl] \u001B[38;5;28;01mfor\u001B[39;00m arr \u001B[38;5;129;01min\u001B[39;00m arrays]\n\u001B[1;32m--> 455\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_nx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconcatenate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mexpanded_arrays\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43maxis\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    456\u001B[0m \u001B[43m                       \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcasting\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcasting\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mMemoryError\u001B[0m: Unable to allocate 12.0 MiB for an array with shape (256, 49152) and data type uint8"
     ]
    }
   ],
   "source": [
    "print(\"Training the model...\")\n",
    "epochs = 100\n",
    "steps_per_epoch = math.ceil(len(train_image_paths) / batch_size)\n",
    "validation_steps = math.ceil(len(val_image_paths) / batch_size)\n",
    "\n",
    "# Custom training loop\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "\n",
    "    # Training\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    batch_count = 0\n",
    "\n",
    "    for batch_features, batch_labels in image_batch_generator(train_image_paths, train_labels, batch_size):\n",
    "        # Train on batch\n",
    "        batch_history = model.train_on_batch(\n",
    "            batch_features,\n",
    "            batch_labels,\n",
    "        )\n",
    "\n",
    "        batch_loss, batch_acc = batch_history\n",
    "        train_loss += batch_loss\n",
    "        train_acc += batch_acc\n",
    "        batch_count += 1\n",
    "\n",
    "        print(f\"\\rBatch {batch_count}/{steps_per_epoch} - loss: {batch_loss:.4f} - accuracy: {batch_acc:.4f}\", end=\"\")\n",
    "\n",
    "    avg_train_loss = train_loss / batch_count\n",
    "    avg_train_acc = train_acc / batch_count\n",
    "    train_losses.append(avg_train_loss)\n",
    "    train_accuracies.append(avg_train_acc)\n",
    "\n",
    "    # Validation\n",
    "    val_loss = 0\n",
    "    val_acc = 0\n",
    "    batch_count = 0\n",
    "\n",
    "    for batch_features, batch_labels in image_batch_generator(val_image_paths, val_labels, batch_size):\n",
    "        # Evaluate on batch\n",
    "        batch_val_loss, batch_val_acc = model.test_on_batch(\n",
    "            batch_features,\n",
    "            batch_labels,\n",
    "        )\n",
    "\n",
    "        val_loss += batch_val_loss\n",
    "        val_acc += batch_val_acc\n",
    "        batch_count += 1\n",
    "\n",
    "    avg_val_loss = val_loss / batch_count\n",
    "    avg_val_acc = val_acc / batch_count\n",
    "    val_losses.append(avg_val_loss)\n",
    "    val_accuracies.append(avg_val_acc)\n",
    "\n",
    "    print(\n",
    "        f\"\\nEpoch {epoch + 1}: loss={avg_train_loss:.4f}, accuracy={avg_train_acc:.4f}, val_loss={avg_val_loss:.4f}, val_accuracy={avg_val_acc:.4f}\")\n",
    "\n",
    "    # Optional early stopping\n",
    "    if epoch > 10 and val_losses[-1] > val_losses[-2] and val_losses[-2] > val_losses[-3]:\n",
    "        print(\"Validation loss increased for 2 consecutive epochs. Early stopping.\")\n",
    "        break\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-27T12:58:52.383923900Z",
     "start_time": "2025-02-27T12:33:10.198582300Z"
    }
   },
   "id": "eaa47ef25fe556a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Evaluate the model on test data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "db63f493a1683170"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Evaluating the model on test data...\")\n",
    "test_loss = 0\n",
    "test_acc = 0\n",
    "batch_count = 0\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "for batch_features, batch_labels in image_batch_generator(test_image_paths, test_labels, batch_size):\n",
    "    # Evaluate on batch\n",
    "    batch_test_loss, batch_test_acc = model.test_on_batch(\n",
    "        batch_features,\n",
    "        batch_labels,\n",
    "    )\n",
    "\n",
    "    # Get predictions for this batch\n",
    "    batch_preds = model.predict_on_batch(batch_features)\n",
    "    batch_pred_classes = np.argmax(batch_preds, axis=1)\n",
    "\n",
    "    all_predictions.extend(batch_pred_classes)\n",
    "    all_labels.extend(batch_labels)\n",
    "\n",
    "    test_loss += batch_test_loss\n",
    "    test_acc += batch_test_acc\n",
    "    batch_count += 1\n",
    "\n",
    "avg_test_loss = test_loss / batch_count\n",
    "avg_test_acc = test_acc / batch_count\n",
    "print(f\"Test Loss: {avg_test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {avg_test_acc:.4f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-02-27T12:58:52.358862200Z"
    }
   },
   "id": "414fc01e63806105"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Visualize results"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fd36e6294c8bf82d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_accuracies, label='Train Accuracy')\n",
    "plt.plot(val_accuracies, label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Loss')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_history.png')\n",
    "plt.show()\n",
    "\n",
    "# Confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "cm = confusion_matrix(all_labels, all_predictions)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=age_encoder.classes_,\n",
    "            yticklabels=age_encoder.classes_)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.tight_layout()\n",
    "plt.savefig('confusion_matrix.png')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-02-27T12:58:52.358862200Z"
    }
   },
   "id": "8ed8e315739a638c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Classification report"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b4d8fde4bf78fd4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Classification Report:\")\n",
    "print(classification_report(all_labels, all_predictions, target_names=age_encoder.classes_))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-02-27T12:58:52.371781500Z"
    }
   },
   "id": "36ce0d81984ed5e8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Save the model and LabelEncoder"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e21e8b64d196b50"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.save('basic_softmax_age_classifier.h5')\n",
    "print(\"Model saved successfully.\")\n",
    "\n",
    "# Save the LabelEncoder\n",
    "joblib.dump(age_encoder, 'age_encoder.pkl')\n",
    "print(\"Age encoder saved successfully.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-02-27T12:58:52.377924200Z"
    }
   },
   "id": "197bae6e0a4ce665"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### exapmle of use"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "316cab2d0639b495"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import joblib\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from img2vec import rgb2flatPCA\n",
    "\n",
    "# Load the trained model\n",
    "model = tf.keras.models.load_model('basic_softmax_age_classifier.h5')\n",
    "print(\"Model loaded successfully.\")\n",
    "\n",
    "# Load the LabelEncoder\n",
    "age_encoder = joblib.load('age_encoder.pkl')\n",
    "print(\"Age encoder loaded successfully.\")\n",
    "\n",
    "\n",
    "# Function to predict age range for a new image\n",
    "def predict_age(image_path, model, age_encoder):\n",
    "    # Extract features using rgb2flatPCA\n",
    "    features = rgb2flatPCA([image_path])\n",
    "    # Normalize features\n",
    "    features = features / 255.0\n",
    "    # Make prediction\n",
    "    pred_probs = model.predict(features)[0]\n",
    "    # Get predicted class\n",
    "    pred_class = np.argmax(pred_probs)\n",
    "    # Convert to age range\n",
    "    pred_age_range = age_encoder.classes_[pred_class]\n",
    "    confidence = pred_probs[pred_class]\n",
    "\n",
    "    return pred_age_range, confidence\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "image_path = \"img.jpg\"\n",
    "pred_age, confidence = predict_age(image_path, model, age_encoder)\n",
    "print(f\"Predicted age range: {pred_age} with confidence {confidence:.2f}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-27T12:58:52.488620600Z",
     "start_time": "2025-02-27T12:58:52.394986500Z"
    }
   },
   "id": "a676b8e8ede7b14d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-02-27T12:58:52.396984Z"
    }
   },
   "id": "c706a072368c3614"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
