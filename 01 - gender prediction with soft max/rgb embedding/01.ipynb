{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from img2vec import rgb2emb\n",
    "import math\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define the batch size\n",
    "batch_size = 256"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-26T11:28:33.462468300Z",
     "start_time": "2025-02-26T11:28:25.138295300Z"
    }
   },
   "id": "b7fdfda1b28f7d93"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### read the data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a63df6bf1d9882ce"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(os.path.join('..', '..', 'data', 'train.csv'))\n",
    "val_data = pd.read_csv(os.path.join('..', '..', 'data', 'val.csv'))\n",
    "test_data = pd.read_csv(os.path.join('..', '..', 'data', 'test.csv'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-26T11:28:33.511787500Z",
     "start_time": "2025-02-26T11:28:33.466490900Z"
    }
   },
   "id": "51d1d26e45f4b741"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### add the path of the images"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d541bc891ff22bf2"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "         user_id  face_id                original_image        age gender  \\\n0    9855553@N08     1581  11658657103_4485e3f5ac_o.jpg  (60, 100)      m   \n1  114841417@N06      502  12059583524_606ca96139_o.jpg   (15, 20)      m   \n2   66870968@N06     1227  11326189206_e08bdf6dfd_o.jpg   (25, 32)      m   \n3    8187011@N06      988  11133041085_e2ee5e12cb_o.jpg     (0, 2)      u   \n4  114841417@N06      485  12059753735_7141b5443c_o.jpg   (15, 20)      f   \n\n                                            img_path  \n0  ..\\data\\faces\\9855553@N08\\coarse_tilt_aligned_...  \n1  ..\\data\\faces\\114841417@N06\\coarse_tilt_aligne...  \n2  ..\\data\\faces\\66870968@N06\\coarse_tilt_aligned...  \n3  ..\\data\\faces\\8187011@N06\\coarse_tilt_aligned_...  \n4  ..\\data\\faces\\114841417@N06\\coarse_tilt_aligne...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user_id</th>\n      <th>face_id</th>\n      <th>original_image</th>\n      <th>age</th>\n      <th>gender</th>\n      <th>img_path</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>9855553@N08</td>\n      <td>1581</td>\n      <td>11658657103_4485e3f5ac_o.jpg</td>\n      <td>(60, 100)</td>\n      <td>m</td>\n      <td>..\\data\\faces\\9855553@N08\\coarse_tilt_aligned_...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>114841417@N06</td>\n      <td>502</td>\n      <td>12059583524_606ca96139_o.jpg</td>\n      <td>(15, 20)</td>\n      <td>m</td>\n      <td>..\\data\\faces\\114841417@N06\\coarse_tilt_aligne...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>66870968@N06</td>\n      <td>1227</td>\n      <td>11326189206_e08bdf6dfd_o.jpg</td>\n      <td>(25, 32)</td>\n      <td>m</td>\n      <td>..\\data\\faces\\66870968@N06\\coarse_tilt_aligned...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>8187011@N06</td>\n      <td>988</td>\n      <td>11133041085_e2ee5e12cb_o.jpg</td>\n      <td>(0, 2)</td>\n      <td>u</td>\n      <td>..\\data\\faces\\8187011@N06\\coarse_tilt_aligned_...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>114841417@N06</td>\n      <td>485</td>\n      <td>12059753735_7141b5443c_o.jpg</td>\n      <td>(15, 20)</td>\n      <td>f</td>\n      <td>..\\data\\faces\\114841417@N06\\coarse_tilt_aligne...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def construct_img_path(row):\n",
    "    return os.path.join(\"..\", \"..\", \"data\", \"faces\", row['user_id'],\n",
    "                        \"coarse_tilt_aligned_face.\" + str(row['face_id']) + \".\" + row['original_image'])\n",
    "\n",
    "\n",
    "train_data['img_path'] = train_data.apply(construct_img_path, axis=1)\n",
    "val_data['img_path'] = val_data.apply(construct_img_path, axis=1)\n",
    "test_data['img_path'] = test_data.apply(construct_img_path, axis=1)\n",
    "train_data.head(5)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-26T11:28:33.848982800Z",
     "start_time": "2025-02-26T11:28:33.515772200Z"
    }
   },
   "id": "7b1d46fc867cb10a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### add column for check if the image exists\n",
    "it will help us to detect if there is any missing image, or if there is any bug in the path construction"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8df65de30e5e25a7"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "         user_id  face_id                original_image        age gender  \\\n0    9855553@N08     1581  11658657103_4485e3f5ac_o.jpg  (60, 100)      m   \n1  114841417@N06      502  12059583524_606ca96139_o.jpg   (15, 20)      m   \n2   66870968@N06     1227  11326189206_e08bdf6dfd_o.jpg   (25, 32)      m   \n3    8187011@N06      988  11133041085_e2ee5e12cb_o.jpg     (0, 2)      u   \n4  114841417@N06      485  12059753735_7141b5443c_o.jpg   (15, 20)      f   \n\n                                            img_path  img_exists  \n0  ..\\data\\faces\\9855553@N08\\coarse_tilt_aligned_...        True  \n1  ..\\data\\faces\\114841417@N06\\coarse_tilt_aligne...        True  \n2  ..\\data\\faces\\66870968@N06\\coarse_tilt_aligned...        True  \n3  ..\\data\\faces\\8187011@N06\\coarse_tilt_aligned_...        True  \n4  ..\\data\\faces\\114841417@N06\\coarse_tilt_aligne...        True  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user_id</th>\n      <th>face_id</th>\n      <th>original_image</th>\n      <th>age</th>\n      <th>gender</th>\n      <th>img_path</th>\n      <th>img_exists</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>9855553@N08</td>\n      <td>1581</td>\n      <td>11658657103_4485e3f5ac_o.jpg</td>\n      <td>(60, 100)</td>\n      <td>m</td>\n      <td>..\\data\\faces\\9855553@N08\\coarse_tilt_aligned_...</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>114841417@N06</td>\n      <td>502</td>\n      <td>12059583524_606ca96139_o.jpg</td>\n      <td>(15, 20)</td>\n      <td>m</td>\n      <td>..\\data\\faces\\114841417@N06\\coarse_tilt_aligne...</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>66870968@N06</td>\n      <td>1227</td>\n      <td>11326189206_e08bdf6dfd_o.jpg</td>\n      <td>(25, 32)</td>\n      <td>m</td>\n      <td>..\\data\\faces\\66870968@N06\\coarse_tilt_aligned...</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>8187011@N06</td>\n      <td>988</td>\n      <td>11133041085_e2ee5e12cb_o.jpg</td>\n      <td>(0, 2)</td>\n      <td>u</td>\n      <td>..\\data\\faces\\8187011@N06\\coarse_tilt_aligned_...</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>114841417@N06</td>\n      <td>485</td>\n      <td>12059753735_7141b5443c_o.jpg</td>\n      <td>(15, 20)</td>\n      <td>f</td>\n      <td>..\\data\\faces\\114841417@N06\\coarse_tilt_aligne...</td>\n      <td>True</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['img_exists'] = train_data['img_path'].apply(os.path.exists)\n",
    "val_data['img_exists'] = val_data['img_path'].apply(os.path.exists)\n",
    "test_data['img_exists'] = test_data['img_path'].apply(os.path.exists)\n",
    "\n",
    "train_data.head(5)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-26T11:28:35.230288700Z",
     "start_time": "2025-02-26T11:28:33.848594Z"
    }
   },
   "id": "2c915be97d0fd9eb"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age classes: ['(0, 2)' '(15, 20)' '(25, 32)' '(38, 43)' '(4, 6)' '(48, 53)' '(60, 100)'\n",
      " '(8, 23)']\n"
     ]
    },
    {
     "data": {
      "text/plain": "         user_id  face_id                original_image        age gender  \\\n0    9855553@N08     1581  11658657103_4485e3f5ac_o.jpg  (60, 100)      m   \n1  114841417@N06      502  12059583524_606ca96139_o.jpg   (15, 20)      m   \n2   66870968@N06     1227  11326189206_e08bdf6dfd_o.jpg   (25, 32)      m   \n3    8187011@N06      988  11133041085_e2ee5e12cb_o.jpg     (0, 2)      u   \n4  114841417@N06      485  12059753735_7141b5443c_o.jpg   (15, 20)      f   \n\n                                            img_path  img_exists  age_label  \n0  ..\\data\\faces\\9855553@N08\\coarse_tilt_aligned_...        True          6  \n1  ..\\data\\faces\\114841417@N06\\coarse_tilt_aligne...        True          1  \n2  ..\\data\\faces\\66870968@N06\\coarse_tilt_aligned...        True          2  \n3  ..\\data\\faces\\8187011@N06\\coarse_tilt_aligned_...        True          0  \n4  ..\\data\\faces\\114841417@N06\\coarse_tilt_aligne...        True          1  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user_id</th>\n      <th>face_id</th>\n      <th>original_image</th>\n      <th>age</th>\n      <th>gender</th>\n      <th>img_path</th>\n      <th>img_exists</th>\n      <th>age_label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>9855553@N08</td>\n      <td>1581</td>\n      <td>11658657103_4485e3f5ac_o.jpg</td>\n      <td>(60, 100)</td>\n      <td>m</td>\n      <td>..\\data\\faces\\9855553@N08\\coarse_tilt_aligned_...</td>\n      <td>True</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>114841417@N06</td>\n      <td>502</td>\n      <td>12059583524_606ca96139_o.jpg</td>\n      <td>(15, 20)</td>\n      <td>m</td>\n      <td>..\\data\\faces\\114841417@N06\\coarse_tilt_aligne...</td>\n      <td>True</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>66870968@N06</td>\n      <td>1227</td>\n      <td>11326189206_e08bdf6dfd_o.jpg</td>\n      <td>(25, 32)</td>\n      <td>m</td>\n      <td>..\\data\\faces\\66870968@N06\\coarse_tilt_aligned...</td>\n      <td>True</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>8187011@N06</td>\n      <td>988</td>\n      <td>11133041085_e2ee5e12cb_o.jpg</td>\n      <td>(0, 2)</td>\n      <td>u</td>\n      <td>..\\data\\faces\\8187011@N06\\coarse_tilt_aligned_...</td>\n      <td>True</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>114841417@N06</td>\n      <td>485</td>\n      <td>12059753735_7141b5443c_o.jpg</td>\n      <td>(15, 20)</td>\n      <td>f</td>\n      <td>..\\data\\faces\\114841417@N06\\coarse_tilt_aligne...</td>\n      <td>True</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Encode age labels\n",
    "age_encoder = LabelEncoder()\n",
    "train_data['age_label'] = age_encoder.fit_transform(train_data['age'])\n",
    "val_data['age_label'] = age_encoder.transform(val_data['age'])\n",
    "test_data['age_label'] = age_encoder.transform(test_data['age'])\n",
    "num_classes = len(age_encoder.classes_)\n",
    "print(\"Age classes:\", age_encoder.classes_)\n",
    "train_data.head(5)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-26T11:28:35.302386800Z",
     "start_time": "2025-02-26T11:28:35.234412300Z"
    }
   },
   "id": "13f972e81bf1aa"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Filter out any rows where the image doesn't exist"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bcb2fa7830f47cf8"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "train_data_filtered = train_data[train_data['img_exists'] == True]\n",
    "val_data_filtered = val_data[val_data['img_exists'] == True]\n",
    "test_data_filtered = test_data[test_data['img_exists'] == True]\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-26T11:28:35.367836400Z",
     "start_time": "2025-02-26T11:28:35.264589500Z"
    }
   },
   "id": "facd05e692b7b1f3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Define a generator function to process images in batches"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3af64d1ee4038b96"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def image_batch_generator(image_paths, labels, batch_size):\n",
    "    num_samples = len(image_paths)\n",
    "    num_batches = math.ceil(num_samples / batch_size)\n",
    "\n",
    "    for i in range(num_batches):\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = min((i + 1) * batch_size, num_samples)\n",
    "\n",
    "        batch_paths = image_paths[start_idx:end_idx]\n",
    "        batch_features = rgb2flat(batch_paths) / 255.0  # Normalize to [0,1]\n",
    "        batch_labels = labels[start_idx:end_idx]\n",
    "\n",
    "        yield batch_features, batch_labels\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-26T11:28:35.375771Z",
     "start_time": "2025-02-26T11:28:35.284817300Z"
    }
   },
   "id": "613ac4858cc2e830"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Extract image paths and labels"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "de85480f4cd99025"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "train_image_paths = train_data_filtered['img_path'].tolist()\n",
    "train_labels = train_data_filtered['age_label'].values\n",
    "\n",
    "val_image_paths = val_data_filtered['img_path'].tolist()\n",
    "val_labels = val_data_filtered['age_label'].values\n",
    "\n",
    "test_image_paths = test_data_filtered['img_path'].tolist()\n",
    "test_labels = test_data_filtered['age_label'].values"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-26T11:28:35.377782900Z",
     "start_time": "2025-02-26T11:28:35.288312600Z"
    }
   },
   "id": "c2fc8e2f5e6e670a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Process a single batch to determine input shape"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "afd81221629cef95"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing a sample batch to determine input dimensions...\n",
      "Input shape: 49152\n"
     ]
    }
   ],
   "source": [
    "print(\"Processing a sample batch to determine input dimensions...\")\n",
    "sample_batch_size = min(10, len(train_image_paths))\n",
    "sample_paths = train_image_paths[:sample_batch_size]\n",
    "sample_batch = rgb2flat(sample_paths)\n",
    "input_shape = sample_batch.shape[1]\n",
    "print(f\"Input shape: {input_shape}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-26T11:28:35.452189100Z",
     "start_time": "2025-02-26T11:28:35.306856100Z"
    }
   },
   "id": "74d999019a500f03"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Define the model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "de337a62f44c586"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shayg\\projects\\ML\\final_project\\venv\\lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": "\u001B[1mModel: \"sequential\"\u001B[0m\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃\u001B[1m \u001B[0m\u001B[1mLayer (type)                   \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1mOutput Shape          \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1m      Param #\u001B[0m\u001B[1m \u001B[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ dense (\u001B[38;5;33mDense\u001B[0m)                   │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m8\u001B[0m)              │       \u001B[38;5;34m393,224\u001B[0m │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)              │       <span style=\"color: #00af00; text-decoration-color: #00af00\">393,224</span> │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "\u001B[1m Total params: \u001B[0m\u001B[38;5;34m393,224\u001B[0m (1.50 MB)\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">393,224</span> (1.50 MB)\n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "\u001B[1m Trainable params: \u001B[0m\u001B[38;5;34m393,224\u001B[0m (1.50 MB)\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">393,224</span> (1.50 MB)\n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "\u001B[1m Non-trainable params: \u001B[0m\u001B[38;5;34m0\u001B[0m (0.00 B)\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Dense(num_classes, activation='softmax', input_shape=(input_shape,))\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-26T11:28:35.571819Z",
     "start_time": "2025-02-26T11:28:35.375771Z"
    }
   },
   "id": "91f4a164009719c0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Train the model using batches"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "def226d53aac2ea5"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model...\n",
      "Epoch 1/100\n",
      "Batch 47/47 - loss: 9.7400 - accuracy: 0.21415\n",
      "Epoch 1: loss=12.9540, accuracy=0.1938, val_loss=8.8671, val_accuracy=0.2233\n",
      "Epoch 2/100\n",
      "Batch 47/47 - loss: 5.4456 - accuracy: 0.2799\n",
      "Epoch 2: loss=6.5671, accuracy=0.2575, val_loss=5.2329, val_accuracy=0.2835\n",
      "Epoch 3/100\n",
      "Batch 47/47 - loss: 4.1222 - accuracy: 0.3107\n",
      "Epoch 3: loss=4.5328, accuracy=0.3000, val_loss=4.0403, val_accuracy=0.3118\n",
      "Epoch 4/100\n",
      "Batch 47/47 - loss: 3.5046 - accuracy: 0.3262\n",
      "Epoch 4: loss=3.7184, accuracy=0.3196, val_loss=3.4520, val_accuracy=0.3290\n",
      "Epoch 5/100\n",
      "Batch 47/47 - loss: 3.2668 - accuracy: 0.3325\n",
      "Epoch 5: loss=3.3342, accuracy=0.3321, val_loss=3.2354, val_accuracy=0.3340\n",
      "Epoch 6/100\n",
      "Batch 47/47 - loss: 3.0061 - accuracy: 0.3445\n",
      "Epoch 6: loss=3.1080, accuracy=0.3389, val_loss=2.9810, val_accuracy=0.3459\n",
      "Epoch 7/100\n",
      "Batch 47/47 - loss: 2.8067 - accuracy: 0.3554\n",
      "Epoch 7: loss=2.8792, accuracy=0.3512, val_loss=2.7864, val_accuracy=0.3570\n",
      "Epoch 8/100\n",
      "Batch 47/47 - loss: 2.6627 - accuracy: 0.3661\n",
      "Epoch 8: loss=2.7076, accuracy=0.3631, val_loss=2.6580, val_accuracy=0.3672\n",
      "Epoch 9/100\n",
      "Batch 47/47 - loss: 2.6042 - accuracy: 0.3742\n",
      "Epoch 9: loss=2.6330, accuracy=0.3713, val_loss=2.5937, val_accuracy=0.3747\n",
      "Epoch 10/100\n",
      "Batch 47/47 - loss: 2.5643 - accuracy: 0.3769\n",
      "Epoch 10: loss=2.5526, accuracy=0.3774, val_loss=2.5887, val_accuracy=0.3755\n",
      "Epoch 11/100\n",
      "Batch 47/47 - loss: 2.6460 - accuracy: 0.3734\n",
      "Epoch 11: loss=2.6510, accuracy=0.3730, val_loss=2.6406, val_accuracy=0.3742\n",
      "Epoch 12/100\n",
      "Batch 47/47 - loss: 2.5820 - accuracy: 0.3772\n",
      "Epoch 12: loss=2.6076, accuracy=0.3763, val_loss=2.5750, val_accuracy=0.3776\n",
      "Epoch 13/100\n",
      "Batch 47/47 - loss: 2.5169 - accuracy: 0.3814\n",
      "Epoch 13: loss=2.5440, accuracy=0.3793, val_loss=2.5093, val_accuracy=0.3820\n",
      "Epoch 14/100\n",
      "Batch 47/47 - loss: 2.4439 - accuracy: 0.3881\n",
      "Epoch 14: loss=2.4728, accuracy=0.3852, val_loss=2.4365, val_accuracy=0.3886\n",
      "Epoch 15/100\n",
      "Batch 47/47 - loss: 2.3764 - accuracy: 0.3946\n",
      "Epoch 15: loss=2.4028, accuracy=0.3918, val_loss=2.3700, val_accuracy=0.3951\n",
      "Epoch 16/100\n",
      "Batch 47/47 - loss: 2.3174 - accuracy: 0.4004\n",
      "Epoch 16: loss=2.3398, accuracy=0.3981, val_loss=2.3119, val_accuracy=0.4008\n",
      "Epoch 17/100\n",
      "Batch 47/47 - loss: 2.2690 - accuracy: 0.4050\n",
      "Epoch 17: loss=2.2873, accuracy=0.4032, val_loss=2.2641, val_accuracy=0.4054\n",
      "Epoch 18/100\n",
      "Batch 47/47 - loss: 2.2188 - accuracy: 0.4105\n",
      "Epoch 18: loss=2.2384, accuracy=0.4083, val_loss=2.2144, val_accuracy=0.4109\n",
      "Epoch 19/100\n",
      "Batch 47/47 - loss: 2.1800 - accuracy: 0.4144\n",
      "Epoch 19: loss=2.1932, accuracy=0.4132, val_loss=2.1780, val_accuracy=0.4146\n",
      "Epoch 20/100\n",
      "Batch 47/47 - loss: 2.1746 - accuracy: 0.4155\n",
      "Epoch 20: loss=2.1734, accuracy=0.4155, val_loss=2.1760, val_accuracy=0.4149\n",
      "Epoch 21/100\n",
      "Batch 47/47 - loss: 2.1556 - accuracy: 0.4166\n",
      "Epoch 21: loss=2.1670, accuracy=0.4155, val_loss=2.1528, val_accuracy=0.4170\n",
      "Epoch 22/100\n",
      "Batch 47/47 - loss: 2.1299 - accuracy: 0.4190\n",
      "Epoch 22: loss=2.1389, accuracy=0.4185, val_loss=2.1284, val_accuracy=0.4191\n",
      "Epoch 23/100\n",
      "Batch 47/47 - loss: 2.1168 - accuracy: 0.4206\n",
      "Epoch 23: loss=2.1217, accuracy=0.4198, val_loss=2.1162, val_accuracy=0.4206\n",
      "Epoch 24/100\n",
      "Batch 47/47 - loss: 2.1013 - accuracy: 0.4230\n",
      "Epoch 24: loss=2.1066, accuracy=0.4222, val_loss=2.1064, val_accuracy=0.4221\n",
      "Epoch 25/100\n",
      "Batch 47/47 - loss: 2.0958 - accuracy: 0.4243\n",
      "Epoch 25: loss=2.1054, accuracy=0.4229, val_loss=2.0939, val_accuracy=0.4245\n",
      "Epoch 26/100\n",
      "Batch 47/47 - loss: 2.0770 - accuracy: 0.4264\n",
      "Epoch 26: loss=2.0836, accuracy=0.4257, val_loss=2.0761, val_accuracy=0.4264\n",
      "Epoch 27/100\n",
      "Batch 47/47 - loss: 2.0532 - accuracy: 0.4298\n",
      "Epoch 27: loss=2.0643, accuracy=0.4281, val_loss=2.0509, val_accuracy=0.4300\n",
      "Epoch 28/100\n",
      "Batch 47/47 - loss: 2.0247 - accuracy: 0.4341\n",
      "Epoch 28: loss=2.0365, accuracy=0.4323, val_loss=2.0230, val_accuracy=0.4343\n",
      "Epoch 29/100\n",
      "Batch 47/47 - loss: 1.9983 - accuracy: 0.4384\n",
      "Epoch 29: loss=2.0096, accuracy=0.4365, val_loss=1.9969, val_accuracy=0.4386\n",
      "Epoch 30/100\n",
      "Batch 47/47 - loss: 1.9741 - accuracy: 0.4423\n",
      "Epoch 30: loss=1.9845, accuracy=0.4406, val_loss=1.9729, val_accuracy=0.4424\n",
      "Epoch 31/100\n",
      "Batch 47/47 - loss: 1.9526 - accuracy: 0.4456\n",
      "Epoch 31: loss=1.9620, accuracy=0.4441, val_loss=1.9516, val_accuracy=0.4457\n",
      "Epoch 32/100\n",
      "Batch 47/47 - loss: 1.9321 - accuracy: 0.4489\n",
      "Epoch 32: loss=1.9411, accuracy=0.4474, val_loss=1.9312, val_accuracy=0.4490\n",
      "Epoch 33/100\n",
      "Batch 47/47 - loss: 1.9127 - accuracy: 0.4520\n",
      "Epoch 33: loss=1.9214, accuracy=0.4505, val_loss=1.9121, val_accuracy=0.4520\n",
      "Epoch 34/100\n",
      "Batch 47/47 - loss: 1.8949 - accuracy: 0.4548\n",
      "Epoch 34: loss=1.9031, accuracy=0.4534, val_loss=1.8944, val_accuracy=0.4548\n",
      "Epoch 35/100\n",
      "Batch 47/47 - loss: 1.8787 - accuracy: 0.4575\n",
      "Epoch 35: loss=1.8862, accuracy=0.4562, val_loss=1.8782, val_accuracy=0.4576\n",
      "Epoch 36/100\n",
      "Batch 47/47 - loss: 1.9243 - accuracy: 0.4561\n",
      "Epoch 36: loss=1.8887, accuracy=0.4574, val_loss=1.9363, val_accuracy=0.4558\n",
      "Epoch 37/100\n",
      "Batch 23/47 - loss: 1.9709 - accuracy: 0.4544"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 96.0 MiB for an array with shape (256, 49152) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mMemoryError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[11], line 20\u001B[0m\n\u001B[0;32m     17\u001B[0m train_acc \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m     18\u001B[0m batch_count \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m---> 20\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m batch_features, batch_labels \u001B[38;5;129;01min\u001B[39;00m image_batch_generator(train_image_paths, train_labels, batch_size):\n\u001B[0;32m     21\u001B[0m     \u001B[38;5;66;03m# Train on batch\u001B[39;00m\n\u001B[0;32m     22\u001B[0m     batch_history \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mtrain_on_batch(\n\u001B[0;32m     23\u001B[0m         batch_features,\n\u001B[0;32m     24\u001B[0m         batch_labels,\n\u001B[0;32m     25\u001B[0m     )\n\u001B[0;32m     27\u001B[0m     batch_loss, batch_acc \u001B[38;5;241m=\u001B[39m batch_history\n",
      "Cell \u001B[1;32mIn[7], line 10\u001B[0m, in \u001B[0;36mimage_batch_generator\u001B[1;34m(image_paths, labels, batch_size)\u001B[0m\n\u001B[0;32m      7\u001B[0m end_idx \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mmin\u001B[39m((i \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m) \u001B[38;5;241m*\u001B[39m batch_size, num_samples)\n\u001B[0;32m      9\u001B[0m batch_paths \u001B[38;5;241m=\u001B[39m image_paths[start_idx:end_idx]\n\u001B[1;32m---> 10\u001B[0m batch_features \u001B[38;5;241m=\u001B[39m \u001B[43mrgb2flat\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch_paths\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m/\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m255.0\u001B[39;49m  \u001B[38;5;66;03m# Normalize to [0,1]\u001B[39;00m\n\u001B[0;32m     11\u001B[0m batch_labels \u001B[38;5;241m=\u001B[39m labels[start_idx:end_idx]\n\u001B[0;32m     13\u001B[0m \u001B[38;5;28;01myield\u001B[39;00m batch_features, batch_labels\n",
      "\u001B[1;31mMemoryError\u001B[0m: Unable to allocate 96.0 MiB for an array with shape (256, 49152) and data type float64"
     ]
    }
   ],
   "source": [
    "print(\"Training the model...\")\n",
    "epochs = 100\n",
    "steps_per_epoch = math.ceil(len(train_image_paths) / batch_size)\n",
    "validation_steps = math.ceil(len(val_image_paths) / batch_size)\n",
    "\n",
    "# Custom training loop\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "\n",
    "    # Training\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    batch_count = 0\n",
    "\n",
    "    for batch_features, batch_labels in image_batch_generator(train_image_paths, train_labels, batch_size):\n",
    "        # Train on batch\n",
    "        batch_history = model.train_on_batch(\n",
    "            batch_features,\n",
    "            batch_labels,\n",
    "        )\n",
    "\n",
    "        batch_loss, batch_acc = batch_history\n",
    "        train_loss += batch_loss\n",
    "        train_acc += batch_acc\n",
    "        batch_count += 1\n",
    "\n",
    "        print(f\"\\rBatch {batch_count}/{steps_per_epoch} - loss: {batch_loss:.4f} - accuracy: {batch_acc:.4f}\", end=\"\")\n",
    "\n",
    "    avg_train_loss = train_loss / batch_count\n",
    "    avg_train_acc = train_acc / batch_count\n",
    "    train_losses.append(avg_train_loss)\n",
    "    train_accuracies.append(avg_train_acc)\n",
    "\n",
    "    # Validation\n",
    "    val_loss = 0\n",
    "    val_acc = 0\n",
    "    batch_count = 0\n",
    "\n",
    "    for batch_features, batch_labels in image_batch_generator(val_image_paths, val_labels, batch_size):\n",
    "        # Evaluate on batch\n",
    "        batch_val_loss, batch_val_acc = model.test_on_batch(\n",
    "            batch_features,\n",
    "            batch_labels,\n",
    "        )\n",
    "\n",
    "        val_loss += batch_val_loss\n",
    "        val_acc += batch_val_acc\n",
    "        batch_count += 1\n",
    "\n",
    "    avg_val_loss = val_loss / batch_count\n",
    "    avg_val_acc = val_acc / batch_count\n",
    "    val_losses.append(avg_val_loss)\n",
    "    val_accuracies.append(avg_val_acc)\n",
    "\n",
    "    print(\n",
    "        f\"\\nEpoch {epoch + 1}: loss={avg_train_loss:.4f}, accuracy={avg_train_acc:.4f}, val_loss={avg_val_loss:.4f}, val_accuracy={avg_val_acc:.4f}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-26T12:21:58.292475800Z",
     "start_time": "2025-02-26T11:28:35.492041300Z"
    }
   },
   "id": "eaa47ef25fe556a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Evaluate the model on test data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "db63f493a1683170"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Evaluating the model on test data...\")\n",
    "test_loss = 0\n",
    "test_acc = 0\n",
    "batch_count = 0\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "for batch_features, batch_labels in image_batch_generator(test_image_paths, test_labels, batch_size):\n",
    "    # Evaluate on batch\n",
    "    batch_test_loss, batch_test_acc = model.test_on_batch(\n",
    "        batch_features,\n",
    "        batch_labels,\n",
    "    )\n",
    "\n",
    "    # Get predictions for this batch\n",
    "    batch_preds = model.predict_on_batch(batch_features)\n",
    "    batch_pred_classes = np.argmax(batch_preds, axis=1)\n",
    "\n",
    "    all_predictions.extend(batch_pred_classes)\n",
    "    all_labels.extend(batch_labels)\n",
    "\n",
    "    test_loss += batch_test_loss\n",
    "    test_acc += batch_test_acc\n",
    "    batch_count += 1\n",
    "\n",
    "avg_test_loss = test_loss / batch_count\n",
    "avg_test_acc = test_acc / batch_count\n",
    "print(f\"Test Loss: {avg_test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {avg_test_acc:.4f}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-26T12:21:58.555352Z",
     "start_time": "2025-02-26T12:21:58.318638100Z"
    }
   },
   "id": "414fc01e63806105"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Visualize results"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fd36e6294c8bf82d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_accuracies, label='Train Accuracy')\n",
    "plt.plot(val_accuracies, label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Loss')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_history.png')\n",
    "plt.show()\n",
    "\n",
    "# Confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "cm = confusion_matrix(all_labels, all_predictions)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=age_encoder.classes_,\n",
    "            yticklabels=age_encoder.classes_)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.tight_layout()\n",
    "plt.savefig('confusion_matrix.png')\n",
    "plt.show()\n",
    "\n",
    "# Classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(all_labels, all_predictions, target_names=age_encoder.classes_))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-02-26T12:21:58.320662400Z"
    }
   },
   "id": "8ed8e315739a638c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Save the model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e21e8b64d196b50"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.save('basic_softmax_age_classifier.h5')\n",
    "print(\"Model saved successfully.\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-02-26T12:21:58.325210700Z"
    }
   },
   "id": "197bae6e0a4ce665"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Function to predict age range for a new image\n",
    "def predict_age(image_path, model, age_encoder):\n",
    "    # Extract features using rgb2flat\n",
    "    features = rgb2emb([image_path])\n",
    "    # Normalize features\n",
    "    features = features / 255.0\n",
    "    # Make prediction\n",
    "    pred_probs = model.predict(features)[0]\n",
    "    # Get predicted class\n",
    "    pred_class = np.argmax(pred_probs)\n",
    "    # Convert to age range\n",
    "    pred_age_range = age_encoder.classes_[pred_class]\n",
    "    confidence = pred_probs[pred_class]\n",
    "\n",
    "    return pred_age_range, confidence\n",
    "\n",
    "# Example usage:\n",
    "# image_path = \"path/to/new/face/image.jpg\"\n",
    "# pred_age, confidence = predict_age(image_path, model, age_encoder)\n",
    "# print(f\"Predicted age range: {pred_age} with confidence {confidence:.2f}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-02-26T12:21:58.359026500Z"
    }
   },
   "id": "a676b8e8ede7b14d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-02-26T12:21:58.364586500Z"
    }
   },
   "id": "c706a072368c3614"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
