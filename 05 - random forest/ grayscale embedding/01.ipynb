{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-04T08:10:18.151168Z",
     "start_time": "2025-03-04T08:10:18.113998Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import math\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from joblib import dump, load\n",
    "\n",
    "# Define the batch size for data processing\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### read the data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fbf67410f652b98c"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(os.path.join('..', '..', 'data', 'train.csv'))\n",
    "val_data = pd.read_csv(os.path.join('..', '..', 'data', 'val.csv'))\n",
    "test_data = pd.read_csv(os.path.join('..', '..', 'data', 'test.csv'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-04T08:10:20.566647Z",
     "start_time": "2025-03-04T08:10:18.159556Z"
    }
   },
   "id": "8ba693e7da77017",
   "execution_count": 26
  },
  {
   "cell_type": "markdown",
   "source": [
    "### add the path of the images"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a3bca6182fbb46bc"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "         user_id  face_id                original_image        age gender  \\\n0    9855553@N08     1581  11658657103_4485e3f5ac_o.jpg  (60, 100)      m   \n1  114841417@N06      502  12059583524_606ca96139_o.jpg   (15, 20)      m   \n2   66870968@N06     1227  11326189206_e08bdf6dfd_o.jpg   (25, 32)      m   \n3    8187011@N06      988  11133041085_e2ee5e12cb_o.jpg     (0, 2)      u   \n4  114841417@N06      485  12059753735_7141b5443c_o.jpg   (15, 20)      f   \n\n                                            img_path  \n0  ../../data/faces/9855553@N08/coarse_tilt_align...  \n1  ../../data/faces/114841417@N06/coarse_tilt_ali...  \n2  ../../data/faces/66870968@N06/coarse_tilt_alig...  \n3  ../../data/faces/8187011@N06/coarse_tilt_align...  \n4  ../../data/faces/114841417@N06/coarse_tilt_ali...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user_id</th>\n      <th>face_id</th>\n      <th>original_image</th>\n      <th>age</th>\n      <th>gender</th>\n      <th>img_path</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>9855553@N08</td>\n      <td>1581</td>\n      <td>11658657103_4485e3f5ac_o.jpg</td>\n      <td>(60, 100)</td>\n      <td>m</td>\n      <td>../../data/faces/9855553@N08/coarse_tilt_align...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>114841417@N06</td>\n      <td>502</td>\n      <td>12059583524_606ca96139_o.jpg</td>\n      <td>(15, 20)</td>\n      <td>m</td>\n      <td>../../data/faces/114841417@N06/coarse_tilt_ali...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>66870968@N06</td>\n      <td>1227</td>\n      <td>11326189206_e08bdf6dfd_o.jpg</td>\n      <td>(25, 32)</td>\n      <td>m</td>\n      <td>../../data/faces/66870968@N06/coarse_tilt_alig...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>8187011@N06</td>\n      <td>988</td>\n      <td>11133041085_e2ee5e12cb_o.jpg</td>\n      <td>(0, 2)</td>\n      <td>u</td>\n      <td>../../data/faces/8187011@N06/coarse_tilt_align...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>114841417@N06</td>\n      <td>485</td>\n      <td>12059753735_7141b5443c_o.jpg</td>\n      <td>(15, 20)</td>\n      <td>f</td>\n      <td>../../data/faces/114841417@N06/coarse_tilt_ali...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def construct_img_path(row):\n",
    "    return os.path.join(\"..\", \"..\", \"data\", \"faces\", row['user_id'],\n",
    "                        \"coarse_tilt_aligned_face.\" + str(row['face_id']) + \".\" + row['original_image'])\n",
    "\n",
    "\n",
    "train_data['img_path'] = train_data.apply(construct_img_path, axis=1)\n",
    "val_data['img_path'] = val_data.apply(construct_img_path, axis=1)\n",
    "test_data['img_path'] = test_data.apply(construct_img_path, axis=1)\n",
    "train_data.head(5)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-04T08:10:20.712752Z",
     "start_time": "2025-03-04T08:10:20.570667Z"
    }
   },
   "id": "15bde55648f6cbcf",
   "execution_count": 27
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### add column for check if the image exists\n",
    "it will help us to detect if there is any missing image, or if there is any bug in the path construction"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "34ccbaf87304f93f"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "         user_id  face_id                original_image        age gender  \\\n0    9855553@N08     1581  11658657103_4485e3f5ac_o.jpg  (60, 100)      m   \n1  114841417@N06      502  12059583524_606ca96139_o.jpg   (15, 20)      m   \n2   66870968@N06     1227  11326189206_e08bdf6dfd_o.jpg   (25, 32)      m   \n3    8187011@N06      988  11133041085_e2ee5e12cb_o.jpg     (0, 2)      u   \n4  114841417@N06      485  12059753735_7141b5443c_o.jpg   (15, 20)      f   \n\n                                            img_path  img_exists  \n0  ../../data/faces/9855553@N08/coarse_tilt_align...        True  \n1  ../../data/faces/114841417@N06/coarse_tilt_ali...        True  \n2  ../../data/faces/66870968@N06/coarse_tilt_alig...        True  \n3  ../../data/faces/8187011@N06/coarse_tilt_align...        True  \n4  ../../data/faces/114841417@N06/coarse_tilt_ali...        True  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user_id</th>\n      <th>face_id</th>\n      <th>original_image</th>\n      <th>age</th>\n      <th>gender</th>\n      <th>img_path</th>\n      <th>img_exists</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>9855553@N08</td>\n      <td>1581</td>\n      <td>11658657103_4485e3f5ac_o.jpg</td>\n      <td>(60, 100)</td>\n      <td>m</td>\n      <td>../../data/faces/9855553@N08/coarse_tilt_align...</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>114841417@N06</td>\n      <td>502</td>\n      <td>12059583524_606ca96139_o.jpg</td>\n      <td>(15, 20)</td>\n      <td>m</td>\n      <td>../../data/faces/114841417@N06/coarse_tilt_ali...</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>66870968@N06</td>\n      <td>1227</td>\n      <td>11326189206_e08bdf6dfd_o.jpg</td>\n      <td>(25, 32)</td>\n      <td>m</td>\n      <td>../../data/faces/66870968@N06/coarse_tilt_alig...</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>8187011@N06</td>\n      <td>988</td>\n      <td>11133041085_e2ee5e12cb_o.jpg</td>\n      <td>(0, 2)</td>\n      <td>u</td>\n      <td>../../data/faces/8187011@N06/coarse_tilt_align...</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>114841417@N06</td>\n      <td>485</td>\n      <td>12059753735_7141b5443c_o.jpg</td>\n      <td>(15, 20)</td>\n      <td>f</td>\n      <td>../../data/faces/114841417@N06/coarse_tilt_ali...</td>\n      <td>True</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['img_exists'] = train_data['img_path'].apply(os.path.exists)\n",
    "val_data['img_exists'] = val_data['img_path'].apply(os.path.exists)\n",
    "test_data['img_exists'] = test_data['img_path'].apply(os.path.exists)\n",
    "\n",
    "train_data.head(5)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-04T08:10:22.029762Z",
     "start_time": "2025-03-04T08:10:20.711438Z"
    }
   },
   "id": "3c493e5d73810fb1",
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gender_age classes: ['f_(0, 2)' 'f_(15, 20)' 'f_(25, 32)' 'f_(38, 43)' 'f_(4, 6)' 'f_(48, 53)'\n",
      " 'f_(60, 100)' 'f_(8, 23)' 'm_(0, 2)' 'm_(15, 20)' 'm_(25, 32)'\n",
      " 'm_(38, 43)' 'm_(4, 6)' 'm_(48, 53)' 'm_(60, 100)' 'm_(8, 23)' 'u_(0, 2)'\n",
      " 'u_(25, 32)' 'u_(60, 100)' 'u_(8, 23)']\n"
     ]
    },
    {
     "data": {
      "text/plain": "         user_id  face_id                original_image        age gender  \\\n0    9855553@N08     1581  11658657103_4485e3f5ac_o.jpg  (60, 100)      m   \n1  114841417@N06      502  12059583524_606ca96139_o.jpg   (15, 20)      m   \n2   66870968@N06     1227  11326189206_e08bdf6dfd_o.jpg   (25, 32)      m   \n3    8187011@N06      988  11133041085_e2ee5e12cb_o.jpg     (0, 2)      u   \n4  114841417@N06      485  12059753735_7141b5443c_o.jpg   (15, 20)      f   \n\n                                            img_path  img_exists  \\\n0  ../../data/faces/9855553@N08/coarse_tilt_align...        True   \n1  ../../data/faces/114841417@N06/coarse_tilt_ali...        True   \n2  ../../data/faces/66870968@N06/coarse_tilt_alig...        True   \n3  ../../data/faces/8187011@N06/coarse_tilt_align...        True   \n4  ../../data/faces/114841417@N06/coarse_tilt_ali...        True   \n\n   gender_age_label  \n0                14  \n1                 9  \n2                10  \n3                16  \n4                 1  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user_id</th>\n      <th>face_id</th>\n      <th>original_image</th>\n      <th>age</th>\n      <th>gender</th>\n      <th>img_path</th>\n      <th>img_exists</th>\n      <th>gender_age_label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>9855553@N08</td>\n      <td>1581</td>\n      <td>11658657103_4485e3f5ac_o.jpg</td>\n      <td>(60, 100)</td>\n      <td>m</td>\n      <td>../../data/faces/9855553@N08/coarse_tilt_align...</td>\n      <td>True</td>\n      <td>14</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>114841417@N06</td>\n      <td>502</td>\n      <td>12059583524_606ca96139_o.jpg</td>\n      <td>(15, 20)</td>\n      <td>m</td>\n      <td>../../data/faces/114841417@N06/coarse_tilt_ali...</td>\n      <td>True</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>66870968@N06</td>\n      <td>1227</td>\n      <td>11326189206_e08bdf6dfd_o.jpg</td>\n      <td>(25, 32)</td>\n      <td>m</td>\n      <td>../../data/faces/66870968@N06/coarse_tilt_alig...</td>\n      <td>True</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>8187011@N06</td>\n      <td>988</td>\n      <td>11133041085_e2ee5e12cb_o.jpg</td>\n      <td>(0, 2)</td>\n      <td>u</td>\n      <td>../../data/faces/8187011@N06/coarse_tilt_align...</td>\n      <td>True</td>\n      <td>16</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>114841417@N06</td>\n      <td>485</td>\n      <td>12059753735_7141b5443c_o.jpg</td>\n      <td>(15, 20)</td>\n      <td>f</td>\n      <td>../../data/faces/114841417@N06/coarse_tilt_ali...</td>\n      <td>True</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def encode_gender_age(train_data, val_data, test_data):\n",
    "    \"\"\"Encodes gender and age combinations into a single label, handling unseen labels.\"\"\"\n",
    "\n",
    "    train_data['gender_age_combined'] = train_data['gender'].astype(str) + '_' + train_data['age'].astype(str)\n",
    "    val_data['gender_age_combined'] = val_data['gender'].astype(str) + '_' + val_data['age'].astype(str)\n",
    "    test_data['gender_age_combined'] = test_data['gender'].astype(str) + '_' + test_data['age'].astype(str)\n",
    "\n",
    "    gender_age_encoder = LabelEncoder()\n",
    "    train_data['gender_age_label'] = gender_age_encoder.fit_transform(train_data['gender_age_combined'])\n",
    "\n",
    "    # Function to handle unseen labels\n",
    "    def transform_with_unknown(data, encoder):\n",
    "        known_classes = set(encoder.classes_)\n",
    "        data['gender_age_label'] = data['gender_age_combined'].apply(\n",
    "            lambda x: encoder.transform([x])[0] if x in known_classes else -1\n",
    "        ) # assign -1 to unseen label.\n",
    "        return data\n",
    "\n",
    "    val_data = transform_with_unknown(val_data, gender_age_encoder)\n",
    "    test_data = transform_with_unknown(test_data, gender_age_encoder)\n",
    "\n",
    "    num_classes = len(gender_age_encoder.classes_)\n",
    "\n",
    "    train_data.drop('gender_age_combined', axis=1, inplace=True)\n",
    "    val_data.drop('gender_age_combined', axis=1, inplace=True)\n",
    "    test_data.drop('gender_age_combined', axis=1, inplace=True)\n",
    "\n",
    "    return train_data, val_data, test_data, num_classes, gender_age_encoder\n",
    "\n",
    "train_data, val_data, test_data, num_classes, gender_age_encoder = encode_gender_age(train_data, val_data, test_data)\n",
    "\n",
    "num_classes = len(gender_age_encoder.classes_)\n",
    "print(\"gender_age classes:\", gender_age_encoder.classes_)\n",
    "train_data.head(5)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-04T08:10:22.660386Z",
     "start_time": "2025-03-04T08:10:22.059620Z"
    }
   },
   "id": "3d940523377f8b25",
   "execution_count": 29
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Filter out any rows where the image doesn't exist\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3967a7d1e840d381"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "train_data_filtered = train_data[train_data['img_exists'] == True]\n",
    "val_data_filtered = val_data[val_data['img_exists'] == True]\n",
    "test_data_filtered = test_data[test_data['img_exists'] == True]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-04T08:10:22.661763Z",
     "start_time": "2025-03-04T08:10:22.606267Z"
    }
   },
   "id": "7db58f446ed7ed8d",
   "execution_count": 30
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Define a generator function to process images in batches"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "10479fab31f4f172"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from img2vec import grayscale2emb\n",
    "\n",
    "\n",
    "def image_batch_generator(image_paths, labels, batch_size):\n",
    "    num_samples = len(image_paths)\n",
    "    num_batches = math.ceil(num_samples / batch_size)\n",
    "\n",
    "    for i in range(num_batches):\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = min((i + 1) * batch_size, num_samples)\n",
    "\n",
    "        batch_paths = image_paths[start_idx:end_idx]\n",
    "        batch_features = grayscale2emb(batch_paths) / 255.0  # Normalize to [0,1]\n",
    "        batch_labels = labels[start_idx:end_idx]\n",
    "\n",
    "        yield batch_features, batch_labels"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-04T08:10:22.663283Z",
     "start_time": "2025-03-04T08:10:22.607430Z"
    }
   },
   "id": "40b839edec59102f",
   "execution_count": 31
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Extract image paths and labels"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8da59c50f0be89fc"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "train_image_paths = train_data_filtered['img_path'].tolist()\n",
    "train_labels = train_data_filtered['gender_age_label'].values\n",
    "\n",
    "val_image_paths = val_data_filtered['img_path'].tolist()\n",
    "val_labels = val_data_filtered['gender_age_label'].values\n",
    "\n",
    "test_image_paths = test_data_filtered['img_path'].tolist()\n",
    "test_labels = test_data_filtered['gender_age_label'].values"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-04T08:10:22.664784Z",
     "start_time": "2025-03-04T08:10:22.607895Z"
    }
   },
   "id": "15b28c169da16536",
   "execution_count": 32
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Print dataset sizes"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "53d4c562693322fe"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 11856\n",
      "Validation samples: 2964\n",
      "Test samples: 3731\n",
      "Number of classes: 20\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training samples: {len(train_image_paths)}\")\n",
    "print(f\"Validation samples: {len(val_image_paths)}\")\n",
    "print(f\"Test samples: {len(test_image_paths)}\")\n",
    "print(f\"Number of classes: {num_classes}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-04T08:10:22.672628Z",
     "start_time": "2025-03-04T08:10:22.608127Z"
    }
   },
   "id": "d8d723cb6011dc27",
   "execution_count": 33
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Extract features for training and validation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "540445fddb53d20a"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features for training data...\n",
      "Processing all 11856 training samples\n"
     ]
    }
   ],
   "source": [
    "print(\"Extracting features for training data...\")\n",
    "print(f\"Processing all {len(train_image_paths)} training samples\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-04T08:10:22.673670Z",
     "start_time": "2025-03-04T08:10:22.608710Z"
    }
   },
   "id": "544690004cb7efa1",
   "execution_count": 34
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Define a function to extract features in batches"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7577176c90b39c29"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def extract_features_in_batches(image_paths, batch_size=64):\n",
    "    \"\"\"Extract features from images in batches to manage memory.\"\"\"\n",
    "    num_samples = len(image_paths)\n",
    "    num_batches = math.ceil(num_samples / batch_size)\n",
    "    all_features = []\n",
    "\n",
    "    for i in range(num_batches):\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = min((i + 1) * batch_size, num_samples)\n",
    "        \n",
    "        print(f\"\\rProcessing batch {i+1}/{num_batches}\", end=\"\")\n",
    "        \n",
    "        batch_paths = image_paths[start_idx:end_idx]\n",
    "        batch_features = grayscale2emb(batch_paths) \n",
    "        \n",
    "        all_features.append(batch_features)\n",
    "    \n",
    "    print(\"\\nFeature extraction complete.\")\n",
    "    return np.vstack(all_features)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-04T08:10:22.674541Z",
     "start_time": "2025-03-04T08:10:22.609200Z"
    }
   },
   "id": "2507def9ce8ee6db",
   "execution_count": 35
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 1/186"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for axis 3 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[37], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m X_train \u001B[38;5;241m=\u001B[39m \u001B[43mextract_features_in_batches\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_image_paths\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbatch_size\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      2\u001B[0m y_train \u001B[38;5;241m=\u001B[39m train_labels\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mExtracting features for validation data...\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[0;32mIn[35], line 14\u001B[0m, in \u001B[0;36mextract_features_in_batches\u001B[0;34m(image_paths, batch_size)\u001B[0m\n\u001B[1;32m     11\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\r\u001B[39;00m\u001B[38;5;124mProcessing batch \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mi\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnum_batches\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m, end\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     13\u001B[0m     batch_paths \u001B[38;5;241m=\u001B[39m image_paths[start_idx:end_idx]\n\u001B[0;32m---> 14\u001B[0m     batch_features \u001B[38;5;241m=\u001B[39m \u001B[43mgrayscale2emb\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch_paths\u001B[49m\u001B[43m)\u001B[49m \n\u001B[1;32m     16\u001B[0m     all_features\u001B[38;5;241m.\u001B[39mappend(batch_features)\n\u001B[1;32m     18\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mFeature extraction complete.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/Desktop/VisAge-main/img2vec.py:99\u001B[0m, in \u001B[0;36mgrayscale2emb\u001B[0;34m(img_paths, batch_size)\u001B[0m\n\u001B[1;32m     95\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mgrayscale2emb\u001B[39m(img_paths: List[\u001B[38;5;28mstr\u001B[39m], batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m32\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m np\u001B[38;5;241m.\u001B[39mndarray:\n\u001B[1;32m     96\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     97\u001B[0m \u001B[38;5;124;03m    Convert grayscale images to embeddings using ResNet50 with batch processing.\u001B[39;00m\n\u001B[1;32m     98\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m---> 99\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_img2emb\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg_paths\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcolor_mode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mgrayscale\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Desktop/VisAge-main/img2vec.py:74\u001B[0m, in \u001B[0;36m_img2emb\u001B[0;34m(img_paths, batch_size, color_mode)\u001B[0m\n\u001B[1;32m     71\u001B[0m         imgs\u001B[38;5;241m.\u001B[39mappend(np\u001B[38;5;241m.\u001B[39mzeros((\u001B[38;5;241m224\u001B[39m, \u001B[38;5;241m224\u001B[39m, \u001B[38;5;241m3\u001B[39m)))\n\u001B[1;32m     73\u001B[0m batch \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39marray(imgs)\n\u001B[0;32m---> 74\u001B[0m batch \u001B[38;5;241m=\u001B[39m \u001B[43mpreprocess_input\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     75\u001B[0m features \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mpredict(batch, verbose\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n\u001B[1;32m     76\u001B[0m all_features\u001B[38;5;241m.\u001B[39mappend(features)\n",
      "File \u001B[0;32m~/Desktop/VisAge-main/.venv/lib/python3.10/site-packages/keras/src/applications/resnet.py:512\u001B[0m, in \u001B[0;36mpreprocess_input\u001B[0;34m(x, data_format)\u001B[0m\n\u001B[1;32m    505\u001B[0m \u001B[38;5;129m@keras_export\u001B[39m(\n\u001B[1;32m    506\u001B[0m     [\n\u001B[1;32m    507\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mkeras.applications.resnet50.preprocess_input\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    510\u001B[0m )\n\u001B[1;32m    511\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mpreprocess_input\u001B[39m(x, data_format\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m--> 512\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mimagenet_utils\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpreprocess_input\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    513\u001B[0m \u001B[43m        \u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata_format\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdata_format\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcaffe\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\n\u001B[1;32m    514\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Desktop/VisAge-main/.venv/lib/python3.10/site-packages/keras/src/applications/imagenet_utils.py:104\u001B[0m, in \u001B[0;36mpreprocess_input\u001B[0;34m(x, data_format, mode)\u001B[0m\n\u001B[1;32m     98\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m     99\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mExpected data_format to be one of `channels_first` or \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    100\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m`channels_last`. Received: data_format=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdata_format\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    101\u001B[0m     )\n\u001B[1;32m    103\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(x, np\u001B[38;5;241m.\u001B[39mndarray):\n\u001B[0;32m--> 104\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_preprocess_numpy_input\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata_format\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdata_format\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    105\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    106\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _preprocess_tensor_input(x, data_format\u001B[38;5;241m=\u001B[39mdata_format, mode\u001B[38;5;241m=\u001B[39mmode)\n",
      "File \u001B[0;32m~/Desktop/VisAge-main/.venv/lib/python3.10/site-packages/keras/src/applications/imagenet_utils.py:225\u001B[0m, in \u001B[0;36m_preprocess_numpy_input\u001B[0;34m(x, data_format, mode)\u001B[0m\n\u001B[1;32m    223\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    224\u001B[0m     x[\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m, \u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m=\u001B[39m mean[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m--> 225\u001B[0m     x[\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m, \u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m=\u001B[39m mean[\u001B[38;5;241m1\u001B[39m]\n\u001B[1;32m    226\u001B[0m     x[\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m, \u001B[38;5;241m2\u001B[39m] \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m=\u001B[39m mean[\u001B[38;5;241m2\u001B[39m]\n\u001B[1;32m    227\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m std \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[0;31mIndexError\u001B[0m: index 1 is out of bounds for axis 3 with size 1"
     ]
    }
   ],
   "source": [
    "X_train = extract_features_in_batches(train_image_paths, batch_size=batch_size)\n",
    "y_train = train_labels\n",
    "\n",
    "print(\"Extracting features for validation data...\")\n",
    "print(f\"Processing all {len(val_image_paths)} validation samples\")\n",
    "\n",
    "X_val = extract_features_in_batches(val_image_paths, batch_size=batch_size)\n",
    "y_val = val_labels\n",
    "\n",
    "\n",
    "print(\"Creating and training RandomForestClassifier model...\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-04T08:11:58.435812Z",
     "start_time": "2025-03-04T08:11:58.280896Z"
    }
   },
   "id": "4cb956c7cd2c93c5",
   "execution_count": 37
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create a pipeline with scaling and RandomeForest\n",
    "This is much more memory efficient for large datasets\n",
    "Create a pipeline that scales data (optional for RF) and then trains a RandomForestClassifier"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "726c08239aff4978"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),  # Scaling can be useful even though RF doesn't strictly require it\n",
    "    ('rf', RandomForestClassifier(\n",
    "        n_estimators=100,           # Number of trees in the forest\n",
    "        criterion='gini',           # Criterion to measure split quality; can also use 'entropy'\n",
    "        max_depth=None,             # Expand nodes until all leaves are pure or until minimum sample split\n",
    "        class_weight='balanced',    # Adjust weights inversely proportional to class frequencies\n",
    "        random_state=42,            # For reproducibility\n",
    "        verbose=1,                  # Controls verbosity during training\n",
    "        n_jobs=-1                   # Use all available cores for parallel processing\n",
    "    ))\n",
    "])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-04T08:10:53.947456Z",
     "start_time": "2025-03-04T08:10:53.944636Z"
    }
   },
   "id": "fcf0b5d10fadbd92",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### define the model and train it"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5b7caf0c672c1747"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Fit the pipeline directly\n",
    "pipeline.fit(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-03-04T08:10:53.946205Z"
    }
   },
   "id": "46ec53300e3cc175",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Evaluate the model on test data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dfb26e7a3d7f5460"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Initialize the model\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Extract features for test data\n",
    "print(\"Extracting features for test data...\")\n",
    "print(f\"Processing all {len(test_image_paths)} test samples\")\n",
    "\n",
    "X_test = extract_features_in_batches(test_image_paths, batch_size=batch_size)\n",
    "y_test = test_labels\n",
    "\n",
    "# Evaluate on test data\n",
    "test_preds = model.predict(X_test)  # Use X_test instead of test_data\n",
    "test_accuracy = accuracy_score(y_test, test_preds)  # Use y_test instead of test_data\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-03-04T08:10:53.946333Z"
    }
   },
   "id": "1195a63f484c1aee",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Visualize results"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a6d0d57d8f2a719b"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_test, test_preds)\n",
    "\n",
    "# Set up figure size\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "# Create a heatmap without masking zero values for better visualization\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', linewidths=0.5, \n",
    "            xticklabels=gender_age_encoder.classes_,\n",
    "            yticklabels=gender_age_encoder.classes_)\n",
    "\n",
    "# Labels and title\n",
    "plt.xlabel('Predicted Label', fontsize=14)\n",
    "plt.ylabel('True Label', fontsize=14)\n",
    "plt.title('Confusion Matrix', fontsize=16)\n",
    "\n",
    "# Improve layout and save figure\n",
    "plt.tight_layout()\n",
    "plt.savefig('confusion_matrix.png', dpi=300)  # Higher DPI for better quality\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-03-04T08:10:53.947043Z"
    }
   },
   "id": "1730004b56a5b749",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Classification report"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d58d40b63f3111d1"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "\n",
    "# Get the unique classes in your test data\n",
    "unique_classes = np.unique(y_test)\n",
    "\n",
    "# Generate classification report using the labels parameter\n",
    "report = classification_report(\n",
    "    y_test, \n",
    "    test_preds, \n",
    "    labels=unique_classes,\n",
    "    target_names=[gender_age_encoder.classes_[i] for i in unique_classes],\n",
    "    zero_division=0  # Handles the division by zero error by setting precision and recall to 0\n",
    ")\n",
    "\n",
    "# Print the classification report with better formatting\n",
    "print(\"=\" * 50)\n",
    "print(\"Classification Report\")\n",
    "print(\"=\" * 50)\n",
    "print(report)\n",
    "print(\"=\" * 50)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-04T08:10:53.965094Z",
     "start_time": "2025-03-04T08:10:53.947919Z"
    }
   },
   "id": "85b3855f7a908e06",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Save the model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a77cc75766f79a15"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "dump(model, 'RF_gray_gender_age_classifier.joblib')\n",
    "dump(gender_age_encoder, 'gender_age_encoder.joblib')\n",
    "print(\"Model saved successfully.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-03-04T08:10:53.964840Z"
    }
   },
   "id": "87397dc9406a57df",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Function to predict gender_age range for a new image\n",
    "def predict_gender_age(image_path, model, gender_age_encoder):\n",
    "    \"\"\"Predict gender and age for a given face image.\"\"\"\n",
    "    # Extract features\n",
    "    features = grayscale2emb([image_path]) / 255.0\n",
    "    \n",
    "    # Apply the same scaling used during training (if using a pipeline with StandardScaler)\n",
    "    if hasattr(model, 'named_steps') and 'scaler' in model.named_steps:\n",
    "        features = model.named_steps['scaler'].transform(features)\n",
    "    \n",
    "    # Make prediction\n",
    "    pred_class = model.predict(features)[0]\n",
    "    \n",
    "    # Get class probabilities if available\n",
    "    if hasattr(model, 'predict_proba'):\n",
    "        pred_probs = model.predict_proba(features)[0]\n",
    "        confidence = pred_probs[pred_class]\n",
    "    else:\n",
    "        # For RandomForestClassifier without calibration, we can't get probabilities\n",
    "        # Use decision function instead as a rough proxy for confidence\n",
    "        decision_values = model.decision_function(features)\n",
    "        # Normalize to [0, 1] range roughly\n",
    "        confidence = 1 / (1 + np.exp(-np.abs(decision_values[0][pred_class])))\n",
    "    \n",
    "    # Convert to gender_age range\n",
    "    pred_gender_age_range = gender_age_encoder.classes_[pred_class]\n",
    "    \n",
    "    return pred_gender_age_range, confidence\n",
    "\n",
    "# Example usage:\n",
    "\"\"\"\n",
    "# Load the model\n",
    "image_path = \"path/to/new/face/image.jpg\"\n",
    "model = load('linear_svc_gender_age_classifier.joblib')\n",
    "gender_age_encoder = load('gender_age_encoder.joblib')\n",
    "\n",
    "# Make prediction\n",
    "pred_gender_age, confidence = predict_gender_age(image_path, model, gender_age_encoder)\n",
    "print(f\"Predicted gender_age range: {pred_gender_age} with confidence {confidence:.2f}\")\n",
    "\"\"\"\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-03-04T08:10:53.964884Z"
    }
   },
   "id": "cb12004bfd9a5603",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
