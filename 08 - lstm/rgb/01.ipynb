{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\", \"..\"))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1d86459781eb9ef",
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import math\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, LSTM, Reshape, Bidirectional\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "from img2vec import rgb2emb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af882f020c34df5",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Check for GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "38721be792e5b56b",
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n",
      "No GPU found. TensorFlow will use CPU.\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "# Print GPU information\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    for gpu in gpus:\n",
    "        print(\"Name:\", gpu.name, \"  Type:\", gpu.device_type)\n",
    "\n",
    "    # Set memory growth to avoid using all GPU memory\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "    print(\"GPU is available for TensorFlow!\")\n",
    "else:\n",
    "    print(\"No GPU found. TensorFlow will use CPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7893bb327e76005c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Set random seed for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d235445b29b5279c",
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de63a3a7caf677",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ed3845e0de39192",
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "data_dir = os.path.join('..', '..', 'data')\n",
    "train_data = pd.read_csv(os.path.join(data_dir, 'train.csv'))\n",
    "val_data = pd.read_csv(os.path.join(data_dir, 'val.csv'))\n",
    "test_data = pd.read_csv(os.path.join(data_dir, 'test.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81dbb3e686f34058",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Print dataset information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ac55933a2859e07c",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 11856\n",
      "Validation set size: 2964\n",
      "Test set size: 3731\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training set size: {len(train_data)}\")\n",
    "print(f\"Validation set size: {len(val_data)}\")\n",
    "print(f\"Test set size: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7ca055ad65ba02",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Check if images exist and filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e5d0dedfd8591c19",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def construct_img_path(row):\n",
    "    return os.path.join(data_dir, 'faces', row['user_id'], \n",
    "                      f\"coarse_tilt_aligned_face.{row['face_id']}.{row['original_image']}\")\n",
    "\n",
    "train_data['img_path'] = train_data.apply(construct_img_path, axis=1)\n",
    "val_data['img_path'] = val_data.apply(construct_img_path, axis=1)\n",
    "test_data['img_path'] = test_data.apply(construct_img_path, axis=1)\n",
    "\n",
    "train_data['img_exists'] = train_data['img_path'].apply(os.path.exists)\n",
    "val_data['img_exists'] = val_data['img_path'].apply(os.path.exists)\n",
    "test_data['img_exists'] = test_data['img_path'].apply(os.path.exists)\n",
    "\n",
    "# Filter to include only rows where images exist\n",
    "train_data = train_data[train_data['img_exists'] == True]\n",
    "val_data = val_data[val_data['img_exists'] == True]\n",
    "test_data = test_data[test_data['img_exists'] == True]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81036f8022ff3853",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Encode labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bb86e11e65c9b59b",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoders created and saved.\n",
      "Age classes: ['(0, 2)' '(15, 20)' '(25, 32)' '(38, 43)' '(4, 6)' '(48, 53)' '(60, 100)'\n",
      " '(8, 23)']\n",
      "Gender classes: ['f' 'm' 'u']\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists('age_encoder.pkl') and os.path.exists('gender_encoder.pkl'):\n",
    "    age_encoder = joblib.load('age_encoder.pkl')\n",
    "    gender_encoder = joblib.load('gender_encoder.pkl')\n",
    "    print(\"Age and gender encoders loaded successfully.\")\n",
    "    \n",
    "    # Add these lines to encode train data when loading existing encoders\n",
    "    train_data['age_encoded'] = age_encoder.transform(train_data['age'])\n",
    "    train_data['gender_encoded'] = gender_encoder.transform(train_data['gender'])\n",
    "else:\n",
    "    # Remove rare classes\n",
    "    age_counts = train_data['age'].value_counts()\n",
    "    gender_counts = train_data['gender'].value_counts()\n",
    "    rare_ages = age_counts[age_counts < 5].index.tolist()\n",
    "    rare_genders = gender_counts[gender_counts < 5].index.tolist()\n",
    "    \n",
    "    # Filter data\n",
    "    train_data = train_data[~train_data['age'].isin(rare_ages) & ~train_data['gender'].isin(rare_genders)]\n",
    "    \n",
    "    # Create encoders\n",
    "    age_encoder = LabelEncoder()\n",
    "    gender_encoder = LabelEncoder()\n",
    "    train_data['age_encoded'] = age_encoder.fit_transform(train_data['age'])\n",
    "    train_data['gender_encoded'] = gender_encoder.fit_transform(train_data['gender'])\n",
    "    \n",
    "    # Save encoders\n",
    "    joblib.dump(age_encoder, 'age_encoder.pkl')\n",
    "    joblib.dump(gender_encoder, 'gender_encoder.pkl')\n",
    "    print(\"Encoders created and saved.\")\n",
    "\n",
    "# Filter validation and test data to include only seen classes\n",
    "val_data = val_data[val_data['age'].isin(age_encoder.classes_)]\n",
    "val_data = val_data[val_data['gender'].isin(gender_encoder.classes_)]\n",
    "test_data = test_data[test_data['age'].isin(age_encoder.classes_)]\n",
    "test_data = test_data[test_data['gender'].isin(gender_encoder.classes_)]\n",
    "\n",
    "# Encode the labels\n",
    "val_data['age_encoded'] = age_encoder.transform(val_data['age'])\n",
    "val_data['gender_encoded'] = gender_encoder.transform(val_data['gender'])\n",
    "test_data['age_encoded'] = age_encoder.transform(test_data['age'])\n",
    "test_data['gender_encoded'] = gender_encoder.transform(test_data['gender'])\n",
    "\n",
    "num_age_classes = len(age_encoder.classes_)\n",
    "num_gender_classes = len(gender_encoder.classes_)\n",
    "print(f\"Age classes: {age_encoder.classes_}\")\n",
    "print(f\"Gender classes: {gender_encoder.classes_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8a32b633a781e0",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Define function that processes features in batches and stores them to avoid recomputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f1f7f32c0f8db63b",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def preprocess_and_save_features(image_paths, output_file, batch_size=64):\n",
    "    if os.path.exists(output_file):\n",
    "        print(f\"Loading pre-processed features from {output_file}\")\n",
    "        return np.load(output_file)\n",
    "\n",
    "    print(f\"Processing {len(image_paths)} images and saving to {output_file}\")\n",
    "    all_features = []\n",
    "\n",
    "    for i in range(0, len(image_paths), batch_size):\n",
    "        batch_paths = image_paths[i:i + batch_size]\n",
    "        print(f\"Processing batch {i // batch_size + 1}/{math.ceil(len(image_paths) / batch_size)}\")\n",
    "        batch_features = rgb2emb(batch_paths)\n",
    "        all_features.append(batch_features)\n",
    "\n",
    "    all_features = np.vstack(all_features)\n",
    "    np.save(output_file, all_features)\n",
    "    return all_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c84b3e3ec33a80",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Process and save features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6d4586caadd1345a",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 11856 images and saving to train_features.npy\n",
      "Processing batch 1/186\n",
      "Processing batch 2/186\n",
      "Processing batch 3/186\n",
      "Processing batch 4/186\n",
      "Processing batch 5/186\n",
      "Processing batch 6/186\n",
      "Processing batch 7/186\n",
      "Processing batch 8/186\n",
      "Processing batch 9/186\n",
      "Processing batch 10/186\n",
      "Processing batch 11/186\n",
      "Processing batch 12/186\n",
      "Processing batch 13/186\n",
      "Processing batch 14/186\n",
      "Processing batch 15/186\n",
      "Processing batch 16/186\n",
      "Processing batch 17/186\n",
      "Processing batch 18/186\n",
      "Processing batch 19/186\n",
      "Processing batch 20/186\n",
      "Processing batch 21/186\n",
      "Processing batch 22/186\n",
      "Processing batch 23/186\n",
      "Processing batch 24/186\n",
      "Processing batch 25/186\n",
      "Processing batch 26/186\n",
      "Processing batch 27/186\n",
      "Processing batch 28/186\n",
      "Processing batch 29/186\n",
      "Processing batch 30/186\n",
      "Processing batch 31/186\n",
      "Processing batch 32/186\n",
      "Processing batch 33/186\n",
      "Processing batch 34/186\n",
      "Processing batch 35/186\n",
      "Processing batch 36/186\n",
      "Processing batch 37/186\n",
      "Processing batch 38/186\n",
      "Processing batch 39/186\n",
      "Processing batch 40/186\n",
      "Processing batch 41/186\n",
      "Processing batch 42/186\n",
      "Processing batch 43/186\n",
      "Processing batch 44/186\n",
      "Processing batch 45/186\n",
      "Processing batch 46/186\n",
      "Processing batch 47/186\n",
      "Processing batch 48/186\n",
      "Processing batch 49/186\n",
      "Processing batch 50/186\n",
      "Processing batch 51/186\n",
      "Processing batch 52/186\n",
      "Processing batch 53/186\n",
      "Processing batch 54/186\n",
      "Processing batch 55/186\n",
      "Processing batch 56/186\n",
      "Processing batch 57/186\n",
      "Processing batch 58/186\n",
      "Processing batch 59/186\n",
      "Processing batch 60/186\n",
      "Processing batch 61/186\n",
      "Processing batch 62/186\n",
      "Processing batch 63/186\n",
      "Processing batch 64/186\n",
      "Processing batch 65/186\n",
      "Processing batch 66/186\n",
      "Processing batch 67/186\n",
      "Processing batch 68/186\n",
      "Processing batch 69/186\n",
      "Processing batch 70/186\n",
      "Processing batch 71/186\n",
      "Processing batch 72/186\n",
      "Processing batch 73/186\n",
      "Processing batch 74/186\n",
      "Processing batch 75/186\n",
      "Processing batch 76/186\n",
      "Processing batch 77/186\n",
      "Processing batch 78/186\n",
      "Processing batch 79/186\n",
      "Processing batch 80/186\n",
      "Processing batch 81/186\n",
      "Processing batch 82/186\n",
      "Processing batch 83/186\n",
      "Processing batch 84/186\n",
      "Processing batch 85/186\n",
      "Processing batch 86/186\n",
      "Processing batch 87/186\n",
      "Processing batch 88/186\n",
      "Processing batch 89/186\n",
      "Processing batch 90/186\n",
      "Processing batch 91/186\n",
      "Processing batch 92/186\n",
      "Processing batch 93/186\n",
      "Processing batch 94/186\n",
      "Processing batch 95/186\n",
      "Processing batch 96/186\n",
      "Processing batch 97/186\n",
      "Processing batch 98/186\n",
      "Processing batch 99/186\n",
      "Processing batch 100/186\n",
      "Processing batch 101/186\n",
      "Processing batch 102/186\n",
      "Processing batch 103/186\n",
      "Processing batch 104/186\n",
      "Processing batch 105/186\n",
      "Processing batch 106/186\n",
      "Processing batch 107/186\n",
      "Processing batch 108/186\n",
      "Processing batch 109/186\n",
      "Processing batch 110/186\n",
      "Processing batch 111/186\n",
      "Processing batch 112/186\n",
      "Processing batch 113/186\n",
      "Processing batch 114/186\n",
      "Processing batch 115/186\n",
      "Processing batch 116/186\n",
      "Processing batch 117/186\n",
      "Processing batch 118/186\n",
      "Processing batch 119/186\n",
      "Processing batch 120/186\n",
      "Processing batch 121/186\n",
      "Processing batch 122/186\n",
      "Processing batch 123/186\n",
      "Processing batch 124/186\n",
      "Processing batch 125/186\n",
      "Processing batch 126/186\n",
      "Processing batch 127/186\n",
      "Processing batch 128/186\n",
      "Processing batch 129/186\n",
      "Processing batch 130/186\n",
      "Processing batch 131/186\n",
      "Processing batch 132/186\n",
      "Processing batch 133/186\n",
      "Processing batch 134/186\n",
      "Processing batch 135/186\n",
      "Processing batch 136/186\n",
      "Processing batch 137/186\n",
      "Processing batch 138/186\n",
      "Processing batch 139/186\n",
      "Processing batch 140/186\n",
      "Processing batch 141/186\n",
      "Processing batch 142/186\n",
      "Processing batch 143/186\n",
      "Processing batch 144/186\n",
      "Processing batch 145/186\n",
      "Processing batch 146/186\n",
      "Processing batch 147/186\n",
      "Processing batch 148/186\n",
      "Processing batch 149/186\n",
      "Processing batch 150/186\n",
      "Processing batch 151/186\n",
      "Processing batch 152/186\n",
      "Processing batch 153/186\n",
      "Processing batch 154/186\n",
      "Processing batch 155/186\n",
      "Processing batch 156/186\n",
      "Processing batch 157/186\n",
      "Processing batch 158/186\n",
      "Processing batch 159/186\n",
      "Processing batch 160/186\n",
      "Processing batch 161/186\n",
      "Processing batch 162/186\n",
      "Processing batch 163/186\n",
      "Processing batch 164/186\n",
      "Processing batch 165/186\n",
      "Processing batch 166/186\n",
      "Processing batch 167/186\n",
      "Processing batch 168/186\n",
      "Processing batch 169/186\n",
      "Processing batch 170/186\n",
      "Processing batch 171/186\n",
      "Processing batch 172/186\n",
      "Processing batch 173/186\n",
      "Processing batch 174/186\n",
      "Processing batch 175/186\n",
      "Processing batch 176/186\n",
      "Processing batch 177/186\n",
      "Processing batch 178/186\n",
      "Processing batch 179/186\n",
      "Processing batch 180/186\n",
      "Processing batch 181/186\n",
      "Processing batch 182/186\n",
      "Processing batch 183/186\n",
      "Processing batch 184/186\n",
      "Processing batch 185/186\n",
      "Processing batch 186/186\n",
      "Processing 2964 images and saving to val_features.npy\n",
      "Processing batch 1/47\n",
      "Processing batch 2/47\n",
      "Processing batch 3/47\n",
      "Processing batch 4/47\n",
      "Processing batch 5/47\n",
      "Processing batch 6/47\n",
      "Processing batch 7/47\n",
      "Processing batch 8/47\n",
      "Processing batch 9/47\n",
      "Processing batch 10/47\n",
      "Processing batch 11/47\n",
      "Processing batch 12/47\n",
      "Processing batch 13/47\n",
      "Processing batch 14/47\n",
      "Processing batch 15/47\n",
      "Processing batch 16/47\n",
      "Processing batch 17/47\n",
      "Processing batch 18/47\n",
      "Processing batch 19/47\n",
      "Processing batch 20/47\n",
      "Processing batch 21/47\n",
      "Processing batch 22/47\n",
      "Processing batch 23/47\n",
      "Processing batch 24/47\n",
      "Processing batch 25/47\n",
      "Processing batch 26/47\n",
      "Processing batch 27/47\n",
      "Processing batch 28/47\n",
      "Processing batch 29/47\n",
      "Processing batch 30/47\n",
      "Processing batch 31/47\n",
      "Processing batch 32/47\n",
      "Processing batch 33/47\n",
      "Processing batch 34/47\n",
      "Processing batch 35/47\n",
      "Processing batch 36/47\n",
      "Processing batch 37/47\n",
      "Processing batch 38/47\n",
      "Processing batch 39/47\n",
      "Processing batch 40/47\n",
      "Processing batch 41/47\n",
      "Processing batch 42/47\n",
      "Processing batch 43/47\n",
      "Processing batch 44/47\n",
      "Processing batch 45/47\n",
      "Processing batch 46/47\n",
      "Processing batch 47/47\n",
      "Processing 3731 images and saving to test_features.npy\n",
      "Processing batch 1/59\n",
      "Processing batch 2/59\n",
      "Processing batch 3/59\n",
      "Processing batch 4/59\n",
      "Processing batch 5/59\n",
      "Processing batch 6/59\n",
      "Processing batch 7/59\n",
      "Processing batch 8/59\n",
      "Processing batch 9/59\n",
      "Processing batch 10/59\n",
      "Processing batch 11/59\n",
      "Processing batch 12/59\n",
      "Processing batch 13/59\n",
      "Processing batch 14/59\n",
      "Processing batch 15/59\n",
      "Processing batch 16/59\n",
      "Processing batch 17/59\n",
      "Processing batch 18/59\n",
      "Processing batch 19/59\n",
      "Processing batch 20/59\n",
      "Processing batch 21/59\n",
      "Processing batch 22/59\n",
      "Processing batch 23/59\n",
      "Processing batch 24/59\n",
      "Processing batch 25/59\n",
      "Processing batch 26/59\n",
      "Processing batch 27/59\n",
      "Processing batch 28/59\n",
      "Processing batch 29/59\n",
      "Processing batch 30/59\n",
      "Processing batch 31/59\n",
      "Processing batch 32/59\n",
      "Processing batch 33/59\n",
      "Processing batch 34/59\n",
      "Processing batch 35/59\n",
      "Processing batch 36/59\n",
      "Processing batch 37/59\n",
      "Processing batch 38/59\n",
      "Processing batch 39/59\n",
      "Processing batch 40/59\n",
      "Processing batch 41/59\n",
      "Processing batch 42/59\n",
      "Processing batch 43/59\n",
      "Processing batch 44/59\n",
      "Processing batch 45/59\n",
      "Processing batch 46/59\n",
      "Processing batch 47/59\n",
      "Processing batch 48/59\n",
      "Processing batch 49/59\n",
      "Processing batch 50/59\n",
      "Processing batch 51/59\n",
      "Processing batch 52/59\n",
      "Processing batch 53/59\n",
      "Processing batch 54/59\n",
      "Processing batch 55/59\n",
      "Processing batch 56/59\n",
      "Processing batch 57/59\n",
      "Processing batch 58/59\n",
      "Processing batch 59/59\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists('train_features.npy'):\n",
    "    train_features = preprocess_and_save_features(train_data['img_path'].tolist(), 'train_features.npy')\n",
    "    val_features = preprocess_and_save_features(val_data['img_path'].tolist(), 'val_features.npy')\n",
    "    test_features = preprocess_and_save_features(test_data['img_path'].tolist(), 'test_features.npy')\n",
    "else:\n",
    "    train_features = np.load('train_features.npy')\n",
    "    val_features = np.load('val_features.npy')\n",
    "    test_features = np.load('test_features.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f4929715b5d7e0",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Standardize features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f5836ec7182bdfd0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature scaler created and saved.\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists('feature_scaler.pkl'):\n",
    "    scaler = joblib.load('feature_scaler.pkl')\n",
    "    print(\"Feature scaler loaded successfully.\")\n",
    "else:\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(train_features)\n",
    "    joblib.dump(scaler, 'feature_scaler.pkl')\n",
    "    print(\"Feature scaler created and saved.\")\n",
    "\n",
    "train_features_scaled = scaler.transform(train_features)\n",
    "val_features_scaled = scaler.transform(val_features)\n",
    "test_features_scaled = scaler.transform(test_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55515eb23e19e756",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Define LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1fa01d2b749bc006",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age Model Summary:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │       <span style=\"color: #00af00; text-decoration-color: #00af00\">263,168</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">164,352</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)              │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,032</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_2 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional (\u001b[38;5;33mBidirectional\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │       \u001b[38;5;34m263,168\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_1 (\u001b[38;5;33mBidirectional\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m164,352\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m16,512\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)              │         \u001b[38;5;34m1,032\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">446,088</span> (1.70 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m446,088\u001b[0m (1.70 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">445,576</span> (1.70 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m445,576\u001b[0m (1.70 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> (2.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m512\u001b[0m (2.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Gender Model Summary:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │       <span style=\"color: #00af00; text-decoration-color: #00af00\">263,168</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">164,352</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_3           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">387</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_3 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_2 (\u001b[38;5;33mBidirectional\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │       \u001b[38;5;34m263,168\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_3 (\u001b[38;5;33mBidirectional\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m164,352\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m16,512\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_3           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              │           \u001b[38;5;34m387\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">445,443</span> (1.70 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m445,443\u001b[0m (1.70 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">444,931</span> (1.70 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m444,931\u001b[0m (1.70 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> (2.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m512\u001b[0m (2.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def reshape_features_for_lstm(features, time_steps=16):\n",
    "    \"\"\"Reshape features to be compatible with LSTM input shape (batch, time_steps, features)\"\"\"\n",
    "    feature_dim = features.shape[1]\n",
    "    # Determine the feature size per time step\n",
    "    features_per_step = feature_dim // time_steps\n",
    "    \n",
    "    # If features aren't cleanly divisible, we'll pad\n",
    "    if feature_dim % time_steps != 0:\n",
    "        pad_size = time_steps - (feature_dim % time_steps)\n",
    "        features = np.pad(features, ((0, 0), (0, pad_size)), 'constant')\n",
    "        feature_dim = features.shape[1]\n",
    "        features_per_step = feature_dim // time_steps\n",
    "    \n",
    "    # Reshape to (batch, time_steps, features_per_step)\n",
    "    return features.reshape(features.shape[0], time_steps, features_per_step)\n",
    "\n",
    "# Reshape data for LSTM\n",
    "time_steps = 16  # You can adjust this value\n",
    "train_features_lstm = reshape_features_for_lstm(train_features_scaled, time_steps)\n",
    "val_features_lstm = reshape_features_for_lstm(val_features_scaled, time_steps)\n",
    "test_features_lstm = reshape_features_for_lstm(test_features_scaled, time_steps)\n",
    "\n",
    "def create_lstm_age_model(input_shape, num_age_classes):\n",
    "    input_layer = Input(shape=input_shape)\n",
    "    \n",
    "    # LSTM layers\n",
    "    x = Bidirectional(LSTM(128, return_sequences=True))(input_layer)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Bidirectional(LSTM(64))(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    # Dense layers for classification\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    age_output = Dense(num_age_classes, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=input_layer, outputs=age_output)\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "def create_lstm_gender_model(input_shape, num_gender_classes):\n",
    "    input_layer = Input(shape=input_shape)\n",
    "    \n",
    "    # LSTM layers\n",
    "    x = Bidirectional(LSTM(128, return_sequences=True))(input_layer)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Bidirectional(LSTM(64))(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    # Dense layers for classification\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    gender_output = Dense(num_gender_classes, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=input_layer, outputs=gender_output)\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create LSTM models\n",
    "lstm_input_shape = (train_features_lstm.shape[1], train_features_lstm.shape[2])\n",
    "age_model = create_lstm_age_model(lstm_input_shape, num_age_classes)\n",
    "gender_model = create_lstm_gender_model(lstm_input_shape, num_gender_classes)\n",
    "\n",
    "# Print model summaries\n",
    "print(\"Age Model Summary:\")\n",
    "age_model.summary()\n",
    "print(\"\\nGender Model Summary:\")\n",
    "gender_model.summary()\n",
    "\n",
    "# Define callbacks for age model\n",
    "age_callbacks = [\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=5,\n",
    "        restore_best_weights=True\n",
    "    ),\n",
    "    ModelCheckpoint(\n",
    "        'best_lstm_age_model.h5',\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "# Define callbacks for gender model\n",
    "gender_callbacks = [\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=5,\n",
    "        restore_best_weights=True\n",
    "    ),\n",
    "    ModelCheckpoint(\n",
    "        'best_lstm_gender_model.h5',\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b19e985a668e8d9",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Compute class weights and get labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "130a6d687592498a",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_age_labels = train_data['age_encoded'].values\n",
    "train_gender_labels = train_data['gender_encoded'].values\n",
    "val_age_labels = val_data['age_encoded'].values\n",
    "val_gender_labels = val_data['gender_encoded'].values\n",
    "\n",
    "age_weights = compute_class_weight('balanced', classes=np.unique(train_data['age_encoded']), \n",
    "                                  y=train_data['age_encoded'])\n",
    "gender_weights = compute_class_weight('balanced', classes=np.unique(train_data['gender_encoded']), \n",
    "                                     y=train_data['gender_encoded'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4641fbd372f581",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d94e824f88d4fb",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the LSTM age model...\n",
      "Epoch 1/30\n",
      "\u001b[1m185/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.2368 - loss: 2.2124"
     ]
    }
   ],
   "source": [
    "print(\"Training the LSTM age model...\")\n",
    "age_history = age_model.fit(\n",
    "    train_features_lstm,\n",
    "    train_age_labels,\n",
    "    validation_data=(val_features_lstm, val_age_labels),\n",
    "    epochs=30,\n",
    "    batch_size=64,\n",
    "    callbacks=age_callbacks,\n",
    "    class_weight=dict(enumerate(age_weights))\n",
    ")\n",
    "\n",
    "print(\"Training the LSTM gender model...\")\n",
    "gender_history = gender_model.fit(\n",
    "    train_features_lstm,\n",
    "    train_gender_labels,\n",
    "    validation_data=(val_features_lstm, val_gender_labels),\n",
    "    epochs=30,\n",
    "    batch_size=64,\n",
    "    callbacks=gender_callbacks,\n",
    "    class_weight=dict(enumerate(gender_weights))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2812617a503aa478",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f271a80ba27a274",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "age_model.save('lstm_age_model.h5')\n",
    "gender_model.save('lstm_gender_model.h5')\n",
    "print(\"LSTM models saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b782bde0a616e8a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e82d78d69948dd",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# After training the separate LSTM models, we can evaluate them together:\n",
    "print(\"Evaluating LSTM models...\")\n",
    "\n",
    "# Make predictions with both models\n",
    "age_predictions = np.argmax(age_model.predict(test_features_lstm), axis=1)\n",
    "gender_predictions = np.argmax(gender_model.predict(test_features_lstm), axis=1)\n",
    "\n",
    "# Calculate individual accuracies\n",
    "age_accuracy = np.mean(age_predictions == test_data['age_encoded'].values)\n",
    "gender_accuracy = np.mean(gender_predictions == test_data['gender_encoded'].values)\n",
    "\n",
    "# Calculate combined accuracy (both predictions correct)\n",
    "correct_both = np.logical_and(\n",
    "    age_predictions == test_data['age_encoded'].values,\n",
    "    gender_predictions == test_data['gender_encoded'].values\n",
    ")\n",
    "combined_accuracy = np.mean(correct_both)\n",
    "\n",
    "print(f\"Age Accuracy: {age_accuracy:.4f}\")\n",
    "print(f\"Gender Accuracy: {gender_accuracy:.4f}\")\n",
    "print(f\"Combined Accuracy (both correct): {combined_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b71477a7930fc9",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Generate classification reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9df2c1eebd0144",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"\\nAge Classification Report:\")\n",
    "print(classification_report(test_data['age_encoded'].values, age_predictions, \n",
    "                         target_names=age_encoder.classes_))\n",
    "print(\"\\nGender Classification Report:\")\n",
    "print(classification_report(test_data['gender_encoded'].values, gender_predictions, \n",
    "                         target_names=gender_encoder.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8fdc31902a9d08",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Plot training history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7339eedd689b579c",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Age accuracy\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(age_history.history['accuracy'], label='Train Age Accuracy')\n",
    "plt.plot(age_history.history['val_accuracy'], label='Validation Age Accuracy')\n",
    "plt.title('LSTM Age Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# Age loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(age_history.history['loss'], label='Train Age Loss')\n",
    "plt.plot(age_history.history['val_loss'], label='Validation Age Loss')\n",
    "plt.title('LSTM Age Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('lstm_age_training_history.png')\n",
    "plt.show()\n",
    "\n",
    "# Visualize training history for gender model\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Gender accuracy\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(gender_history.history['accuracy'], label='Train Gender Accuracy')\n",
    "plt.plot(gender_history.history['val_accuracy'], label='Validation Gender Accuracy')\n",
    "plt.title('LSTM Gender Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# Gender loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(gender_history.history['loss'], label='Train Gender Loss')\n",
    "plt.plot(gender_history.history['val_loss'], label='Validation Gender Loss')\n",
    "plt.title('LSTM Gender Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('lstm_gender_training_history.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca410b1103a6624e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Example of how to load the model and use it for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443479341485d75b",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def predict_age_and_gender_with_lstm(image_path):\n",
    "    \"\"\"\n",
    "    Load pre-trained LSTM models and predict age and gender from an image.\n",
    "    \n",
    "    Args:\n",
    "        image_path (str): Path to the image file\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (age_range, gender) as strings\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    import os\n",
    "    import joblib\n",
    "    from tensorflow.keras.models import load_model\n",
    "    from img2vec import rgb2emb\n",
    "    \n",
    "    # Check if the image exists\n",
    "    if not os.path.exists(image_path):\n",
    "        return \"Error: Image not found\"\n",
    "    \n",
    "    # Load the models\n",
    "    try:\n",
    "        age_model = load_model('lstm_age_model.h5')\n",
    "        gender_model = load_model('lstm_gender_model.h5')\n",
    "    except Exception as e:\n",
    "        return f\"Error loading models: {str(e)}\"\n",
    "    \n",
    "    # Load the encoders\n",
    "    try:\n",
    "        age_encoder = joblib.load('age_encoder.pkl')\n",
    "        gender_encoder = joblib.load('gender_encoder.pkl')\n",
    "        scaler = joblib.load('feature_scaler.pkl')\n",
    "    except Exception as e:\n",
    "        return f\"Error loading encoders: {str(e)}\"\n",
    "    \n",
    "    # Extract features from the image\n",
    "    try:\n",
    "        # Convert to batch format (list with single image)\n",
    "        features = rgb2emb([image_path])\n",
    "        \n",
    "        # Scale features\n",
    "        features_scaled = scaler.transform(features)\n",
    "        \n",
    "        # Reshape for LSTM input\n",
    "        features_lstm = reshape_features_for_lstm(features_scaled, time_steps=16)\n",
    "        \n",
    "        # Make predictions\n",
    "        age_pred = np.argmax(age_model.predict(features_lstm), axis=1)[0]\n",
    "        gender_pred = np.argmax(gender_model.predict(features_lstm), axis=1)[0]\n",
    "        \n",
    "        # Convert numerical predictions to original labels\n",
    "        age_range = age_encoder.inverse_transform([age_pred])[0]\n",
    "        gender = gender_encoder.inverse_transform([gender_pred])[0]\n",
    "        \n",
    "        return (age_range, gender)\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"Error during prediction: {str(e)}\"\n",
    "\n",
    "# Example usage:\n",
    "age_range, gender = predict_age_and_gender_with_lstm(\"../../img.jpg\")\n",
    "print(f\"Predicted Age Range: {age_range}\")\n",
    "print(f\"Predicted Gender: {gender}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf376d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
