{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from img2vec import rgb2emb\n",
    "import math\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from joblib import dump, load\n",
    "\n",
    "# Define the batch size for data processing\n",
    "batch_size = 64\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-26T15:48:36.362534200Z",
     "start_time": "2025-02-26T15:48:30.719076700Z"
    }
   },
   "id": "b7fdfda1b28f7d93"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### read the data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a63df6bf1d9882ce"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(os.path.join('..', '..', 'data', 'train.csv'))\n",
    "val_data = pd.read_csv(os.path.join('..', '..', 'data', 'val.csv'))\n",
    "test_data = pd.read_csv(os.path.join('..', '..', 'data', 'test.csv'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-26T15:48:36.429338100Z",
     "start_time": "2025-02-26T15:48:36.369090400Z"
    }
   },
   "id": "51d1d26e45f4b741"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### add the path of the images"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d541bc891ff22bf2"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "         user_id  face_id                original_image        age gender  \\\n0    9855553@N08     1581  11658657103_4485e3f5ac_o.jpg  (60, 100)      m   \n1  114841417@N06      502  12059583524_606ca96139_o.jpg   (15, 20)      m   \n2   66870968@N06     1227  11326189206_e08bdf6dfd_o.jpg   (25, 32)      m   \n3    8187011@N06      988  11133041085_e2ee5e12cb_o.jpg     (0, 2)      u   \n4  114841417@N06      485  12059753735_7141b5443c_o.jpg   (15, 20)      f   \n\n                                            img_path  \n0  ..\\..\\data\\faces\\9855553@N08\\coarse_tilt_align...  \n1  ..\\..\\data\\faces\\114841417@N06\\coarse_tilt_ali...  \n2  ..\\..\\data\\faces\\66870968@N06\\coarse_tilt_alig...  \n3  ..\\..\\data\\faces\\8187011@N06\\coarse_tilt_align...  \n4  ..\\..\\data\\faces\\114841417@N06\\coarse_tilt_ali...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user_id</th>\n      <th>face_id</th>\n      <th>original_image</th>\n      <th>age</th>\n      <th>gender</th>\n      <th>img_path</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>9855553@N08</td>\n      <td>1581</td>\n      <td>11658657103_4485e3f5ac_o.jpg</td>\n      <td>(60, 100)</td>\n      <td>m</td>\n      <td>..\\..\\data\\faces\\9855553@N08\\coarse_tilt_align...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>114841417@N06</td>\n      <td>502</td>\n      <td>12059583524_606ca96139_o.jpg</td>\n      <td>(15, 20)</td>\n      <td>m</td>\n      <td>..\\..\\data\\faces\\114841417@N06\\coarse_tilt_ali...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>66870968@N06</td>\n      <td>1227</td>\n      <td>11326189206_e08bdf6dfd_o.jpg</td>\n      <td>(25, 32)</td>\n      <td>m</td>\n      <td>..\\..\\data\\faces\\66870968@N06\\coarse_tilt_alig...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>8187011@N06</td>\n      <td>988</td>\n      <td>11133041085_e2ee5e12cb_o.jpg</td>\n      <td>(0, 2)</td>\n      <td>u</td>\n      <td>..\\..\\data\\faces\\8187011@N06\\coarse_tilt_align...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>114841417@N06</td>\n      <td>485</td>\n      <td>12059753735_7141b5443c_o.jpg</td>\n      <td>(15, 20)</td>\n      <td>f</td>\n      <td>..\\..\\data\\faces\\114841417@N06\\coarse_tilt_ali...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def construct_img_path(row):\n",
    "    return os.path.join(\"..\", \"..\", \"data\", \"faces\", row['user_id'],\n",
    "                        \"coarse_tilt_aligned_face.\" + str(row['face_id']) + \".\" + row['original_image'])\n",
    "\n",
    "\n",
    "train_data['img_path'] = train_data.apply(construct_img_path, axis=1)\n",
    "val_data['img_path'] = val_data.apply(construct_img_path, axis=1)\n",
    "test_data['img_path'] = test_data.apply(construct_img_path, axis=1)\n",
    "train_data.head(5)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-26T15:48:36.851116300Z",
     "start_time": "2025-02-26T15:48:36.431342300Z"
    }
   },
   "id": "7b1d46fc867cb10a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### add column for check if the image exists\n",
    "it will help us to detect if there is any missing image, or if there is any bug in the path construction"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8df65de30e5e25a7"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "         user_id  face_id                original_image        age gender  \\\n0    9855553@N08     1581  11658657103_4485e3f5ac_o.jpg  (60, 100)      m   \n1  114841417@N06      502  12059583524_606ca96139_o.jpg   (15, 20)      m   \n2   66870968@N06     1227  11326189206_e08bdf6dfd_o.jpg   (25, 32)      m   \n3    8187011@N06      988  11133041085_e2ee5e12cb_o.jpg     (0, 2)      u   \n4  114841417@N06      485  12059753735_7141b5443c_o.jpg   (15, 20)      f   \n\n                                            img_path  img_exists  \n0  ..\\..\\data\\faces\\9855553@N08\\coarse_tilt_align...        True  \n1  ..\\..\\data\\faces\\114841417@N06\\coarse_tilt_ali...        True  \n2  ..\\..\\data\\faces\\66870968@N06\\coarse_tilt_alig...        True  \n3  ..\\..\\data\\faces\\8187011@N06\\coarse_tilt_align...        True  \n4  ..\\..\\data\\faces\\114841417@N06\\coarse_tilt_ali...        True  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user_id</th>\n      <th>face_id</th>\n      <th>original_image</th>\n      <th>age</th>\n      <th>gender</th>\n      <th>img_path</th>\n      <th>img_exists</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>9855553@N08</td>\n      <td>1581</td>\n      <td>11658657103_4485e3f5ac_o.jpg</td>\n      <td>(60, 100)</td>\n      <td>m</td>\n      <td>..\\..\\data\\faces\\9855553@N08\\coarse_tilt_align...</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>114841417@N06</td>\n      <td>502</td>\n      <td>12059583524_606ca96139_o.jpg</td>\n      <td>(15, 20)</td>\n      <td>m</td>\n      <td>..\\..\\data\\faces\\114841417@N06\\coarse_tilt_ali...</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>66870968@N06</td>\n      <td>1227</td>\n      <td>11326189206_e08bdf6dfd_o.jpg</td>\n      <td>(25, 32)</td>\n      <td>m</td>\n      <td>..\\..\\data\\faces\\66870968@N06\\coarse_tilt_alig...</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>8187011@N06</td>\n      <td>988</td>\n      <td>11133041085_e2ee5e12cb_o.jpg</td>\n      <td>(0, 2)</td>\n      <td>u</td>\n      <td>..\\..\\data\\faces\\8187011@N06\\coarse_tilt_align...</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>114841417@N06</td>\n      <td>485</td>\n      <td>12059753735_7141b5443c_o.jpg</td>\n      <td>(15, 20)</td>\n      <td>f</td>\n      <td>..\\..\\data\\faces\\114841417@N06\\coarse_tilt_ali...</td>\n      <td>True</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['img_exists'] = train_data['img_path'].apply(os.path.exists)\n",
    "val_data['img_exists'] = val_data['img_path'].apply(os.path.exists)\n",
    "test_data['img_exists'] = test_data['img_path'].apply(os.path.exists)\n",
    "\n",
    "train_data.head(5)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-26T15:48:39.078754300Z",
     "start_time": "2025-02-26T15:48:36.857170400Z"
    }
   },
   "id": "2c915be97d0fd9eb"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gender_age classes: ['f_(0, 2)' 'f_(15, 20)' 'f_(25, 32)' 'f_(38, 43)' 'f_(4, 6)' 'f_(48, 53)'\n",
      " 'f_(60, 100)' 'f_(8, 23)' 'm_(0, 2)' 'm_(15, 20)' 'm_(25, 32)'\n",
      " 'm_(38, 43)' 'm_(4, 6)' 'm_(48, 53)' 'm_(60, 100)' 'm_(8, 23)' 'u_(0, 2)'\n",
      " 'u_(25, 32)' 'u_(60, 100)' 'u_(8, 23)']\n"
     ]
    },
    {
     "data": {
      "text/plain": "         user_id  face_id                original_image        age gender  \\\n0    9855553@N08     1581  11658657103_4485e3f5ac_o.jpg  (60, 100)      m   \n1  114841417@N06      502  12059583524_606ca96139_o.jpg   (15, 20)      m   \n2   66870968@N06     1227  11326189206_e08bdf6dfd_o.jpg   (25, 32)      m   \n3    8187011@N06      988  11133041085_e2ee5e12cb_o.jpg     (0, 2)      u   \n4  114841417@N06      485  12059753735_7141b5443c_o.jpg   (15, 20)      f   \n\n                                            img_path  img_exists  \\\n0  ..\\..\\data\\faces\\9855553@N08\\coarse_tilt_align...        True   \n1  ..\\..\\data\\faces\\114841417@N06\\coarse_tilt_ali...        True   \n2  ..\\..\\data\\faces\\66870968@N06\\coarse_tilt_alig...        True   \n3  ..\\..\\data\\faces\\8187011@N06\\coarse_tilt_align...        True   \n4  ..\\..\\data\\faces\\114841417@N06\\coarse_tilt_ali...        True   \n\n   gender_age_label  \n0                14  \n1                 9  \n2                10  \n3                16  \n4                 1  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user_id</th>\n      <th>face_id</th>\n      <th>original_image</th>\n      <th>age</th>\n      <th>gender</th>\n      <th>img_path</th>\n      <th>img_exists</th>\n      <th>gender_age_label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>9855553@N08</td>\n      <td>1581</td>\n      <td>11658657103_4485e3f5ac_o.jpg</td>\n      <td>(60, 100)</td>\n      <td>m</td>\n      <td>..\\..\\data\\faces\\9855553@N08\\coarse_tilt_align...</td>\n      <td>True</td>\n      <td>14</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>114841417@N06</td>\n      <td>502</td>\n      <td>12059583524_606ca96139_o.jpg</td>\n      <td>(15, 20)</td>\n      <td>m</td>\n      <td>..\\..\\data\\faces\\114841417@N06\\coarse_tilt_ali...</td>\n      <td>True</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>66870968@N06</td>\n      <td>1227</td>\n      <td>11326189206_e08bdf6dfd_o.jpg</td>\n      <td>(25, 32)</td>\n      <td>m</td>\n      <td>..\\..\\data\\faces\\66870968@N06\\coarse_tilt_alig...</td>\n      <td>True</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>8187011@N06</td>\n      <td>988</td>\n      <td>11133041085_e2ee5e12cb_o.jpg</td>\n      <td>(0, 2)</td>\n      <td>u</td>\n      <td>..\\..\\data\\faces\\8187011@N06\\coarse_tilt_align...</td>\n      <td>True</td>\n      <td>16</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>114841417@N06</td>\n      <td>485</td>\n      <td>12059753735_7141b5443c_o.jpg</td>\n      <td>(15, 20)</td>\n      <td>f</td>\n      <td>..\\..\\data\\faces\\114841417@N06\\coarse_tilt_ali...</td>\n      <td>True</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def encode_gender_age(train_data, val_data, test_data):\n",
    "    \"\"\"Encodes gender and age combinations into a single label, handling unseen labels.\"\"\"\n",
    "\n",
    "    train_data['gender_age_combined'] = train_data['gender'].astype(str) + '_' + train_data['age'].astype(str)\n",
    "    val_data['gender_age_combined'] = val_data['gender'].astype(str) + '_' + val_data['age'].astype(str)\n",
    "    test_data['gender_age_combined'] = test_data['gender'].astype(str) + '_' + test_data['age'].astype(str)\n",
    "\n",
    "    gender_age_encoder = LabelEncoder()\n",
    "    train_data['gender_age_label'] = gender_age_encoder.fit_transform(train_data['gender_age_combined'])\n",
    "\n",
    "    # Function to handle unseen labels\n",
    "    def transform_with_unknown(data, encoder):\n",
    "        known_classes = set(encoder.classes_)\n",
    "        data['gender_age_label'] = data['gender_age_combined'].apply(\n",
    "            lambda x: encoder.transform([x])[0] if x in known_classes else -1\n",
    "        ) # assign -1 to unseen label.\n",
    "        return data\n",
    "\n",
    "    val_data = transform_with_unknown(val_data, gender_age_encoder)\n",
    "    test_data = transform_with_unknown(test_data, gender_age_encoder)\n",
    "\n",
    "    num_classes = len(gender_age_encoder.classes_)\n",
    "\n",
    "    train_data.drop('gender_age_combined', axis=1, inplace=True)\n",
    "    val_data.drop('gender_age_combined', axis=1, inplace=True)\n",
    "    test_data.drop('gender_age_combined', axis=1, inplace=True)\n",
    "\n",
    "    return train_data, val_data, test_data, num_classes, gender_age_encoder\n",
    "\n",
    "train_data, val_data, test_data, num_classes, gender_age_encoder = encode_gender_age(train_data, val_data, test_data)\n",
    "\n",
    "num_classes = len(gender_age_encoder.classes_)\n",
    "print(\"gender_age classes:\", gender_age_encoder.classes_)\n",
    "train_data.head(5)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-26T15:48:40.791165400Z",
     "start_time": "2025-02-26T15:48:39.123495900Z"
    }
   },
   "id": "13f972e81bf1aa"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Filter out any rows where the image doesn't exist"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bcb2fa7830f47cf8"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "train_data_filtered = train_data[train_data['img_exists'] == True]\n",
    "val_data_filtered = val_data[val_data['img_exists'] == True]\n",
    "test_data_filtered = test_data[test_data['img_exists'] == True]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-26T15:48:40.855618800Z",
     "start_time": "2025-02-26T15:48:40.790161400Z"
    }
   },
   "id": "facd05e692b7b1f3"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "c6ed72b1ccd0d0ae"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Define a generator function to process images in batches"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3af64d1ee4038b96"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def image_batch_generator(image_paths, labels, batch_size):\n",
    "    num_samples = len(image_paths)\n",
    "    num_batches = math.ceil(num_samples / batch_size)\n",
    "\n",
    "    for i in range(num_batches):\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = min((i + 1) * batch_size, num_samples)\n",
    "\n",
    "        batch_paths = image_paths[start_idx:end_idx]\n",
    "        batch_features = rgb2emb(batch_paths) / 255.0  # Normalize to [0,1]\n",
    "        batch_labels = labels[start_idx:end_idx]\n",
    "\n",
    "        yield batch_features, batch_labels\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-26T15:48:40.918172300Z",
     "start_time": "2025-02-26T15:48:40.818823700Z"
    }
   },
   "id": "613ac4858cc2e830"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Extract image paths and labels"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "de85480f4cd99025"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "train_image_paths = train_data_filtered['img_path'].tolist()\n",
    "train_labels = train_data_filtered['gender_age_label'].values\n",
    "\n",
    "val_image_paths = val_data_filtered['img_path'].tolist()\n",
    "val_labels = val_data_filtered['gender_age_label'].values\n",
    "\n",
    "test_image_paths = test_data_filtered['img_path'].tolist()\n",
    "test_labels = test_data_filtered['gender_age_label'].values"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-26T15:48:40.951746200Z",
     "start_time": "2025-02-26T15:48:40.830907700Z"
    }
   },
   "id": "c2fc8e2f5e6e670a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Print dataset sizes"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "85a0755002651c63"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 11856\n",
      "Validation samples: 2964\n",
      "Test samples: 3731\n",
      "Number of classes: 20\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training samples: {len(train_image_paths)}\")\n",
    "print(f\"Validation samples: {len(val_image_paths)}\")\n",
    "print(f\"Test samples: {len(test_image_paths)}\")\n",
    "print(f\"Number of classes: {num_classes}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-26T15:48:40.953273700Z",
     "start_time": "2025-02-26T15:48:40.851093200Z"
    }
   },
   "id": "dfb5764aa61d8166"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Extract features for training and validation\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d90a1a384ac289e1"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features for training data...\n",
      "Processing all 11856 training samples\n"
     ]
    }
   ],
   "source": [
    "print(\"Extracting features for training data...\")\n",
    "print(f\"Processing all {len(train_image_paths)} training samples\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-26T15:48:40.997093300Z",
     "start_time": "2025-02-26T15:48:40.865691800Z"
    }
   },
   "id": "f42bb6bd36c9a27a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Define a function to extract features in batches\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f82481e06b30e2b5"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def extract_features_in_batches(image_paths, batch_size=64):\n",
    "    \"\"\"Extract features from images in batches to manage memory.\"\"\"\n",
    "    num_samples = len(image_paths)\n",
    "    num_batches = math.ceil(num_samples / batch_size)\n",
    "    all_features = []\n",
    "\n",
    "    for i in range(num_batches):\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = min((i + 1) * batch_size, num_samples)\n",
    "        \n",
    "        print(f\"\\rProcessing batch {i+1}/{num_batches}\", end=\"\")\n",
    "        \n",
    "        batch_paths = image_paths[start_idx:end_idx]\n",
    "        batch_features = rgb2emb(batch_paths) / 255.0  # Normalize to [0,1]\n",
    "        \n",
    "        all_features.append(batch_features)\n",
    "    \n",
    "    print(\"\\nFeature extraction complete.\")\n",
    "    return np.vstack(all_features)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-26T15:48:41.105658900Z",
     "start_time": "2025-02-26T15:48:40.879907400Z"
    }
   },
   "id": "99d90679478f36d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m2/2\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m5s\u001B[0m 954ms/step\n",
      "\u001B[1m2/2\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m5s\u001B[0m 857ms/step\n",
      "\u001B[1m2/2\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m6s\u001B[0m 1s/step\n",
      "\u001B[1m2/2\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 850ms/step\n",
      "Processing batch 5/186WARNING:tensorflow:5 out of the last 9 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x0000021FE29695A0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001B[1m2/2\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m5s\u001B[0m 831ms/step\n",
      "Processing batch 6/186WARNING:tensorflow:6 out of the last 11 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x0000021FE2A49090> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001B[1m2/2\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 840ms/step\n",
      "\u001B[1m2/2\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 728ms/step\n",
      "\u001B[1m2/2\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 813ms/step\n",
      "\u001B[1m2/2\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 786ms/step\n",
      "\u001B[1m2/2\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m7s\u001B[0m 906ms/step\n",
      "\u001B[1m2/2\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m8s\u001B[0m 1s/step\n",
      "\u001B[1m2/2\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m6s\u001B[0m 1s/step\n",
      "\u001B[1m2/2\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m7s\u001B[0m 2s/step\n",
      "Processing batch 14/186"
     ]
    }
   ],
   "source": [
    "X_train = extract_features_in_batches(train_image_paths, batch_size=batch_size)\n",
    "y_train = train_labels\n",
    "\n",
    "print(\"Extracting features for validation data...\")\n",
    "print(f\"Processing all {len(val_image_paths)} validation samples\")\n",
    "\n",
    "X_val = extract_features_in_batches(val_image_paths, batch_size=batch_size)\n",
    "y_val = val_labels\n",
    "\n",
    "\n",
    "print(\"Creating and training LinearSVC model...\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2025-02-26T15:48:40.900028200Z"
    }
   },
   "id": "7f88029e0bae8d50"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create a pipeline with scaling and LinearSVC\n",
    "This is much more memory efficient for large datasets"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "afd81221629cef95"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),  # Scale features for better performance\n",
    "    ('svm', LinearSVC(\n",
    "        C=1.0,                    # Regularization parameter\n",
    "        penalty='l2',             # Penalty norm (l2 is default)\n",
    "        loss='squared_hinge',     # Loss function\n",
    "        dual=False,               # Prefer dual=False when n_samples > n_features\n",
    "        max_iter=1000,            # Increase if needed for convergence\n",
    "        class_weight='balanced',  # Handle class imbalance\n",
    "        verbose=1,\n",
    "        random_state=42\n",
    "    ))\n",
    "])"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "91f4a164009719c0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### define the model and train it"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6e063e59977f81a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = CalibratedClassifierCV(pipeline, cv=3)\n",
    "model.fit(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "a4970594951e3f75"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "e045def1939c7a3a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Evaluate the model on test data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "db63f493a1683170"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "print(\"Evaluating the model...\")\n",
    "train_preds = model.predict(X_train)\n",
    "train_accuracy = (train_preds == y_train).mean()\n",
    "print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
    "\n",
    "val_preds = model.predict(X_val)\n",
    "val_accuracy = (val_preds == y_val).mean()\n",
    "print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "# Test the model on the entire test set\n",
    "print(f\"Extracting features for all {len(test_image_paths)} test samples...\")\n",
    "X_test = extract_features_in_batches(test_image_paths, batch_size=batch_size)\n",
    "y_test = test_labels\n",
    "\n",
    "test_preds = model.predict(X_test)\n",
    "test_accuracy = (test_preds == y_test).mean()\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "414fc01e63806105"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Visualize results"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fd36e6294c8bf82d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 10))\n",
    "cm = confusion_matrix(y_test, test_preds)\n",
    "# Use a mask for the heatmap to only show cells with non-zero values\n",
    "mask = cm == 0\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', mask=mask,\n",
    "           xticklabels=gender_age_encoder.classes_,\n",
    "           yticklabels=gender_age_encoder.classes_)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.tight_layout()\n",
    "plt.savefig('linear_svc_confusion_matrix.png')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "8ed8e315739a638c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Classification report"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "536a98145ffe3f2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, test_preds, target_names=gender_age_encoder.classes_))"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "2f20747921cf3e89"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Save the model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e21e8b64d196b50"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dump(model, 'linear_svc_gender_age_classifier.joblib')\n",
    "dump(gender_age_encoder, 'gender_age_encoder.joblib')\n",
    "print(\"Model saved successfully.\")"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "197bae6e0a4ce665"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Function to predict gender_age range for a new image\n",
    "def predict_gender_age(image_path, model, gender_age_encoder):\n",
    "    \"\"\"Predict gender and age for a given face image.\"\"\"\n",
    "    # Extract features\n",
    "    features = rgb2emb([image_path]) / 255.0\n",
    "    \n",
    "    # Apply the same scaling used during training (if using a pipeline with StandardScaler)\n",
    "    if hasattr(model, 'named_steps') and 'scaler' in model.named_steps:\n",
    "        features = model.named_steps['scaler'].transform(features)\n",
    "    \n",
    "    # Make prediction\n",
    "    pred_class = model.predict(features)[0]\n",
    "    \n",
    "    # Get class probabilities if available\n",
    "    if hasattr(model, 'predict_proba'):\n",
    "        pred_probs = model.predict_proba(features)[0]\n",
    "        confidence = pred_probs[pred_class]\n",
    "    else:\n",
    "        # For LinearSVC without calibration, we can't get probabilities\n",
    "        # Use decision function instead as a rough proxy for confidence\n",
    "        decision_values = model.decision_function(features)\n",
    "        # Normalize to [0, 1] range roughly\n",
    "        confidence = 1 / (1 + np.exp(-np.abs(decision_values[0][pred_class])))\n",
    "    \n",
    "    # Convert to gender_age range\n",
    "    pred_gender_age_range = gender_age_encoder.classes_[pred_class]\n",
    "    \n",
    "    return pred_gender_age_range, confidence\n",
    "\n",
    "# Example usage:\n",
    "\"\"\"\n",
    "# Load the model\n",
    "image_path = \"path/to/new/face/image.jpg\"\n",
    "model = load('linear_svc_gender_age_classifier.joblib')\n",
    "gender_age_encoder = load('gender_age_encoder.joblib')\n",
    "\n",
    "# Make prediction\n",
    "pred_gender_age, confidence = predict_gender_age(image_path, model, gender_age_encoder)\n",
    "print(f\"Predicted gender_age range: {pred_gender_age} with confidence {confidence:.2f}\")\n",
    "\"\"\"\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "a676b8e8ede7b14d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "c706a072368c3614"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
