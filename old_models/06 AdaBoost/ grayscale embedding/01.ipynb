{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-10T08:39:57.369330700Z",
     "start_time": "2025-03-10T08:39:57.352931300Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import math\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from joblib import dump, load\n",
    "import numpy as np\n",
    "# Define the batch size for data processing\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf67410f652b98c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ba693e7da77017",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-10T08:25:34.986303400Z",
     "start_time": "2025-03-10T08:25:34.887147100Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(os.path.join('..', '..', 'data', 'train.csv'))\n",
    "val_data = pd.read_csv(os.path.join('..', '..', 'data', 'val.csv'))\n",
    "test_data = pd.read_csv(os.path.join('..', '..', 'data', 'test.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3bca6182fbb46bc",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### add the path of the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15bde55648f6cbcf",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-10T08:25:35.269778100Z",
     "start_time": "2025-03-10T08:25:35.008191600Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "         user_id  face_id                original_image        age gender  \\\n0    9855553@N08     1581  11658657103_4485e3f5ac_o.jpg  (60, 100)      m   \n1  114841417@N06      502  12059583524_606ca96139_o.jpg   (15, 20)      m   \n2   66870968@N06     1227  11326189206_e08bdf6dfd_o.jpg   (25, 32)      m   \n3    8187011@N06      988  11133041085_e2ee5e12cb_o.jpg     (0, 2)      u   \n4  114841417@N06      485  12059753735_7141b5443c_o.jpg   (15, 20)      f   \n\n                                            img_path  \n0  ..\\..\\data\\faces\\9855553@N08\\coarse_tilt_align...  \n1  ..\\..\\data\\faces\\114841417@N06\\coarse_tilt_ali...  \n2  ..\\..\\data\\faces\\66870968@N06\\coarse_tilt_alig...  \n3  ..\\..\\data\\faces\\8187011@N06\\coarse_tilt_align...  \n4  ..\\..\\data\\faces\\114841417@N06\\coarse_tilt_ali...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user_id</th>\n      <th>face_id</th>\n      <th>original_image</th>\n      <th>age</th>\n      <th>gender</th>\n      <th>img_path</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>9855553@N08</td>\n      <td>1581</td>\n      <td>11658657103_4485e3f5ac_o.jpg</td>\n      <td>(60, 100)</td>\n      <td>m</td>\n      <td>..\\..\\data\\faces\\9855553@N08\\coarse_tilt_align...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>114841417@N06</td>\n      <td>502</td>\n      <td>12059583524_606ca96139_o.jpg</td>\n      <td>(15, 20)</td>\n      <td>m</td>\n      <td>..\\..\\data\\faces\\114841417@N06\\coarse_tilt_ali...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>66870968@N06</td>\n      <td>1227</td>\n      <td>11326189206_e08bdf6dfd_o.jpg</td>\n      <td>(25, 32)</td>\n      <td>m</td>\n      <td>..\\..\\data\\faces\\66870968@N06\\coarse_tilt_alig...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>8187011@N06</td>\n      <td>988</td>\n      <td>11133041085_e2ee5e12cb_o.jpg</td>\n      <td>(0, 2)</td>\n      <td>u</td>\n      <td>..\\..\\data\\faces\\8187011@N06\\coarse_tilt_align...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>114841417@N06</td>\n      <td>485</td>\n      <td>12059753735_7141b5443c_o.jpg</td>\n      <td>(15, 20)</td>\n      <td>f</td>\n      <td>..\\..\\data\\faces\\114841417@N06\\coarse_tilt_ali...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def construct_img_path(row):\n",
    "    return os.path.join(\"..\", \"..\", \"data\", \"faces\", row['user_id'],\n",
    "                        \"coarse_tilt_aligned_face.\" + str(row['face_id']) + \".\" + row['original_image'])\n",
    "\n",
    "\n",
    "train_data['img_path'] = train_data.apply(construct_img_path, axis=1)\n",
    "val_data['img_path'] = val_data.apply(construct_img_path, axis=1)\n",
    "test_data['img_path'] = test_data.apply(construct_img_path, axis=1)\n",
    "train_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ccbaf87304f93f",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### add column for check if the image exists\n",
    "it will help us to detect if there is any missing image, or if there is any bug in the path construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c493e5d73810fb1",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-10T08:25:37.064015700Z",
     "start_time": "2025-03-10T08:25:35.262844500Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "         user_id  face_id                original_image        age gender  \\\n0    9855553@N08     1581  11658657103_4485e3f5ac_o.jpg  (60, 100)      m   \n1  114841417@N06      502  12059583524_606ca96139_o.jpg   (15, 20)      m   \n2   66870968@N06     1227  11326189206_e08bdf6dfd_o.jpg   (25, 32)      m   \n3    8187011@N06      988  11133041085_e2ee5e12cb_o.jpg     (0, 2)      u   \n4  114841417@N06      485  12059753735_7141b5443c_o.jpg   (15, 20)      f   \n\n                                            img_path  img_exists  \n0  ..\\..\\data\\faces\\9855553@N08\\coarse_tilt_align...        True  \n1  ..\\..\\data\\faces\\114841417@N06\\coarse_tilt_ali...        True  \n2  ..\\..\\data\\faces\\66870968@N06\\coarse_tilt_alig...        True  \n3  ..\\..\\data\\faces\\8187011@N06\\coarse_tilt_align...        True  \n4  ..\\..\\data\\faces\\114841417@N06\\coarse_tilt_ali...        True  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user_id</th>\n      <th>face_id</th>\n      <th>original_image</th>\n      <th>age</th>\n      <th>gender</th>\n      <th>img_path</th>\n      <th>img_exists</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>9855553@N08</td>\n      <td>1581</td>\n      <td>11658657103_4485e3f5ac_o.jpg</td>\n      <td>(60, 100)</td>\n      <td>m</td>\n      <td>..\\..\\data\\faces\\9855553@N08\\coarse_tilt_align...</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>114841417@N06</td>\n      <td>502</td>\n      <td>12059583524_606ca96139_o.jpg</td>\n      <td>(15, 20)</td>\n      <td>m</td>\n      <td>..\\..\\data\\faces\\114841417@N06\\coarse_tilt_ali...</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>66870968@N06</td>\n      <td>1227</td>\n      <td>11326189206_e08bdf6dfd_o.jpg</td>\n      <td>(25, 32)</td>\n      <td>m</td>\n      <td>..\\..\\data\\faces\\66870968@N06\\coarse_tilt_alig...</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>8187011@N06</td>\n      <td>988</td>\n      <td>11133041085_e2ee5e12cb_o.jpg</td>\n      <td>(0, 2)</td>\n      <td>u</td>\n      <td>..\\..\\data\\faces\\8187011@N06\\coarse_tilt_align...</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>114841417@N06</td>\n      <td>485</td>\n      <td>12059753735_7141b5443c_o.jpg</td>\n      <td>(15, 20)</td>\n      <td>f</td>\n      <td>..\\..\\data\\faces\\114841417@N06\\coarse_tilt_ali...</td>\n      <td>True</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['img_exists'] = train_data['img_path'].apply(os.path.exists)\n",
    "val_data['img_exists'] = val_data['img_path'].apply(os.path.exists)\n",
    "test_data['img_exists'] = test_data['img_path'].apply(os.path.exists)\n",
    "\n",
    "train_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d940523377f8b25",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-10T08:25:37.709280600Z",
     "start_time": "2025-03-10T08:25:37.062004900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gender_age classes: ['f_(0, 2)' 'f_(15, 20)' 'f_(25, 32)' 'f_(38, 43)' 'f_(4, 6)' 'f_(48, 53)'\n",
      " 'f_(60, 100)' 'f_(8, 23)' 'm_(0, 2)' 'm_(15, 20)' 'm_(25, 32)'\n",
      " 'm_(38, 43)' 'm_(4, 6)' 'm_(48, 53)' 'm_(60, 100)' 'm_(8, 23)' 'u_(0, 2)'\n",
      " 'u_(25, 32)' 'u_(60, 100)' 'u_(8, 23)']\n"
     ]
    },
    {
     "data": {
      "text/plain": "         user_id  face_id                original_image        age gender  \\\n0    9855553@N08     1581  11658657103_4485e3f5ac_o.jpg  (60, 100)      m   \n1  114841417@N06      502  12059583524_606ca96139_o.jpg   (15, 20)      m   \n2   66870968@N06     1227  11326189206_e08bdf6dfd_o.jpg   (25, 32)      m   \n3    8187011@N06      988  11133041085_e2ee5e12cb_o.jpg     (0, 2)      u   \n4  114841417@N06      485  12059753735_7141b5443c_o.jpg   (15, 20)      f   \n\n                                            img_path  img_exists  \\\n0  ..\\..\\data\\faces\\9855553@N08\\coarse_tilt_align...        True   \n1  ..\\..\\data\\faces\\114841417@N06\\coarse_tilt_ali...        True   \n2  ..\\..\\data\\faces\\66870968@N06\\coarse_tilt_alig...        True   \n3  ..\\..\\data\\faces\\8187011@N06\\coarse_tilt_align...        True   \n4  ..\\..\\data\\faces\\114841417@N06\\coarse_tilt_ali...        True   \n\n   gender_age_label  \n0                14  \n1                 9  \n2                10  \n3                16  \n4                 1  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user_id</th>\n      <th>face_id</th>\n      <th>original_image</th>\n      <th>age</th>\n      <th>gender</th>\n      <th>img_path</th>\n      <th>img_exists</th>\n      <th>gender_age_label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>9855553@N08</td>\n      <td>1581</td>\n      <td>11658657103_4485e3f5ac_o.jpg</td>\n      <td>(60, 100)</td>\n      <td>m</td>\n      <td>..\\..\\data\\faces\\9855553@N08\\coarse_tilt_align...</td>\n      <td>True</td>\n      <td>14</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>114841417@N06</td>\n      <td>502</td>\n      <td>12059583524_606ca96139_o.jpg</td>\n      <td>(15, 20)</td>\n      <td>m</td>\n      <td>..\\..\\data\\faces\\114841417@N06\\coarse_tilt_ali...</td>\n      <td>True</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>66870968@N06</td>\n      <td>1227</td>\n      <td>11326189206_e08bdf6dfd_o.jpg</td>\n      <td>(25, 32)</td>\n      <td>m</td>\n      <td>..\\..\\data\\faces\\66870968@N06\\coarse_tilt_alig...</td>\n      <td>True</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>8187011@N06</td>\n      <td>988</td>\n      <td>11133041085_e2ee5e12cb_o.jpg</td>\n      <td>(0, 2)</td>\n      <td>u</td>\n      <td>..\\..\\data\\faces\\8187011@N06\\coarse_tilt_align...</td>\n      <td>True</td>\n      <td>16</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>114841417@N06</td>\n      <td>485</td>\n      <td>12059753735_7141b5443c_o.jpg</td>\n      <td>(15, 20)</td>\n      <td>f</td>\n      <td>..\\..\\data\\faces\\114841417@N06\\coarse_tilt_ali...</td>\n      <td>True</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def encode_gender_age(train_data, val_data, test_data):\n",
    "    \"\"\"Encodes gender and age combinations into a single label, handling unseen labels.\"\"\"\n",
    "\n",
    "    train_data['gender_age_combined'] = train_data['gender'].astype(str) + '_' + train_data['age'].astype(str)\n",
    "    val_data['gender_age_combined'] = val_data['gender'].astype(str) + '_' + val_data['age'].astype(str)\n",
    "    test_data['gender_age_combined'] = test_data['gender'].astype(str) + '_' + test_data['age'].astype(str)\n",
    "\n",
    "    gender_age_encoder = LabelEncoder()\n",
    "    train_data['gender_age_label'] = gender_age_encoder.fit_transform(train_data['gender_age_combined'])\n",
    "\n",
    "    # Function to handle unseen labels\n",
    "    def transform_with_unknown(data, encoder):\n",
    "        known_classes = set(encoder.classes_)\n",
    "        data['gender_age_label'] = data['gender_age_combined'].apply(\n",
    "            lambda x: encoder.transform([x])[0] if x in known_classes else -1\n",
    "        ) # assign -1 to unseen label.\n",
    "        return data\n",
    "\n",
    "    val_data = transform_with_unknown(val_data, gender_age_encoder)\n",
    "    test_data = transform_with_unknown(test_data, gender_age_encoder)\n",
    "\n",
    "    num_classes = len(gender_age_encoder.classes_)\n",
    "\n",
    "    train_data.drop('gender_age_combined', axis=1, inplace=True)\n",
    "    val_data.drop('gender_age_combined', axis=1, inplace=True)\n",
    "    test_data.drop('gender_age_combined', axis=1, inplace=True)\n",
    "\n",
    "    return train_data, val_data, test_data, num_classes, gender_age_encoder\n",
    "\n",
    "train_data, val_data, test_data, num_classes, gender_age_encoder = encode_gender_age(train_data, val_data, test_data)\n",
    "\n",
    "num_classes = len(gender_age_encoder.classes_)\n",
    "print(\"gender_age classes:\", gender_age_encoder.classes_)\n",
    "train_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3967a7d1e840d381",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Filter out any rows where the image doesn't exist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7db58f446ed7ed8d",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-10T08:25:37.820801600Z",
     "start_time": "2025-03-10T08:25:37.703248400Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data_filtered = train_data[train_data['img_exists'] == True]\n",
    "val_data_filtered = val_data[val_data['img_exists'] == True]\n",
    "test_data_filtered = test_data[test_data['img_exists'] == True]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10479fab31f4f172",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Define a generator function to process images in batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40b839edec59102f",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-10T08:25:43.902195800Z",
     "start_time": "2025-03-10T08:25:37.738077Z"
    }
   },
   "outputs": [],
   "source": [
    "from img2vec import grayscale2emb\n",
    "\n",
    "\n",
    "def image_batch_generator(image_paths, labels, batch_size):\n",
    "    num_samples = len(image_paths)\n",
    "    num_batches = math.ceil(num_samples / batch_size)\n",
    "\n",
    "    for i in range(num_batches):\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = min((i + 1) * batch_size, num_samples)\n",
    "\n",
    "        batch_paths = image_paths[start_idx:end_idx]\n",
    "        batch_features = grayscale2emb(batch_paths) / 255.0  # Normalize to [0,1]\n",
    "        batch_labels = labels[start_idx:end_idx]\n",
    "\n",
    "        yield batch_features, batch_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da59c50f0be89fc",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Extract image paths and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15b28c169da16536",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-10T08:25:43.930834300Z",
     "start_time": "2025-03-10T08:25:43.911239700Z"
    }
   },
   "outputs": [],
   "source": [
    "train_image_paths = train_data_filtered['img_path'].tolist()\n",
    "train_labels = train_data_filtered['gender_age_label'].values\n",
    "\n",
    "val_image_paths = val_data_filtered['img_path'].tolist()\n",
    "val_labels = val_data_filtered['gender_age_label'].values\n",
    "\n",
    "test_image_paths = test_data_filtered['img_path'].tolist()\n",
    "test_labels = test_data_filtered['gender_age_label'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d4c562693322fe",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Print dataset sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8d723cb6011dc27",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-10T08:25:43.941733Z",
     "start_time": "2025-03-10T08:25:43.925062300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 11856\n",
      "Validation samples: 2964\n",
      "Test samples: 3731\n",
      "Number of classes: 20\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training samples: {len(train_image_paths)}\")\n",
    "print(f\"Validation samples: {len(val_image_paths)}\")\n",
    "print(f\"Test samples: {len(test_image_paths)}\")\n",
    "print(f\"Number of classes: {num_classes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540445fddb53d20a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Extract features for training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "544690004cb7efa1",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-10T08:25:43.982392600Z",
     "start_time": "2025-03-10T08:25:43.938281800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features for training data...\n",
      "Processing all 11856 training samples\n"
     ]
    }
   ],
   "source": [
    "print(\"Extracting features for training data...\")\n",
    "print(f\"Processing all {len(train_image_paths)} training samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7577176c90b39c29",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Define a function to extract features in batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2507def9ce8ee6db",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-10T08:40:04.009115900Z",
     "start_time": "2025-03-10T08:40:03.978119500Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_features_in_batches(image_paths, batch_size=64):\n",
    "    \"\"\"Extract features from images in batches to manage memory.\"\"\"\n",
    "    num_samples = len(image_paths)\n",
    "    num_batches = math.ceil(num_samples / batch_size)\n",
    "    all_features = []\n",
    "\n",
    "    for i in range(num_batches):\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = min((i + 1) * batch_size, num_samples)\n",
    "        print(f\"\\rProcessing batch {i+1}/{num_batches}\", end=\"\")\n",
    "        batch_paths = image_paths[start_idx:end_idx]\n",
    "        batch_features = grayscale2emb(batch_paths)\n",
    "\n",
    "        # Debugging output\n",
    "        print(f\"Batch features shape: {batch_features.shape}\")\n",
    "\n",
    "        # Ensure batch_features has correct shape (batch_size, 2048)\n",
    "        if batch_features.ndim != 2 or batch_features.shape[1] != 2048:\n",
    "            raise ValueError(f\"Unexpected feature shape: {batch_features.shape}\")\n",
    "        \n",
    "        all_features.append(batch_features)\n",
    "        \n",
    "    print(\"\\nFeature extraction complete.\")\n",
    "    return np.vstack(all_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4cb956c7cd2c93c5",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-03-10T08:40:07.739800500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 1/186Batch features shape: (64, 2048)\n",
      "Processing batch 2/186Batch features shape: (64, 2048)\n",
      "Processing batch 3/186Batch features shape: (64, 2048)\n",
      "Processing batch 4/186Batch features shape: (64, 2048)\n",
      "Processing batch 5/186Batch features shape: (64, 2048)\n",
      "Processing batch 6/186Batch features shape: (64, 2048)\n",
      "Processing batch 7/186Batch features shape: (64, 2048)\n",
      "Processing batch 8/186Batch features shape: (64, 2048)\n",
      "Processing batch 9/186Batch features shape: (64, 2048)\n",
      "Processing batch 10/186Batch features shape: (64, 2048)\n",
      "Processing batch 11/186Batch features shape: (64, 2048)\n",
      "Processing batch 12/186Batch features shape: (64, 2048)\n",
      "Processing batch 13/186Batch features shape: (64, 2048)\n",
      "Processing batch 14/186Batch features shape: (64, 2048)\n",
      "Processing batch 15/186Batch features shape: (64, 2048)\n",
      "Processing batch 16/186Batch features shape: (64, 2048)\n",
      "Processing batch 17/186Batch features shape: (64, 2048)\n",
      "Processing batch 18/186Batch features shape: (64, 2048)\n",
      "Processing batch 19/186Batch features shape: (64, 2048)\n",
      "Processing batch 20/186Batch features shape: (64, 2048)\n",
      "Processing batch 21/186Batch features shape: (64, 2048)\n",
      "Processing batch 22/186Batch features shape: (64, 2048)\n",
      "Processing batch 23/186Batch features shape: (64, 2048)\n",
      "Processing batch 24/186Batch features shape: (64, 2048)\n",
      "Processing batch 25/186Batch features shape: (64, 2048)\n",
      "Processing batch 26/186Batch features shape: (64, 2048)\n",
      "Processing batch 27/186Batch features shape: (64, 2048)\n",
      "Processing batch 28/186Batch features shape: (64, 2048)\n",
      "Processing batch 29/186Batch features shape: (64, 2048)\n",
      "Processing batch 30/186Batch features shape: (64, 2048)\n",
      "Processing batch 31/186Batch features shape: (64, 2048)\n",
      "Processing batch 32/186Batch features shape: (64, 2048)\n",
      "Processing batch 33/186Batch features shape: (64, 2048)\n",
      "Processing batch 34/186Batch features shape: (64, 2048)\n",
      "Processing batch 35/186Batch features shape: (64, 2048)\n",
      "Processing batch 36/186Batch features shape: (64, 2048)\n",
      "Processing batch 37/186Batch features shape: (64, 2048)\n",
      "Processing batch 38/186Batch features shape: (64, 2048)\n",
      "Processing batch 39/186Batch features shape: (64, 2048)\n",
      "Processing batch 40/186Batch features shape: (64, 2048)\n",
      "Processing batch 41/186Batch features shape: (64, 2048)\n",
      "Processing batch 42/186Batch features shape: (64, 2048)\n",
      "Processing batch 43/186Batch features shape: (64, 2048)\n",
      "Processing batch 44/186Batch features shape: (64, 2048)\n",
      "Processing batch 45/186Batch features shape: (64, 2048)\n",
      "Processing batch 46/186Batch features shape: (64, 2048)\n",
      "Processing batch 47/186Batch features shape: (64, 2048)\n",
      "Processing batch 48/186Batch features shape: (64, 2048)\n",
      "Processing batch 49/186Batch features shape: (64, 2048)\n",
      "Processing batch 50/186Batch features shape: (64, 2048)\n",
      "Processing batch 51/186Batch features shape: (64, 2048)\n",
      "Processing batch 52/186Batch features shape: (64, 2048)\n",
      "Processing batch 53/186Batch features shape: (64, 2048)\n",
      "Processing batch 54/186Batch features shape: (64, 2048)\n",
      "Processing batch 55/186Batch features shape: (64, 2048)\n",
      "Processing batch 56/186Batch features shape: (64, 2048)\n",
      "Processing batch 57/186Batch features shape: (64, 2048)\n",
      "Processing batch 58/186Batch features shape: (64, 2048)\n",
      "Processing batch 59/186Batch features shape: (64, 2048)\n",
      "Processing batch 60/186Batch features shape: (64, 2048)\n",
      "Processing batch 61/186Batch features shape: (64, 2048)\n",
      "Processing batch 62/186Batch features shape: (64, 2048)\n",
      "Processing batch 63/186Batch features shape: (64, 2048)\n",
      "Processing batch 64/186Batch features shape: (64, 2048)\n",
      "Processing batch 65/186Batch features shape: (64, 2048)\n",
      "Processing batch 66/186Batch features shape: (64, 2048)\n",
      "Processing batch 67/186Batch features shape: (64, 2048)\n",
      "Processing batch 68/186Batch features shape: (64, 2048)\n",
      "Processing batch 69/186Batch features shape: (64, 2048)\n",
      "Processing batch 70/186Batch features shape: (64, 2048)\n",
      "Processing batch 71/186Batch features shape: (64, 2048)\n",
      "Processing batch 72/186Batch features shape: (64, 2048)\n",
      "Processing batch 73/186Batch features shape: (64, 2048)\n",
      "Processing batch 74/186Batch features shape: (64, 2048)\n",
      "Processing batch 75/186Batch features shape: (64, 2048)\n",
      "Processing batch 76/186Batch features shape: (64, 2048)\n",
      "Processing batch 77/186Batch features shape: (64, 2048)\n",
      "Processing batch 78/186Batch features shape: (64, 2048)\n",
      "Processing batch 79/186Batch features shape: (64, 2048)\n",
      "Processing batch 80/186Batch features shape: (64, 2048)\n",
      "Processing batch 81/186Batch features shape: (64, 2048)\n",
      "Processing batch 82/186Batch features shape: (64, 2048)\n",
      "Processing batch 83/186Batch features shape: (64, 2048)\n",
      "Processing batch 84/186Batch features shape: (64, 2048)\n",
      "Processing batch 85/186Batch features shape: (64, 2048)\n",
      "Processing batch 86/186Batch features shape: (64, 2048)\n",
      "Processing batch 87/186Batch features shape: (64, 2048)\n",
      "Processing batch 88/186Batch features shape: (64, 2048)\n",
      "Processing batch 89/186Batch features shape: (64, 2048)\n",
      "Processing batch 90/186Batch features shape: (64, 2048)\n",
      "Processing batch 91/186Batch features shape: (64, 2048)\n",
      "Processing batch 92/186Batch features shape: (64, 2048)\n",
      "Processing batch 93/186Batch features shape: (64, 2048)\n",
      "Processing batch 94/186Batch features shape: (64, 2048)\n",
      "Processing batch 95/186Batch features shape: (64, 2048)\n",
      "Processing batch 96/186Batch features shape: (64, 2048)\n",
      "Processing batch 97/186Batch features shape: (64, 2048)\n",
      "Processing batch 98/186Batch features shape: (64, 2048)\n",
      "Processing batch 99/186Batch features shape: (64, 2048)\n",
      "Processing batch 100/186Batch features shape: (64, 2048)\n",
      "Processing batch 101/186Batch features shape: (64, 2048)\n",
      "Processing batch 102/186Batch features shape: (64, 2048)\n",
      "Processing batch 103/186Batch features shape: (64, 2048)\n",
      "Processing batch 104/186Batch features shape: (64, 2048)\n",
      "Processing batch 105/186Batch features shape: (64, 2048)\n",
      "Processing batch 106/186Batch features shape: (64, 2048)\n",
      "Processing batch 107/186Batch features shape: (64, 2048)\n",
      "Processing batch 108/186Batch features shape: (64, 2048)\n",
      "Processing batch 109/186Batch features shape: (64, 2048)\n",
      "Processing batch 110/186Batch features shape: (64, 2048)\n",
      "Processing batch 111/186Batch features shape: (64, 2048)\n",
      "Processing batch 112/186Batch features shape: (64, 2048)\n",
      "Processing batch 113/186Batch features shape: (64, 2048)\n",
      "Processing batch 114/186Batch features shape: (64, 2048)\n",
      "Processing batch 115/186Batch features shape: (64, 2048)\n",
      "Processing batch 116/186Batch features shape: (64, 2048)\n",
      "Processing batch 117/186Batch features shape: (64, 2048)\n",
      "Processing batch 118/186Batch features shape: (64, 2048)\n",
      "Processing batch 119/186Batch features shape: (64, 2048)\n",
      "Processing batch 120/186Batch features shape: (64, 2048)\n",
      "Processing batch 121/186Batch features shape: (64, 2048)\n",
      "Processing batch 122/186Batch features shape: (64, 2048)\n",
      "Processing batch 123/186Batch features shape: (64, 2048)\n",
      "Processing batch 124/186Batch features shape: (64, 2048)\n",
      "Processing batch 125/186Batch features shape: (64, 2048)\n",
      "Processing batch 126/186Batch features shape: (64, 2048)\n",
      "Processing batch 127/186Batch features shape: (64, 2048)\n",
      "Processing batch 128/186Batch features shape: (64, 2048)\n",
      "Processing batch 129/186Batch features shape: (64, 2048)\n",
      "Processing batch 130/186Batch features shape: (64, 2048)\n",
      "Processing batch 131/186Batch features shape: (64, 2048)\n",
      "Processing batch 132/186Batch features shape: (64, 2048)\n",
      "Processing batch 133/186Batch features shape: (64, 2048)\n",
      "Processing batch 134/186Batch features shape: (64, 2048)\n",
      "Processing batch 135/186Batch features shape: (64, 2048)\n",
      "Processing batch 136/186Batch features shape: (64, 2048)\n",
      "Processing batch 137/186Batch features shape: (64, 2048)\n",
      "Processing batch 138/186Batch features shape: (64, 2048)\n",
      "Processing batch 139/186Batch features shape: (64, 2048)\n",
      "Processing batch 140/186Batch features shape: (64, 2048)\n",
      "Processing batch 141/186Batch features shape: (64, 2048)\n",
      "Processing batch 142/186Batch features shape: (64, 2048)\n",
      "Processing batch 143/186Batch features shape: (64, 2048)\n",
      "Processing batch 144/186Batch features shape: (64, 2048)\n",
      "Processing batch 145/186Batch features shape: (64, 2048)\n",
      "Processing batch 146/186Batch features shape: (64, 2048)\n",
      "Processing batch 147/186Batch features shape: (64, 2048)\n",
      "Processing batch 148/186Batch features shape: (64, 2048)\n",
      "Processing batch 149/186Batch features shape: (64, 2048)\n",
      "Processing batch 150/186Batch features shape: (64, 2048)\n",
      "Processing batch 151/186Batch features shape: (64, 2048)\n",
      "Processing batch 152/186Batch features shape: (64, 2048)\n",
      "Processing batch 153/186Batch features shape: (64, 2048)\n",
      "Processing batch 154/186Batch features shape: (64, 2048)\n",
      "Processing batch 155/186Batch features shape: (64, 2048)\n",
      "Processing batch 156/186Batch features shape: (64, 2048)\n",
      "Processing batch 157/186Batch features shape: (64, 2048)\n",
      "Processing batch 158/186Batch features shape: (64, 2048)\n",
      "Processing batch 159/186Batch features shape: (64, 2048)\n",
      "Processing batch 160/186Batch features shape: (64, 2048)\n",
      "Processing batch 161/186Batch features shape: (64, 2048)\n",
      "Processing batch 162/186Batch features shape: (64, 2048)\n",
      "Processing batch 163/186Batch features shape: (64, 2048)\n",
      "Processing batch 164/186Batch features shape: (64, 2048)\n",
      "Processing batch 165/186Batch features shape: (64, 2048)\n",
      "Processing batch 166/186Batch features shape: (64, 2048)\n",
      "Processing batch 167/186Batch features shape: (64, 2048)\n",
      "Processing batch 168/186Batch features shape: (64, 2048)\n",
      "Processing batch 169/186Batch features shape: (64, 2048)\n",
      "Processing batch 170/186Batch features shape: (64, 2048)\n",
      "Processing batch 171/186Batch features shape: (64, 2048)\n",
      "Processing batch 172/186Batch features shape: (64, 2048)\n",
      "Processing batch 173/186Batch features shape: (64, 2048)\n",
      "Processing batch 174/186Batch features shape: (64, 2048)\n",
      "Processing batch 175/186Batch features shape: (64, 2048)\n",
      "Processing batch 176/186Batch features shape: (64, 2048)\n",
      "Processing batch 177/186Batch features shape: (64, 2048)\n",
      "Processing batch 178/186Batch features shape: (64, 2048)\n",
      "Processing batch 179/186Batch features shape: (64, 2048)\n",
      "Processing batch 180/186Batch features shape: (64, 2048)\n",
      "Processing batch 181/186Batch features shape: (64, 2048)\n",
      "Processing batch 182/186Batch features shape: (64, 2048)\n",
      "Processing batch 183/186Batch features shape: (64, 2048)\n",
      "Processing batch 184/186Batch features shape: (64, 2048)\n",
      "Processing batch 185/186Batch features shape: (64, 2048)\n",
      "Processing batch 186/186Batch features shape: (16, 2048)\n",
      "\n",
      "Feature extraction complete.\n",
      "Extracting features for validation data...\n",
      "Processing all 2964 validation samples\n",
      "Processing batch 1/47Batch features shape: (64, 2048)\n",
      "Processing batch 2/47Batch features shape: (64, 2048)\n",
      "Processing batch 3/47Batch features shape: (64, 2048)\n",
      "Processing batch 4/47Batch features shape: (64, 2048)\n",
      "Processing batch 5/47Batch features shape: (64, 2048)\n",
      "Processing batch 6/47Batch features shape: (64, 2048)\n",
      "Processing batch 7/47Batch features shape: (64, 2048)\n",
      "Processing batch 8/47Batch features shape: (64, 2048)\n",
      "Processing batch 9/47Batch features shape: (64, 2048)\n",
      "Processing batch 10/47Batch features shape: (64, 2048)\n",
      "Processing batch 11/47Batch features shape: (64, 2048)\n",
      "Processing batch 12/47Batch features shape: (64, 2048)\n",
      "Processing batch 13/47Batch features shape: (64, 2048)\n",
      "Processing batch 14/47Batch features shape: (64, 2048)\n",
      "Processing batch 15/47Batch features shape: (64, 2048)\n",
      "Processing batch 16/47Batch features shape: (64, 2048)\n",
      "Processing batch 17/47Batch features shape: (64, 2048)\n",
      "Processing batch 18/47Batch features shape: (64, 2048)\n",
      "Processing batch 19/47Batch features shape: (64, 2048)\n",
      "Processing batch 20/47Batch features shape: (64, 2048)\n",
      "Processing batch 21/47Batch features shape: (64, 2048)\n",
      "Processing batch 22/47Batch features shape: (64, 2048)\n",
      "Processing batch 23/47Batch features shape: (64, 2048)\n",
      "Processing batch 24/47Batch features shape: (64, 2048)\n",
      "Processing batch 25/47Batch features shape: (64, 2048)\n",
      "Processing batch 26/47Batch features shape: (64, 2048)\n",
      "Processing batch 27/47Batch features shape: (64, 2048)\n",
      "Processing batch 28/47Batch features shape: (64, 2048)\n",
      "Processing batch 29/47Batch features shape: (64, 2048)\n",
      "Processing batch 30/47Batch features shape: (64, 2048)\n",
      "Processing batch 31/47Batch features shape: (64, 2048)\n",
      "Processing batch 32/47Batch features shape: (64, 2048)\n",
      "Processing batch 33/47Batch features shape: (64, 2048)\n",
      "Processing batch 34/47Batch features shape: (64, 2048)\n",
      "Processing batch 35/47Batch features shape: (64, 2048)\n",
      "Processing batch 36/47Batch features shape: (64, 2048)\n",
      "Processing batch 37/47Batch features shape: (64, 2048)\n",
      "Processing batch 38/47Batch features shape: (64, 2048)\n",
      "Processing batch 39/47Batch features shape: (64, 2048)\n",
      "Processing batch 40/47Batch features shape: (64, 2048)\n",
      "Processing batch 41/47Batch features shape: (64, 2048)\n",
      "Processing batch 42/47Batch features shape: (64, 2048)\n",
      "Processing batch 43/47Batch features shape: (64, 2048)\n",
      "Processing batch 44/47Batch features shape: (64, 2048)\n",
      "Processing batch 45/47Batch features shape: (64, 2048)\n",
      "Processing batch 46/47Batch features shape: (64, 2048)\n",
      "Processing batch 47/47Batch features shape: (20, 2048)\n",
      "\n",
      "Feature extraction complete.\n",
      "Creating and training AdaBoostClassifier model...\n"
     ]
    }
   ],
   "source": [
    "X_train = extract_features_in_batches(train_image_paths, batch_size=batch_size)\n",
    "y_train = train_labels\n",
    "\n",
    "print(\"Extracting features for validation data...\")\n",
    "print(f\"Processing all {len(val_image_paths)} validation samples\")\n",
    "\n",
    "X_val = extract_features_in_batches(val_image_paths, batch_size=batch_size)\n",
    "y_val = val_labels\n",
    "\n",
    "\n",
    "print(\"Creating and training AdaBoostClassifier model...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726c08239aff4978",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Create a pipeline with scaling and RandomeForest\n",
    "This is much more memory efficient for large datasets\n",
    "Create a pipeline that scales data (optional for RF) and then trains a RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf0b5d10fadbd92",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-03-10T08:38:49.238672100Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),  # Scaling can be useful even though RF doesn't strictly require it\n",
    "    ('AB', AdaBoostClassifier(\n",
    "        n_estimators=100          \n",
    "                        \n",
    "    ))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7caf0c672c1747",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### define the model and train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ec53300e3cc175",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-03-10T08:38:49.242692100Z"
    }
   },
   "outputs": [],
   "source": [
    "# Fit the pipeline directly\n",
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb26e7a3d7f5460",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Evaluate the model on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1195a63f484c1aee",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-03-10T08:38:49.249252900Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Initialize the model\n",
    "model = AdaBoostClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Extract features for test data\n",
    "print(\"Extracting features for test data...\")\n",
    "print(f\"Processing all {len(test_image_paths)} test samples\")\n",
    "\n",
    "X_test = extract_features_in_batches(test_image_paths, batch_size=batch_size)\n",
    "y_test = test_labels\n",
    "\n",
    "# Evaluate on test data\n",
    "test_preds = model.predict(X_test)  # Use X_test instead of test_data\n",
    "test_accuracy = accuracy_score(y_test, test_preds)  # Use y_test instead of test_data\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d0d57d8f2a719b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Visualize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1730004b56a5b749",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-03-10T08:38:49.257895400Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_test, test_preds)\n",
    "\n",
    "# Set up figure size\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "# Create a heatmap without masking zero values for better visualization\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', linewidths=0.5, \n",
    "            xticklabels=gender_age_encoder.classes_,\n",
    "            yticklabels=gender_age_encoder.classes_)\n",
    "\n",
    "# Labels and title\n",
    "plt.xlabel('Predicted Label', fontsize=14)\n",
    "plt.ylabel('True Label', fontsize=14)\n",
    "plt.title('Confusion Matrix', fontsize=16)\n",
    "\n",
    "# Improve layout and save figure\n",
    "plt.tight_layout()\n",
    "plt.savefig('confusion_matrix.png', dpi=300)  # Higher DPI for better quality\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58d40b63f3111d1",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b3855f7a908e06",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-03-10T08:38:49.257895400Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "\n",
    "# Get the unique classes in your test data\n",
    "unique_classes = np.unique(y_test)\n",
    "\n",
    "# Generate classification report using the labels parameter\n",
    "report = classification_report(\n",
    "    y_test, \n",
    "    test_preds, \n",
    "    labels=unique_classes,\n",
    "    target_names=[gender_age_encoder.classes_[i] for i in unique_classes],\n",
    "    zero_division=0  # Handles the division by zero error by setting precision and recall to 0\n",
    ")\n",
    "\n",
    "# Print the classification report with better formatting\n",
    "print(\"=\" * 50)\n",
    "print(\"Classification Report\")\n",
    "print(\"=\" * 50)\n",
    "print(report)\n",
    "print(\"=\" * 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77cc75766f79a15",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87397dc9406a57df",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-03-10T08:38:49.257895400Z"
    }
   },
   "outputs": [],
   "source": [
    "dump(model, 'RF_gray_gender_age_classifier.joblib')\n",
    "dump(gender_age_encoder, 'gender_age_encoder.joblib')\n",
    "print(\"Model saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb12004bfd9a5603",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-10T08:38:49.342389600Z",
     "start_time": "2025-03-10T08:38:49.267004200Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to predict gender_age range for a new image\n",
    "def predict_gender_age(image_path, model, gender_age_encoder):\n",
    "    \"\"\"Predict gender and age for a given face image.\"\"\"\n",
    "    # Extract features\n",
    "    features = grayscale2emb([image_path]) / 255.0\n",
    "    \n",
    "    # Apply the same scaling used during training (if using a pipeline with StandardScaler)\n",
    "    if hasattr(model, 'named_steps') and 'scaler' in model.named_steps:\n",
    "        features = model.named_steps['scaler'].transform(features)\n",
    "    \n",
    "    # Make prediction\n",
    "    pred_class = model.predict(features)[0]\n",
    "    \n",
    "    # Get class probabilities if available\n",
    "    if hasattr(model, 'predict_proba'):\n",
    "        pred_probs = model.predict_proba(features)[0]\n",
    "        confidence = pred_probs[pred_class]\n",
    "    else:\n",
    "        # For AdaBoostClassifier without calibration, we can't get probabilities\n",
    "        # Use decision function instead as a rough proxy for confidence\n",
    "        decision_values = model.decision_function(features)\n",
    "        # Normalize to [0, 1] range roughly\n",
    "        confidence = 1 / (1 + np.exp(-np.abs(decision_values[0][pred_class])))\n",
    "    \n",
    "    # Convert to gender_age range\n",
    "    pred_gender_age_range = gender_age_encoder.classes_[pred_class]\n",
    "    \n",
    "    return pred_gender_age_range, confidence\n",
    "\n",
    "# Example usage:\n",
    "\"\"\"\n",
    "# Load the model\n",
    "image_path = \"path/to/new/face/image.jpg\"\n",
    "model = load('linear_svc_gender_age_classifier.joblib')\n",
    "gender_age_encoder = load('gender_age_encoder.joblib')\n",
    "\n",
    "# Make prediction\n",
    "pred_gender_age, confidence = predict_gender_age(image_path, model, gender_age_encoder)\n",
    "print(f\"Predicted gender_age range: {pred_gender_age} with confidence {confidence:.2f}\")\n",
    "\"\"\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
